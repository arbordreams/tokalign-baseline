# TokAlign Evaluation Results Summary

**Generated by**: TokAlign ACL Publication-Grade Evaluation Framework

## Quick Summary

| Metric | Baseline | Adapted | Delta | Status |
|--------|----------|---------|-------|--------|
| Spanish PPL | -- | -- | --% | -- |
| English PPL | -- | -- | --% | -- |
| Spanish Fertility | -- | -- | --% | -- |
| Throughput (tok/s) | -- | -- | --% | -- |

---

## 1. Tokenizer Efficiency

### Table 1: Tokenizer Metrics on Spanish Corpus

```latex
\begin{table}[h]
\centering
\caption{Tokenizer Efficiency Analysis on Spanish Corpus (N=20,000 samples)}
\label{tab:tokenizer-efficiency}
\begin{tabular}{lccccc}
\toprule
Model & Fertility$\downarrow$ & Compression$\uparrow$ & PCW$\downarrow$ & UNK$\downarrow$ & STRR$\uparrow$ \\
\midrule
Baseline & -- & -- & -- & -- & -- \\
Adapted & -- & -- & -- & -- & -- \\
\midrule
$\Delta$ & --\% & --\% & --\% & -- & --\% \\
\bottomrule
\end{tabular}
\end{table}
```

### Fertility Gap Analysis

| Language | Baseline | Adapted | Gap Reduction |
|----------|----------|---------|---------------|
| English | -- | -- | -- |
| Spanish | -- | -- | -- |
| Overhead | --% | --% | --% |

---

## 2. Perplexity Evaluation

### Table 2: Perplexity Results

```latex
\begin{table}[h]
\centering
\caption{Perplexity Evaluation Results}
\label{tab:perplexity}
\begin{tabular}{lcccccc}
\toprule
Model & \multicolumn{3}{c}{Spanish (OSCAR)} & \multicolumn{3}{c}{English (WikiText-2)} \\
\cmidrule(lr){2-4} \cmidrule(lr){5-7}
 & Mean & Median & P95 & Mean & Median & P95 \\
\midrule
Baseline & -- & -- & -- & -- & -- & -- \\
Adapted & -- & -- & -- & -- & -- & -- \\
\midrule
$\Delta$ & --\% & --\% & -- & --\% & --\% & -- \\
\bottomrule
\end{tabular}
\end{table}
```

### Catastrophic Forgetting Check

- **Threshold**: <5% English PPL increase
- **Baseline English PPL**: --
- **Adapted English PPL**: --
- **Change**: --%
- **Status**: [PASS/FAIL]

---

## 3. Downstream NLU Tasks

### Table 3: NLU Benchmark Results

```latex
\begin{table}[h]
\centering
\caption{Downstream NLU Task Performance (Accuracy)}
\label{tab:nlu-results}
\begin{tabular}{lcccc}
\toprule
Task & Baseline & Adapted & $\Delta$ & Language \\
\midrule
Belebele & -- & -- & -- & Spanish \\
Belebele & -- & -- & -- & English \\
ARC-Easy & -- & -- & -- & English \\
HellaSwag & -- & -- & -- & English \\
LAMBADA & -- & -- & -- & English \\
\bottomrule
\end{tabular}
\end{table}
```

### English Regression Test

- **Threshold**: <2% accuracy drop
- **Status**: [PASS/FAIL]

---

## 4. Machine Translation

### Table 4: MT Evaluation (Spanish â†’ English)

```latex
\begin{table}[h]
\centering
\caption{Machine Translation Results on FLORES-200 (spa$\rightarrow$eng)}
\label{tab:mt-results}
\begin{tabular}{lcccc}
\toprule
Model & BLEU$\uparrow$ & chrF++$\uparrow$ & COMET$\uparrow$ & TER$\downarrow$ \\
\midrule
Baseline & -- & -- & -- & -- \\
Adapted & -- & -- & -- & -- \\
\midrule
$\Delta$ & -- & -- & -- & -- \\
\bottomrule
\end{tabular}
\end{table}
```

---

## 5. Generation Quality

### Table 5: Generation Diversity Metrics

```latex
\begin{table}[h]
\centering
\caption{Generation Quality Analysis (500 Spanish prompts)}
\label{tab:generation-quality}
\begin{tabular}{lccccc}
\toprule
Model & Distinct-1$\uparrow$ & Distinct-2$\uparrow$ & Distinct-3$\uparrow$ & Rep-4$\downarrow$ & Self-BLEU$\downarrow$ \\
\midrule
\multicolumn{6}{l}{\textit{Greedy Decoding}} \\
Baseline & -- & -- & -- & -- & -- \\
Adapted & -- & -- & -- & -- & -- \\
\midrule
\multicolumn{6}{l}{\textit{Nucleus Sampling (p=0.9)}} \\
Baseline & -- & -- & -- & -- & -- \\
Adapted & -- & -- & -- & -- & -- \\
\bottomrule
\end{tabular}
\end{table}
```

---

## 6. Computational Efficiency

### Table 6: Inference Efficiency

```latex
\begin{table}[h]
\centering
\caption{Computational Efficiency (H100 80GB, batch\_size=8)}
\label{tab:efficiency}
\begin{tabular}{lccc}
\toprule
Model & Tokens/sec$\uparrow$ & TTFT (ms)$\downarrow$ & Peak VRAM (MB)$\downarrow$ \\
\midrule
Baseline & -- & -- & -- \\
Adapted & -- & -- & -- \\
\midrule
Speedup & --\% & -- & -- \\
\bottomrule
\end{tabular}
\end{table}
```

### Inference Speedup Analysis

Per CVA Study (Yamaguchi et al., EMNLP 2024):
- Expected speedup from fertility reduction: up to 271.5%
- Observed speedup: --%

---

## 7. Statistical Significance

### Table 7: Statistical Analysis

| Metric | t-statistic | p-value | Cohen's d | Effect Size |
|--------|-------------|---------|-----------|-------------|
| Fertility | -- | -- | -- | -- |
| Compression | -- | -- | -- | -- |
| PCW | -- | -- | -- | -- |
| STRR | -- | -- | -- | -- |

Significance levels: * p<0.05, ** p<0.01, *** p<0.001

---

## Output Files Reference

| File | Description |
|------|-------------|
| `results/tokenizer_analysis_baseline.csv` | Per-sample tokenizer metrics (baseline) |
| `results/tokenizer_analysis_adapted.csv` | Per-sample tokenizer metrics (adapted) |
| `results/tokenizer_comparison.csv` | Statistical comparison results |
| `results/perplexity_baseline.csv` | Per-sample PPL (baseline) |
| `results/perplexity_adapted.csv` | Per-sample PPL (adapted) |
| `results/nlu_results_baseline.json` | lm-eval harness output (baseline) |
| `results/nlu_results_adapted.json` | lm-eval harness output (adapted) |
| `results/mt_results_baseline.csv` | Translation outputs (baseline) |
| `results/mt_results_adapted.csv` | Translation outputs (adapted) |
| `results/generation_baseline.csv` | Generation outputs (baseline) |
| `results/generation_adapted.csv` | Generation outputs (adapted) |
| `results/efficiency_baseline.csv` | Throughput metrics (baseline) |
| `results/efficiency_adapted.csv` | Throughput metrics (adapted) |

---

## Citation

If using this evaluation framework, please cite the relevant papers:

```bibtex
@inproceedings{yamaguchi2024cva,
  title={An Empirical Study on Cross-lingual Vocabulary Adaptation for Efficient Language Model Inference},
  author={Yamaguchi, Atsuki and Villavicencio, Aline and Aletras, Nikolaos},
  booktitle={EMNLP},
  year={2024}
}

@inproceedings{bandarkar2024belebele,
  title={The Belebele Benchmark: a Parallel Reading Comprehension Dataset in 122 Language Variants},
  author={Bandarkar, Lucas and others},
  booktitle={ACL},
  year={2024}
}

@inproceedings{adelani2023sib200,
  title={SIB-200: A Simple, Inclusive, and Big Evaluation Dataset for Topic Classification in 200+ Languages},
  author={Adelani, David Ifeoluwa and others},
  booktitle={EACL},
  year={2023}
}
```

