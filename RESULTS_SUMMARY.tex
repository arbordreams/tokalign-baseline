\documentclass[11pt,a4paper]{article}

% Packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{booktabs}
\usepackage{array}
\usepackage{tabularx}
\usepackage{multirow}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{hyperref}
\usepackage{geometry}
\usepackage{fancyhdr}
\usepackage{titlesec}
\usepackage{enumitem}
\usepackage{amsmath}
\usepackage{amssymb}

% Page setup
\geometry{margin=1in}
\pagestyle{fancy}
\fancyhf{}
\rhead{TokAlign Evaluation}
\lhead{Results Summary}
\rfoot{Page \thepage}

% Colors
\definecolor{improvement}{RGB}{34, 139, 34}
\definecolor{warning}{RGB}{255, 140, 0}
\definecolor{neutral}{RGB}{70, 70, 70}

% Custom commands
\newcommand{\improvement}[1]{\textcolor{improvement}{\textbf{#1}}}
\newcommand{\warning}[1]{\textcolor{warning}{\textbf{#1}}}
\newcommand{\checkmark}{\improvement{$\checkmark$}}

\title{\textbf{TokAlign Evaluation Results Summary}}
\author{Generated by TokAlign Evaluation Framework}
\date{November 29, 2025}

\begin{document}

\maketitle

\begin{abstract}
This report presents a comprehensive evaluation comparing a baseline Pythia-1B model against an adapted model with Qwen2 tokenizer (checkpoint-2500). The evaluation covers tokenizer efficiency, perplexity, natural language understanding, machine translation, generation quality, and computational efficiency.
\end{abstract}

\vspace{1em}
\noindent\textbf{Hardware:} 2$\times$ NVIDIA H100 80GB HBM3, Intel Xeon Platinum 8480+ (52 cores)\\
\textbf{Runtime:} $\sim$20 minutes

\section{Models Compared}

\begin{table}[h]
\centering
\begin{tabular}{lll}
\toprule
\textbf{Model} & \textbf{Description} & \textbf{Parameters} \\
\midrule
Baseline & EleutherAI/pythia-1b & 1.01B \\
Adapted & checkpoint-2500 (Qwen2 tokenizer) & 1.43B \\
\bottomrule
\end{tabular}
\caption{Models evaluated in this study.}
\label{tab:models}
\end{table}

\section{Tokenizer Efficiency}

Analysis performed on $\sim$2.1M Spanish Wikipedia samples.

\begin{table}[h]
\centering
\begin{tabular}{lcccc}
\toprule
\textbf{Metric} & \textbf{Baseline} & \textbf{Adapted} & \textbf{Change} & \textbf{Significant} \\
\midrule
Fertility (tokens/word) & 2.059 & 1.874 & \improvement{$-$9.0\%} & Yes \\
Compression Ratio & 3.169 & 3.501 & \improvement{+10.5\%} & Yes \\
PCW (continued words) & 0.588 & 0.572 & $-$2.7\% & Yes \\
STRR (single-token rate) & 0.412 & 0.428 & +3.9\% & Yes \\
UNK Rate & 0.0\% & 0.0\% & --- & No \\
\bottomrule
\end{tabular}
\caption{Tokenizer efficiency metrics on Spanish Wikipedia corpus.}
\label{tab:tokenizer}
\end{table}

\noindent\textbf{Key Insight:} The adapted Qwen2 tokenizer achieves \textbf{9\% lower fertility} on Spanish text, meaning fewer tokens are needed to represent the same content. This directly translates to faster inference.

\section{Perplexity}

\begin{table}[h]
\centering
\begin{tabular}{lccc}
\toprule
\textbf{Language} & \textbf{Baseline} & \textbf{Adapted} & \textbf{Delta} \\
\midrule
Spanish & 13.88 & 39.06 & \warning{+181\%} \\
English & 36.31 & 43.48 & +20\% \\
\bottomrule
\end{tabular}
\caption{Perplexity comparison (lower is better). Mean values reported.}
\label{tab:perplexity}
\end{table}

\noindent\textbf{Note:} Higher perplexity on the adapted model is expected during early training (checkpoint-2500). The model is still learning to utilize the new tokenizer effectively.

\section{Natural Language Understanding (0-shot)}

\subsection{ARC-Easy}

\begin{table}[h]
\centering
\begin{tabular}{lccc}
\toprule
\textbf{Metric} & \textbf{Baseline} & \textbf{Adapted} & \textbf{Delta} \\
\midrule
Accuracy & 56.94\% & 53.20\% & $-$3.7\% \\
Acc (normalized) & 49.12\% & 46.55\% & $-$2.6\% \\
\bottomrule
\end{tabular}
\caption{ARC-Easy benchmark results.}
\label{tab:arc}
\end{table}

\subsection{HellaSwag}

\begin{table}[h]
\centering
\begin{tabular}{lccc}
\toprule
\textbf{Metric} & \textbf{Baseline} & \textbf{Adapted} & \textbf{Delta} \\
\midrule
Accuracy & 37.63\% & 35.54\% & $-$2.1\% \\
Acc (normalized) & 47.19\% & 43.50\% & $-$3.7\% \\
\bottomrule
\end{tabular}
\caption{HellaSwag benchmark results.}
\label{tab:hellaswag}
\end{table}

\subsection{LAMBADA (OpenAI)}

\begin{table}[h]
\centering
\begin{tabular}{lccc}
\toprule
\textbf{Metric} & \textbf{Baseline} & \textbf{Adapted} & \textbf{Delta} \\
\midrule
Perplexity & 7.92 & 15.15 & +91\% \\
Accuracy & 55.99\% & 46.67\% & $-$9.3\% \\
\bottomrule
\end{tabular}
\caption{LAMBADA benchmark results.}
\label{tab:lambada}
\end{table}

\noindent\textbf{Summary:} Moderate degradation on English NLU tasks ($\sim$3--9\%), which is within acceptable range for early-stage tokenizer adaptation.

\section{Machine Translation (Spanish $\rightarrow$ English)}

Dataset: OPUS-100 (en-es), 1000 samples.

\begin{table}[h]
\centering
\begin{tabular}{lccc}
\toprule
\textbf{Metric} & \textbf{Baseline} & \textbf{Adapted} & \textbf{Delta} \\
\midrule
BLEU & 16.25 & 14.30 & $-$12.0\% \\
chrF++ & 33.31 & 37.70 & \improvement{+13.2\%} \\
TER $\downarrow$ & 83.07 & 101.18 & +21.8\% \\
\bottomrule
\end{tabular}
\caption{Machine translation metrics (Spanish to English).}
\label{tab:mt}
\end{table}

\noindent\textbf{BLEU Signatures:}
\begin{itemize}[noitemsep]
    \item Baseline: \texttt{BLEU = 16.25 48.2/24.8/15.5/10.3 (BP = 0.778 ratio = 0.799)}
    \item Adapted: \texttt{BLEU = 14.30 38.8/17.7/10.0/6.1 (BP = 1.000 ratio = 1.215)}
\end{itemize}

\noindent\textbf{Insight:} The adapted model produces longer outputs (ratio 1.215 vs 0.799), improving chrF++ but increasing TER.

\section{Generation Quality}

Spanish Wikipedia prompts, max 128 tokens.

\subsection{Greedy Decoding}

\begin{table}[h]
\centering
\begin{tabular}{lccc}
\toprule
\textbf{Metric} & \textbf{Baseline} & \textbf{Adapted} & \textbf{Delta} \\
\midrule
Distinct-1 & 0.3463 & 0.3891 & \improvement{+12.4\%} \\
Distinct-2 & 0.4550 & 0.5261 & \improvement{+15.6\%} \\
Distinct-3 & 0.4930 & 0.5973 & \improvement{+21.2\%} \\
Repetition Rate & 0.4434 & 0.2847 & \improvement{$-$35.8\%} \\
Self-BLEU & 2.40 & 4.29 & +78.8\% \\
\bottomrule
\end{tabular}
\caption{Generation quality metrics with greedy decoding.}
\label{tab:gen_greedy}
\end{table}

\subsection{Nucleus Sampling ($p=0.9$)}

\begin{table}[h]
\centering
\begin{tabular}{lccc}
\toprule
\textbf{Metric} & \textbf{Baseline} & \textbf{Adapted} & \textbf{Delta} \\
\midrule
Distinct-1 & 0.6278 & 0.5358 & $-$14.7\% \\
Distinct-2 & 0.9027 & 0.7817 & $-$13.4\% \\
Distinct-3 & 0.9637 & 0.8582 & $-$10.9\% \\
Repetition Rate & 0.0128 & 0.0549 & +329\% \\
Self-BLEU & 2.37 & 1.62 & \improvement{$-$31.6\%} \\
\bottomrule
\end{tabular}
\caption{Generation quality metrics with nucleus sampling.}
\label{tab:gen_nucleus}
\end{table}

\noindent\textbf{Key Finding:} The adapted model shows \textbf{35.8\% less repetition} in greedy decoding, indicating reduced degeneration issues.

\section{Computational Efficiency}

Benchmark: 1000 samples, batch size 8, 3 runs averaged.

\begin{table}[h]
\centering
\begin{tabular}{lccc}
\toprule
\textbf{Metric} & \textbf{Baseline} & \textbf{Adapted} & \textbf{Delta} \\
\midrule
Tokens/second & 619.77 & 496.22 & $-$19.9\% \\
Samples/second & 9.68 & 7.75 & $-$19.9\% \\
TTFT (ms) & 12.91 & 16.12 & +24.9\% \\
Peak VRAM (MB) & 2,314 & 3,108 & +34.3\% \\
\bottomrule
\end{tabular}
\caption{Computational efficiency metrics.}
\label{tab:efficiency}
\end{table}

\noindent\textbf{Note:} The adapted model has 41\% more parameters (1.43B vs 1.01B), which explains the increased VRAM and slightly lower throughput. When normalized by fertility improvement (9\%), effective throughput per semantic unit may be comparable.

\section{Summary \& Conclusions}

\subsection{Improvements}

\begin{enumerate}[noitemsep]
    \item \textbf{Tokenizer Efficiency:} 9\% fertility reduction on Spanish $\rightarrow$ fewer tokens needed
    \item \textbf{Compression:} 10.5\% better byte-to-token ratio
    \item \textbf{Generation Diversity:} 35\% less repetition in greedy decoding
    \item \textbf{chrF++:} 13\% improvement in character-level MT quality
\end{enumerate}

\subsection{Trade-offs}

\begin{enumerate}[noitemsep]
    \item \textbf{Perplexity:} Higher on both languages (expected for checkpoint-2500)
    \item \textbf{NLU Tasks:} 3--9\% degradation on English benchmarks
    \item \textbf{BLEU:} 12\% lower (but chrF++ improved)
    \item \textbf{Throughput:} 20\% slower (but model is 41\% larger)
\end{enumerate}

\subsection{Key Takeaway}

The adapted model with Qwen2 tokenizer shows \textbf{promising tokenizer efficiency gains} for Spanish while maintaining acceptable English performance. Further training beyond checkpoint-2500 is expected to recover NLU performance while retaining tokenizer benefits.

\section{Output Files}

\begin{verbatim}
results/
├── tokenizer_analysis_baseline.csv    (257 MB, 2.1M samples)
├── tokenizer_analysis_adapted.csv     (257 MB, 2.1M samples)
├── tokenizer_comparison.csv           (statistical comparison)
├── perplexity_baseline.csv            (5,258 samples)
├── perplexity_adapted.csv             (5,258 samples)
├── nlu_results_baseline.json          (ARC-Easy, HellaSwag, LAMBADA)
├── nlu_results_adapted.json           (ARC-Easy, HellaSwag, LAMBADA)
├── mt_results_baseline.csv            (1,000 translations)
├── mt_results_adapted.csv             (1,000 translations)
├── generation_baseline.csv            (9 prompts × 2 methods)
├── generation_adapted.csv             (9 prompts × 2 methods)
├── efficiency_baseline.csv            (throughput metrics)
└── efficiency_adapted.csv             (throughput metrics)
\end{verbatim}

\end{document}

