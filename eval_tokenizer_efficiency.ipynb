{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Tokenizer Efficiency Analysis\n",
        "\n",
        "**TokAlign ACL Publication-Grade Evaluation Framework - Notebook 1/3**\n",
        "\n",
        "This notebook performs tokenizer-level analysis comparing baseline and adapted tokenizers.\n",
        "No model inference is required - pure tokenizer metrics only.\n",
        "\n",
        "## Metrics (per CVA study + STRR paper)\n",
        "\n",
        "| Metric | Formula | Target |\n",
        "|--------|---------|--------|\n",
        "| **Fertility** | tokens / words | Spanish approaching English ~1.4 |\n",
        "| **Compression Ratio** | bytes / tokens | Higher = better |\n",
        "| **PCW** | % words split into 2+ subwords | Lower = better |\n",
        "| **UNK Rate** | % unknown tokens | Should approach 0% |\n",
        "| **STRR** | % words preserved as single tokens | Higher = better |\n",
        "\n",
        "## References\n",
        "- Trans-Tokenization (Remy et al., 2024)\n",
        "- CVA Study (Yamaguchi et al., EMNLP 2024) - 271.5% inference speedup\n",
        "- STRR Metric (Nayeem et al., 2025) - Single Token Retention Rate\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 1: Configuration\n",
        "# =====================\n",
        "\n",
        "# === MODEL PATHS ===\n",
        "BASELINE_MODEL = \"EleutherAI/pythia-1b\"\n",
        "ADAPTED_MODEL = \"ADAPTED_MODEL_PATH\"  # Replace with your adapted model path\n",
        "\n",
        "# === DATASET CONFIGURATION ===\n",
        "WIKIPEDIA_SAMPLES = 10_000\n",
        "OSCAR_SAMPLES = 10_000\n",
        "\n",
        "# === OUTPUT FILES ===\n",
        "OUTPUT_DIR = \"results\"\n",
        "BASELINE_OUTPUT = f\"{OUTPUT_DIR}/tokenizer_analysis_baseline.csv\"\n",
        "ADAPTED_OUTPUT = f\"{OUTPUT_DIR}/tokenizer_analysis_adapted.csv\"\n",
        "COMPARISON_OUTPUT = f\"{OUTPUT_DIR}/tokenizer_comparison.csv\"\n",
        "\n",
        "# === STATISTICAL ANALYSIS ===\n",
        "BOOTSTRAP_ITERATIONS = 1000\n",
        "CONFIDENCE_LEVEL = 0.95\n",
        "RANDOM_SEED = 42\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 2: Install Dependencies\n",
        "!pip install transformers datasets pandas numpy scipy tqdm -q\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 3: Imports\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy import stats\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer\n",
        "from tqdm.auto import tqdm\n",
        "from typing import Dict, List, Tuple\n",
        "from dataclasses import dataclass\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(RANDOM_SEED)\n",
        "\n",
        "# Create output directory\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "print(\"Imports completed successfully.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 4: Tokenizer Metrics Functions\n",
        "# ====================================\n",
        "\n",
        "@dataclass\n",
        "class TokenizerMetrics:\n",
        "    \"\"\"Container for all tokenizer metrics for a single text sample.\"\"\"\n",
        "    text: str\n",
        "    num_words: int\n",
        "    num_tokens: int\n",
        "    num_bytes: int\n",
        "    num_unk_tokens: int\n",
        "    num_continued_words: int  # Words split into 2+ subwords\n",
        "    num_single_token_words: int  # Words preserved as single tokens\n",
        "    \n",
        "    @property\n",
        "    def fertility(self) -> float:\n",
        "        \"\"\"Tokens per word (lower is better for efficiency).\"\"\"\n",
        "        return self.num_tokens / max(self.num_words, 1)\n",
        "    \n",
        "    @property\n",
        "    def compression_ratio(self) -> float:\n",
        "        \"\"\"Bytes per token (higher is better - more compression).\"\"\"\n",
        "        return self.num_bytes / max(self.num_tokens, 1)\n",
        "    \n",
        "    @property\n",
        "    def pcw(self) -> float:\n",
        "        \"\"\"Proportion of Continued Words - % words split into 2+ subwords.\"\"\"\n",
        "        return self.num_continued_words / max(self.num_words, 1)\n",
        "    \n",
        "    @property\n",
        "    def unk_rate(self) -> float:\n",
        "        \"\"\"Frequency of unknown tokens.\"\"\"\n",
        "        return self.num_unk_tokens / max(self.num_tokens, 1)\n",
        "    \n",
        "    @property\n",
        "    def strr(self) -> float:\n",
        "        \"\"\"Single Token Retention Rate - % words preserved as single tokens.\"\"\"\n",
        "        return self.num_single_token_words / max(self.num_words, 1)\n",
        "\n",
        "\n",
        "def analyze_text(text: str, tokenizer) -> TokenizerMetrics:\n",
        "    \"\"\"\n",
        "    Compute all tokenizer metrics for a single text.\n",
        "    \n",
        "    Metrics per CVA study (Yamaguchi et al., EMNLP 2024) and STRR paper (Nayeem et al., 2025).\n",
        "    \"\"\"\n",
        "    # Basic counts\n",
        "    words = text.split()\n",
        "    num_words = len(words)\n",
        "    num_bytes = len(text.encode('utf-8'))\n",
        "    \n",
        "    # Tokenize full text\n",
        "    token_ids = tokenizer.encode(text, add_special_tokens=False)\n",
        "    num_tokens = len(token_ids)\n",
        "    \n",
        "    # Count UNK tokens\n",
        "    unk_token_id = tokenizer.unk_token_id\n",
        "    num_unk_tokens = sum(1 for tid in token_ids if tid == unk_token_id) if unk_token_id is not None else 0\n",
        "    \n",
        "    # Analyze word-level tokenization\n",
        "    num_continued_words = 0\n",
        "    num_single_token_words = 0\n",
        "    \n",
        "    for word in words:\n",
        "        word_tokens = tokenizer.encode(word, add_special_tokens=False)\n",
        "        if len(word_tokens) == 1:\n",
        "            num_single_token_words += 1\n",
        "        elif len(word_tokens) > 1:\n",
        "            num_continued_words += 1\n",
        "    \n",
        "    return TokenizerMetrics(\n",
        "        text=text,\n",
        "        num_words=num_words,\n",
        "        num_tokens=num_tokens,\n",
        "        num_bytes=num_bytes,\n",
        "        num_unk_tokens=num_unk_tokens,\n",
        "        num_continued_words=num_continued_words,\n",
        "        num_single_token_words=num_single_token_words\n",
        "    )\n",
        "\n",
        "\n",
        "def analyze_corpus(texts: List[str], tokenizer, desc: str = \"Analyzing\") -> pd.DataFrame:\n",
        "    \"\"\"Analyze a corpus of texts and return metrics DataFrame.\"\"\"\n",
        "    results = []\n",
        "    \n",
        "    for text in tqdm(texts, desc=desc):\n",
        "        if not text or not text.strip():\n",
        "            continue\n",
        "        metrics = analyze_text(text, tokenizer)\n",
        "        results.append({\n",
        "            'text': metrics.text[:200],  # Truncate for storage\n",
        "            'num_words': metrics.num_words,\n",
        "            'num_tokens': metrics.num_tokens,\n",
        "            'num_bytes': metrics.num_bytes,\n",
        "            'num_unk_tokens': metrics.num_unk_tokens,\n",
        "            'num_continued_words': metrics.num_continued_words,\n",
        "            'num_single_token_words': metrics.num_single_token_words,\n",
        "            'fertility': metrics.fertility,\n",
        "            'compression_ratio': metrics.compression_ratio,\n",
        "            'pcw': metrics.pcw,\n",
        "            'unk_rate': metrics.unk_rate,\n",
        "            'strr': metrics.strr\n",
        "        })\n",
        "    \n",
        "    return pd.DataFrame(results)\n",
        "\n",
        "\n",
        "print(\"Tokenizer metrics functions defined.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 5: Statistical Analysis Functions\n",
        "# ======================================\n",
        "\n",
        "def bootstrap_ci(data: np.ndarray, n_bootstrap: int = 1000, confidence: float = 0.95) -> Tuple[float, float, float]:\n",
        "    \"\"\"\n",
        "    Compute bootstrap confidence interval.\n",
        "    \n",
        "    Returns: (mean, lower_ci, upper_ci)\n",
        "    \"\"\"\n",
        "    means = []\n",
        "    n = len(data)\n",
        "    \n",
        "    for _ in range(n_bootstrap):\n",
        "        sample = np.random.choice(data, size=n, replace=True)\n",
        "        means.append(np.mean(sample))\n",
        "    \n",
        "    alpha = 1 - confidence\n",
        "    lower = np.percentile(means, 100 * alpha / 2)\n",
        "    upper = np.percentile(means, 100 * (1 - alpha / 2))\n",
        "    \n",
        "    return np.mean(data), lower, upper\n",
        "\n",
        "\n",
        "def cohens_d(group1: np.ndarray, group2: np.ndarray) -> float:\n",
        "    \"\"\"\n",
        "    Compute Cohen's d effect size.\n",
        "    \n",
        "    Interpretation:\n",
        "    - |d| < 0.2: negligible\n",
        "    - 0.2 <= |d| < 0.5: small\n",
        "    - 0.5 <= |d| < 0.8: medium\n",
        "    - |d| >= 0.8: large\n",
        "    \"\"\"\n",
        "    n1, n2 = len(group1), len(group2)\n",
        "    var1, var2 = np.var(group1, ddof=1), np.var(group2, ddof=1)\n",
        "    \n",
        "    # Pooled standard deviation\n",
        "    pooled_std = np.sqrt(((n1 - 1) * var1 + (n2 - 1) * var2) / (n1 + n2 - 2))\n",
        "    \n",
        "    return (np.mean(group1) - np.mean(group2)) / pooled_std if pooled_std > 0 else 0\n",
        "\n",
        "\n",
        "def compare_tokenizers(baseline_df: pd.DataFrame, adapted_df: pd.DataFrame, \n",
        "                       metric: str) -> Dict:\n",
        "    \"\"\"\n",
        "    Statistical comparison of a metric between two tokenizers.\n",
        "    \n",
        "    Returns dict with:\n",
        "    - baseline_mean, baseline_ci\n",
        "    - adapted_mean, adapted_ci\n",
        "    - t_statistic, p_value (paired t-test)\n",
        "    - cohens_d\n",
        "    - percent_change\n",
        "    \"\"\"\n",
        "    baseline_vals = baseline_df[metric].values\n",
        "    adapted_vals = adapted_df[metric].values\n",
        "    \n",
        "    # Ensure same length for paired test\n",
        "    min_len = min(len(baseline_vals), len(adapted_vals))\n",
        "    baseline_vals = baseline_vals[:min_len]\n",
        "    adapted_vals = adapted_vals[:min_len]\n",
        "    \n",
        "    # Bootstrap CIs\n",
        "    b_mean, b_lower, b_upper = bootstrap_ci(baseline_vals, BOOTSTRAP_ITERATIONS, CONFIDENCE_LEVEL)\n",
        "    a_mean, a_lower, a_upper = bootstrap_ci(adapted_vals, BOOTSTRAP_ITERATIONS, CONFIDENCE_LEVEL)\n",
        "    \n",
        "    # Paired t-test\n",
        "    t_stat, p_value = stats.ttest_rel(baseline_vals, adapted_vals)\n",
        "    \n",
        "    # Effect size\n",
        "    d = cohens_d(baseline_vals, adapted_vals)\n",
        "    \n",
        "    # Percent change\n",
        "    pct_change = ((a_mean - b_mean) / b_mean * 100) if b_mean != 0 else 0\n",
        "    \n",
        "    return {\n",
        "        'metric': metric,\n",
        "        'baseline_mean': b_mean,\n",
        "        'baseline_ci_lower': b_lower,\n",
        "        'baseline_ci_upper': b_upper,\n",
        "        'adapted_mean': a_mean,\n",
        "        'adapted_ci_lower': a_lower,\n",
        "        'adapted_ci_upper': a_upper,\n",
        "        't_statistic': t_stat,\n",
        "        'p_value': p_value,\n",
        "        'cohens_d': d,\n",
        "        'percent_change': pct_change,\n",
        "        'significant': p_value < 0.05\n",
        "    }\n",
        "\n",
        "\n",
        "def interpret_effect_size(d: float) -> str:\n",
        "    \"\"\"Interpret Cohen's d effect size.\"\"\"\n",
        "    d_abs = abs(d)\n",
        "    if d_abs < 0.2:\n",
        "        return \"negligible\"\n",
        "    elif d_abs < 0.5:\n",
        "        return \"small\"\n",
        "    elif d_abs < 0.8:\n",
        "        return \"medium\"\n",
        "    else:\n",
        "        return \"large\"\n",
        "\n",
        "\n",
        "print(\"Statistical analysis functions defined.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 6: Load Tokenizers\n",
        "# =======================\n",
        "\n",
        "print(\"Loading tokenizers...\")\n",
        "\n",
        "# Load baseline tokenizer\n",
        "baseline_tokenizer = AutoTokenizer.from_pretrained(BASELINE_MODEL)\n",
        "print(f\"Baseline tokenizer: {BASELINE_MODEL}\")\n",
        "print(f\"  Vocab size: {baseline_tokenizer.vocab_size}\")\n",
        "\n",
        "# Load adapted tokenizer (will fail gracefully if not available)\n",
        "try:\n",
        "    adapted_tokenizer = AutoTokenizer.from_pretrained(ADAPTED_MODEL)\n",
        "    print(f\"\\nAdapted tokenizer: {ADAPTED_MODEL}\")\n",
        "    print(f\"  Vocab size: {adapted_tokenizer.vocab_size}\")\n",
        "    HAS_ADAPTED = True\n",
        "except Exception as e:\n",
        "    print(f\"\\nAdapted model not found: {ADAPTED_MODEL}\")\n",
        "    print(\"  Running baseline-only analysis.\")\n",
        "    adapted_tokenizer = None\n",
        "    HAS_ADAPTED = False\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 7: Load Datasets\n",
        "# =====================\n",
        "\n",
        "print(\"Loading datasets...\")\n",
        "\n",
        "# Load Spanish Wikipedia\n",
        "print(\"\\n1. Loading Spanish Wikipedia...\")\n",
        "wiki_dataset = load_dataset(\n",
        "    \"wikimedia/wikipedia\", \n",
        "    \"20231101.es\",\n",
        "    split=\"train\",\n",
        "    trust_remote_code=True\n",
        ")\n",
        "wiki_texts = [x['text'] for x in wiki_dataset.shuffle(seed=RANDOM_SEED).select(range(WIKIPEDIA_SAMPLES))]\n",
        "print(f\"   Loaded {len(wiki_texts)} Wikipedia samples\")\n",
        "\n",
        "# Load Spanish OSCAR\n",
        "print(\"\\n2. Loading Spanish OSCAR...\")\n",
        "oscar_dataset = load_dataset(\n",
        "    \"oscar-corpus/OSCAR-2301\",\n",
        "    \"es\",\n",
        "    split=\"train\",\n",
        "    trust_remote_code=True,\n",
        "    streaming=True  # Use streaming for large dataset\n",
        ")\n",
        "\n",
        "# Collect OSCAR samples via streaming\n",
        "oscar_texts = []\n",
        "for i, sample in enumerate(oscar_dataset):\n",
        "    if i >= OSCAR_SAMPLES:\n",
        "        break\n",
        "    text = sample.get('text', '')\n",
        "    if text and len(text) > 50:  # Filter short texts\n",
        "        oscar_texts.append(text)\n",
        "        \n",
        "print(f\"   Loaded {len(oscar_texts)} OSCAR samples\")\n",
        "\n",
        "# Combine datasets\n",
        "all_spanish_texts = wiki_texts + oscar_texts\n",
        "print(f\"\\nTotal Spanish corpus: {len(all_spanish_texts)} samples\")\n",
        "\n",
        "# Also load English for comparison (fertility baseline)\n",
        "print(\"\\n3. Loading English Wikipedia for comparison...\")\n",
        "wiki_en = load_dataset(\n",
        "    \"wikimedia/wikipedia\",\n",
        "    \"20231101.en\",\n",
        "    split=\"train\",\n",
        "    trust_remote_code=True\n",
        ")\n",
        "english_texts = [x['text'] for x in wiki_en.shuffle(seed=RANDOM_SEED).select(range(5000))]\n",
        "print(f\"   Loaded {len(english_texts)} English Wikipedia samples\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 8: Analyze Baseline Tokenizer\n",
        "# ===================================\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"ANALYZING BASELINE TOKENIZER\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Spanish analysis\n",
        "print(\"\\n--- Spanish Corpus ---\")\n",
        "baseline_spanish_df = analyze_corpus(all_spanish_texts, baseline_tokenizer, \"Baseline (Spanish)\")\n",
        "\n",
        "# English analysis (for fertility comparison)\n",
        "print(\"\\n--- English Corpus (Reference) ---\")\n",
        "baseline_english_df = analyze_corpus(english_texts, baseline_tokenizer, \"Baseline (English)\")\n",
        "\n",
        "# Save baseline results\n",
        "baseline_spanish_df.to_csv(BASELINE_OUTPUT, index=False)\n",
        "print(f\"\\nBaseline Spanish results saved to: {BASELINE_OUTPUT}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 9: Analyze Adapted Tokenizer (if available)\n",
        "# ================================================\n",
        "\n",
        "if HAS_ADAPTED:\n",
        "    print(\"=\" * 70)\n",
        "    print(\"ANALYZING ADAPTED TOKENIZER\")\n",
        "    print(\"=\" * 70)\n",
        "    \n",
        "    # Spanish analysis\n",
        "    print(\"\\n--- Spanish Corpus ---\")\n",
        "    adapted_spanish_df = analyze_corpus(all_spanish_texts, adapted_tokenizer, \"Adapted (Spanish)\")\n",
        "    \n",
        "    # English analysis\n",
        "    print(\"\\n--- English Corpus ---\")\n",
        "    adapted_english_df = analyze_corpus(english_texts, adapted_tokenizer, \"Adapted (English)\")\n",
        "    \n",
        "    # Save adapted results\n",
        "    adapted_spanish_df.to_csv(ADAPTED_OUTPUT, index=False)\n",
        "    print(f\"\\nAdapted Spanish results saved to: {ADAPTED_OUTPUT}\")\n",
        "else:\n",
        "    adapted_spanish_df = None\n",
        "    adapted_english_df = None\n",
        "    print(\"Skipping adapted tokenizer analysis (model not available).\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 10: Summary Statistics\n",
        "# ============================\n",
        "\n",
        "def print_summary_stats(df: pd.DataFrame, name: str):\n",
        "    \"\"\"Print summary statistics for a metrics DataFrame.\"\"\"\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"{name}\")\n",
        "    print(f\"{'='*60}\")\n",
        "    \n",
        "    metrics = ['fertility', 'compression_ratio', 'pcw', 'unk_rate', 'strr']\n",
        "    \n",
        "    print(f\"\\n{'Metric':<20} {'Mean':>10} {'Std':>10} {'Median':>10} {'P5':>10} {'P95':>10}\")\n",
        "    print(\"-\" * 70)\n",
        "    \n",
        "    for metric in metrics:\n",
        "        vals = df[metric].values\n",
        "        mean_val, ci_lower, ci_upper = bootstrap_ci(vals, BOOTSTRAP_ITERATIONS, CONFIDENCE_LEVEL)\n",
        "        print(f\"{metric:<20} {np.mean(vals):>10.4f} {np.std(vals):>10.4f} \"\n",
        "              f\"{np.median(vals):>10.4f} {np.percentile(vals, 5):>10.4f} {np.percentile(vals, 95):>10.4f}\")\n",
        "        print(f\"{'  95% CI':<20} [{ci_lower:>10.4f}, {ci_upper:>10.4f}]\")\n",
        "\n",
        "\n",
        "# Print baseline statistics\n",
        "print_summary_stats(baseline_spanish_df, \"BASELINE TOKENIZER - SPANISH\")\n",
        "print_summary_stats(baseline_english_df, \"BASELINE TOKENIZER - ENGLISH (Reference)\")\n",
        "\n",
        "# Print adapted statistics if available\n",
        "if HAS_ADAPTED:\n",
        "    print_summary_stats(adapted_spanish_df, \"ADAPTED TOKENIZER - SPANISH\")\n",
        "    print_summary_stats(adapted_english_df, \"ADAPTED TOKENIZER - ENGLISH\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 11: Statistical Comparison (Baseline vs Adapted)\n",
        "# =====================================================\n",
        "\n",
        "if HAS_ADAPTED:\n",
        "    print(\"=\" * 70)\n",
        "    print(\"STATISTICAL COMPARISON: BASELINE vs ADAPTED (SPANISH)\")\n",
        "    print(\"=\" * 70)\n",
        "    \n",
        "    metrics_to_compare = ['fertility', 'compression_ratio', 'pcw', 'unk_rate', 'strr']\n",
        "    comparison_results = []\n",
        "    \n",
        "    for metric in metrics_to_compare:\n",
        "        result = compare_tokenizers(baseline_spanish_df, adapted_spanish_df, metric)\n",
        "        comparison_results.append(result)\n",
        "        \n",
        "        effect_interpretation = interpret_effect_size(result['cohens_d'])\n",
        "        sig_marker = \"***\" if result['p_value'] < 0.001 else \"**\" if result['p_value'] < 0.01 else \"*\" if result['p_value'] < 0.05 else \"\"\n",
        "        \n",
        "        print(f\"\\n--- {metric.upper()} ---\")\n",
        "        print(f\"  Baseline: {result['baseline_mean']:.4f} [{result['baseline_ci_lower']:.4f}, {result['baseline_ci_upper']:.4f}]\")\n",
        "        print(f\"  Adapted:  {result['adapted_mean']:.4f} [{result['adapted_ci_lower']:.4f}, {result['adapted_ci_upper']:.4f}]\")\n",
        "        print(f\"  Change:   {result['percent_change']:+.2f}%\")\n",
        "        print(f\"  t-stat:   {result['t_statistic']:.3f}, p-value: {result['p_value']:.2e} {sig_marker}\")\n",
        "        print(f\"  Cohen's d: {result['cohens_d']:.3f} ({effect_interpretation})\")\n",
        "    \n",
        "    # Save comparison results\n",
        "    comparison_df = pd.DataFrame(comparison_results)\n",
        "    comparison_df.to_csv(COMPARISON_OUTPUT, index=False)\n",
        "    print(f\"\\nComparison results saved to: {COMPARISON_OUTPUT}\")\n",
        "else:\n",
        "    print(\"Comparison skipped (adapted model not available).\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 12: Fertility Gap Analysis\n",
        "# ================================\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"FERTILITY GAP ANALYSIS\")\n",
        "print(\"=\" * 70)\n",
        "print(\"\\nGoal: Spanish fertility approaching English (~1.4 tokens/word)\")\n",
        "\n",
        "# Calculate fertility gap for baseline\n",
        "baseline_es_fertility = baseline_spanish_df['fertility'].mean()\n",
        "baseline_en_fertility = baseline_english_df['fertility'].mean()\n",
        "baseline_gap = baseline_es_fertility - baseline_en_fertility\n",
        "baseline_gap_pct = (baseline_gap / baseline_en_fertility) * 100\n",
        "\n",
        "print(f\"\\n--- BASELINE ---\")\n",
        "print(f\"  English fertility:  {baseline_en_fertility:.4f} tokens/word\")\n",
        "print(f\"  Spanish fertility:  {baseline_es_fertility:.4f} tokens/word\")\n",
        "print(f\"  Fertility gap:      {baseline_gap:+.4f} ({baseline_gap_pct:+.1f}% overhead)\")\n",
        "\n",
        "if HAS_ADAPTED:\n",
        "    adapted_es_fertility = adapted_spanish_df['fertility'].mean()\n",
        "    adapted_en_fertility = adapted_english_df['fertility'].mean()\n",
        "    adapted_gap = adapted_es_fertility - adapted_en_fertility\n",
        "    adapted_gap_pct = (adapted_gap / adapted_en_fertility) * 100\n",
        "    \n",
        "    print(f\"\\n--- ADAPTED ---\")\n",
        "    print(f\"  English fertility:  {adapted_en_fertility:.4f} tokens/word\")\n",
        "    print(f\"  Spanish fertility:  {adapted_es_fertility:.4f} tokens/word\")\n",
        "    print(f\"  Fertility gap:      {adapted_gap:+.4f} ({adapted_gap_pct:+.1f}% overhead)\")\n",
        "    \n",
        "    # Improvement calculation\n",
        "    gap_reduction = baseline_gap - adapted_gap\n",
        "    gap_reduction_pct = (gap_reduction / baseline_gap) * 100 if baseline_gap != 0 else 0\n",
        "    \n",
        "    print(f\"\\n--- IMPROVEMENT ---\")\n",
        "    print(f\"  Fertility gap reduction: {gap_reduction:.4f} ({gap_reduction_pct:.1f}% reduction)\")\n",
        "    print(f\"  Estimated inference speedup: {(baseline_es_fertility / adapted_es_fertility - 1) * 100:.1f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 13: Generate LaTeX Tables\n",
        "# ===============================\n",
        "\n",
        "def generate_latex_table():\n",
        "    \"\"\"Generate LaTeX-formatted results table.\"\"\"\n",
        "    \n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"LATEX TABLE OUTPUT\")\n",
        "    print(\"=\" * 70)\n",
        "    \n",
        "    # Table 1: Summary metrics\n",
        "    print(\"\\n% Table 1: Tokenizer Efficiency Metrics\")\n",
        "    print(\"\\\\begin{table}[h]\")\n",
        "    print(\"\\\\centering\")\n",
        "    print(\"\\\\caption{Tokenizer Efficiency Analysis on Spanish Corpus}\")\n",
        "    print(\"\\\\label{tab:tokenizer-efficiency}\")\n",
        "    print(\"\\\\begin{tabular}{lccccc}\")\n",
        "    print(\"\\\\toprule\")\n",
        "    print(\"Model & Fertility$\\\\downarrow$ & Compression$\\\\uparrow$ & PCW$\\\\downarrow$ & UNK$\\\\downarrow$ & STRR$\\\\uparrow$ \\\\\\\\\")\n",
        "    print(\"\\\\midrule\")\n",
        "    \n",
        "    # Baseline row\n",
        "    b_fert = baseline_spanish_df['fertility'].mean()\n",
        "    b_comp = baseline_spanish_df['compression_ratio'].mean()\n",
        "    b_pcw = baseline_spanish_df['pcw'].mean()\n",
        "    b_unk = baseline_spanish_df['unk_rate'].mean()\n",
        "    b_strr = baseline_spanish_df['strr'].mean()\n",
        "    print(f\"Baseline & {b_fert:.3f} & {b_comp:.3f} & {b_pcw:.3f} & {b_unk:.4f} & {b_strr:.3f} \\\\\\\\\")\n",
        "    \n",
        "    if HAS_ADAPTED:\n",
        "        a_fert = adapted_spanish_df['fertility'].mean()\n",
        "        a_comp = adapted_spanish_df['compression_ratio'].mean()\n",
        "        a_pcw = adapted_spanish_df['pcw'].mean()\n",
        "        a_unk = adapted_spanish_df['unk_rate'].mean()\n",
        "        a_strr = adapted_spanish_df['strr'].mean()\n",
        "        print(f\"Adapted & {a_fert:.3f} & {a_comp:.3f} & {a_pcw:.3f} & {a_unk:.4f} & {a_strr:.3f} \\\\\\\\\")\n",
        "        \n",
        "        # Delta row\n",
        "        print(\"\\\\midrule\")\n",
        "        delta_fert = ((a_fert - b_fert) / b_fert) * 100 if b_fert != 0 else 0\n",
        "        delta_comp = ((a_comp - b_comp) / b_comp) * 100 if b_comp != 0 else 0\n",
        "        delta_pcw = ((a_pcw - b_pcw) / b_pcw) * 100 if b_pcw != 0 else 0\n",
        "        delta_strr = ((a_strr - b_strr) / b_strr) * 100 if b_strr != 0 else 0\n",
        "        print(f\"$\\\\Delta$ & {delta_fert:+.1f}\\\\% & {delta_comp:+.1f}\\\\% & {delta_pcw:+.1f}\\\\% & -- & {delta_strr:+.1f}\\\\% \\\\\\\\\")\n",
        "    \n",
        "    print(\"\\\\bottomrule\")\n",
        "    print(\"\\\\end{tabular}\")\n",
        "    print(\"\\\\end{table}\")\n",
        "\n",
        "\n",
        "generate_latex_table()\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"TOKENIZER EFFICIENCY ANALYSIS COMPLETE\")\n",
        "print(\"=\" * 70)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
