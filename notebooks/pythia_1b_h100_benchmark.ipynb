{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Package Installation\n",
    "!pip install transformers datasets accelerate pandas nvidia-ml-py flash-attn --no-build-isolation -q\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU: NVIDIA H100 80GB HBM3\n",
      "Compute Capability: 9.0\n",
      "CUDA Version: 12.8\n",
      "PyTorch Version: 2.7.0\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Imports and GPU Validation\n",
    "import torch\n",
    "import pandas as pd\n",
    "import pynvml\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import time\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Hard check for H100/A100 GPU (compute capability >= 8.0 required for Flash Attention 2)\n",
    "assert torch.cuda.is_available(), \"CUDA is not available - H100 GPU required\"\n",
    "compute_capability = torch.cuda.get_device_capability()\n",
    "assert compute_capability[0] >= 8, f\"H100/A100 GPU required for Flash Attention 2 (got compute capability {compute_capability})\"\n",
    "\n",
    "# Initialize pynvml for VRAM monitoring\n",
    "pynvml.nvmlInit()\n",
    "gpu_handle = pynvml.nvmlDeviceGetHandleByIndex(0)\n",
    "\n",
    "print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"Compute Capability: {compute_capability[0]}.{compute_capability[1]}\")\n",
    "print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading EleutherAI/pythia-1b with Flash Attention 2...\n",
      "Model loaded successfully on cuda:0\n",
      "Model dtype: torch.bfloat16\n",
      "Tokenizer pad_token: '<|endoftext|>' (id: 0)\n",
      "Tokenizer padding_side: left\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Model Loading (H100 Native)\n",
    "MODEL_NAME = \"EleutherAI/pythia-1b\"\n",
    "\n",
    "print(f\"Loading {MODEL_NAME} with Flash Attention 2...\")\n",
    "\n",
    "# Load model with H100-optimized settings\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    dtype=torch.bfloat16,\n",
    "    device_map=\"cuda\",\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "# Load tokenizer with proper padding configuration for batched generation\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"left\"  # Required for batched generation\n",
    "\n",
    "print(f\"Model loaded successfully on {next(model.parameters()).device}\")\n",
    "print(f\"Model dtype: {next(model.parameters()).dtype}\")\n",
    "print(f\"Tokenizer pad_token: '{tokenizer.pad_token}' (id: {tokenizer.pad_token_id})\")\n",
    "print(f\"Tokenizer padding_side: {tokenizer.padding_side}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading globalvoices-en-es dataset...\n",
      "\n",
      "Raw dataset size: 355136\n",
      "Dataset column names: ['en', 'es']\n",
      "First sample: {'en': 'Argentina: Stencil Graffiti · Global Voices ', 'es': 'Argentina: Graffitis '}\n",
      "\n",
      "--- Applying Quality Filters ---\n",
      "Min chars: 30, Max chars: 500, Min words: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 355136/355136 [00:05<00:00, 61675.58 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples before filtering: 355136\n",
      "Samples after filtering: 321373\n",
      "Filtered out: 33763 (9.5%)\n",
      "\n",
      "Final dataset size: 5000\n",
      "Sample after filtering: {'spanish': '“Estos estados del CCG no son para nada competentes para lidiar con pedidos populares de libertad, sin dejar de mencionar gobierno democrático, porque ellos mismos son en su mayoría regímenes despóticos”, observó el Consejo de Coordinación de Yemen de la Revolución Juvenil de Cambio (CCYRC, por sus siglas en inglés). ', 'english': '“These GCC states are not at all competent to deal with popular requests for liberty and freedom, not to mention democratic government, because they themselves are mostly despotic regimes,” observed Yemen’s Coordinating Council of the Youth Revolution of Change (CCYRC). '}\n",
      "\n",
      "--- Dataset Statistics ---\n",
      "Spanish chars: min=30, max=492, mean=132\n",
      "English chars: min=30, max=465, mean=122\n",
      "\n",
      "DataLoader created: 157 batches of size 32\n",
      "Max new tokens for generation: 128\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Data Pipeline with Quality Filtering\n",
    "\n",
    "# === CONFIGURATION ===\n",
    "BATCH_SIZE = 32  # Reduced for longer sequences\n",
    "NUM_SAMPLES = 5000\n",
    "MAX_NEW_TOKENS = 128  # Increased from 64 to avoid truncation\n",
    "\n",
    "# Quality filter thresholds\n",
    "MIN_CHARS = 30  # Minimum characters per text\n",
    "MAX_CHARS = 500  # Maximum characters (avoid very long sequences)\n",
    "MIN_WORDS = 5  # Minimum words per text\n",
    "\n",
    "print(\"Loading globalvoices-en-es dataset...\")\n",
    "dataset = load_dataset(\"alvations/globalvoices-en-es\", split=\"train\")\n",
    "\n",
    "# Inspection step: verify dataset structure\n",
    "print(f\"\\nRaw dataset size: {len(dataset)}\")\n",
    "print(f\"Dataset column names: {dataset.column_names}\")\n",
    "print(f\"First sample: {dataset[0]}\")\n",
    "\n",
    "# Rename columns from en/es to spanish/english for consistency\n",
    "def rename_columns(example):\n",
    "    return {\n",
    "        \"spanish\": example[\"es\"],\n",
    "        \"english\": example[\"en\"]\n",
    "    }\n",
    "\n",
    "dataset = dataset.map(rename_columns, remove_columns=dataset.column_names)\n",
    "\n",
    "# === QUALITY FILTERING ===\n",
    "print(f\"\\n--- Applying Quality Filters ---\")\n",
    "print(f\"Min chars: {MIN_CHARS}, Max chars: {MAX_CHARS}, Min words: {MIN_WORDS}\")\n",
    "\n",
    "def is_quality_sample(example):\n",
    "    \"\"\"Filter out degenerate samples.\"\"\"\n",
    "    es = example[\"spanish\"]\n",
    "    en = example[\"english\"]\n",
    "    \n",
    "    # Length checks\n",
    "    if len(es) < MIN_CHARS or len(en) < MIN_CHARS:\n",
    "        return False\n",
    "    if len(es) > MAX_CHARS or len(en) > MAX_CHARS:\n",
    "        return False\n",
    "    \n",
    "    # Word count checks\n",
    "    if len(es.split()) < MIN_WORDS or len(en.split()) < MIN_WORDS:\n",
    "        return False\n",
    "    \n",
    "    # Content quality checks\n",
    "    # Reject if mostly punctuation/special chars\n",
    "    alpha_ratio_es = sum(c.isalpha() for c in es) / max(len(es), 1)\n",
    "    alpha_ratio_en = sum(c.isalpha() for c in en) / max(len(en), 1)\n",
    "    if alpha_ratio_es < 0.5 or alpha_ratio_en < 0.5:\n",
    "        return False\n",
    "    \n",
    "    # Reject titles/headers (often end with · Global Voices)\n",
    "    if es.strip().endswith(\"·\") or en.strip().endswith(\"·\"):\n",
    "        return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "pre_filter_size = len(dataset)\n",
    "dataset = dataset.filter(is_quality_sample)\n",
    "post_filter_size = len(dataset)\n",
    "filtered_out = pre_filter_size - post_filter_size\n",
    "\n",
    "print(f\"Samples before filtering: {pre_filter_size}\")\n",
    "print(f\"Samples after filtering: {post_filter_size}\")\n",
    "print(f\"Filtered out: {filtered_out} ({100*filtered_out/pre_filter_size:.1f}%)\")\n",
    "\n",
    "# Shuffle and select samples\n",
    "dataset = dataset.shuffle(seed=42)\n",
    "dataset = dataset.select(range(min(NUM_SAMPLES, len(dataset))))\n",
    "\n",
    "print(f\"\\nFinal dataset size: {len(dataset)}\")\n",
    "print(f\"Sample after filtering: {dataset[0]}\")\n",
    "\n",
    "# Show dataset statistics\n",
    "spanish_lens = [len(x[\"spanish\"]) for x in dataset]\n",
    "english_lens = [len(x[\"english\"]) for x in dataset]\n",
    "print(f\"\\n--- Dataset Statistics ---\")\n",
    "print(f\"Spanish chars: min={min(spanish_lens)}, max={max(spanish_lens)}, mean={sum(spanish_lens)/len(spanish_lens):.0f}\")\n",
    "print(f\"English chars: min={min(english_lens)}, max={max(english_lens)}, mean={sum(english_lens)/len(english_lens):.0f}\")\n",
    "\n",
    "# Create a custom Dataset class for DataLoader\n",
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, hf_dataset):\n",
    "        self.data = hf_dataset\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        return item[\"spanish\"], item[\"english\"]\n",
    "\n",
    "# Create DataLoader\n",
    "torch_dataset = TranslationDataset(dataset)\n",
    "dataloader = DataLoader(\n",
    "    torch_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=False,\n",
    "    num_workers=0,  # Avoid multiprocessing issues in notebook\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "print(f\"\\nDataLoader created: {len(dataloader)} batches of size {BATCH_SIZE}\")\n",
    "print(f\"Max new tokens for generation: {MAX_NEW_TOKENS}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmark functions defined successfully.\n",
      "Generation will use max_new_tokens=128 with early stopping\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Benchmark Functions (Improved)\n",
    "\n",
    "@torch.no_grad()\n",
    "def calculate_batch_perplexity(texts: list[str], model, tokenizer) -> list[float]:\n",
    "    \"\"\"\n",
    "    Vectorized perplexity calculation using cross-entropy loss.\n",
    "    Uses teacher-forcing: compute loss on input tokens and convert to PPL via exp(loss).\n",
    "    \"\"\"\n",
    "    # Tokenize batch with padding\n",
    "    encodings = tokenizer(\n",
    "        texts,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "    ).to(\"cuda\")\n",
    "    \n",
    "    input_ids = encodings.input_ids\n",
    "    attention_mask = encodings.attention_mask\n",
    "    \n",
    "    # Forward pass\n",
    "    outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "    logits = outputs.logits\n",
    "    \n",
    "    # Shift logits and labels for causal LM loss calculation\n",
    "    shift_logits = logits[:, :-1, :].contiguous()\n",
    "    shift_labels = input_ids[:, 1:].contiguous()\n",
    "    shift_mask = attention_mask[:, 1:].contiguous()\n",
    "    \n",
    "    # Calculate per-token cross entropy loss\n",
    "    loss_fct = torch.nn.CrossEntropyLoss(reduction='none')\n",
    "    losses = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "    losses = losses.view(shift_labels.size())\n",
    "    \n",
    "    # Mask out padding tokens and calculate mean loss per sequence\n",
    "    masked_losses = losses * shift_mask\n",
    "    seq_lengths = shift_mask.sum(dim=1).clamp(min=1)  # Avoid division by zero\n",
    "    mean_losses = masked_losses.sum(dim=1) / seq_lengths\n",
    "    \n",
    "    # Convert to perplexity\n",
    "    perplexities = torch.exp(mean_losses)\n",
    "    \n",
    "    return perplexities.cpu().tolist()\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate_translations(spanish_texts: list[str], model, tokenizer, max_new_tokens: int = 128) -> tuple[list[str], list[bool]]:\n",
    "    \"\"\"\n",
    "    Batched generation with greedy decoding and early stopping.\n",
    "    Format: \"Spanish: {es}\\nEnglish:\"\n",
    "    \n",
    "    Returns:\n",
    "        - generated_texts: List of generated translations\n",
    "        - hit_max_tokens: List of bools indicating if generation hit the token limit\n",
    "    \"\"\"\n",
    "    # Format prompts\n",
    "    prompts = [f\"Spanish: {text}\\nEnglish:\" for text in spanish_texts]\n",
    "    \n",
    "    # Tokenize with left padding (already configured in tokenizer)\n",
    "    encodings = tokenizer(\n",
    "        prompts,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "    ).to(\"cuda\")\n",
    "    \n",
    "    # Generate with greedy decoding + stopping criteria\n",
    "    generated_ids = model.generate(\n",
    "        input_ids=encodings.input_ids,\n",
    "        attention_mask=encodings.attention_mask,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=False,  # Greedy decoding\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        # Stop on newline to prevent runaway generation\n",
    "        stop_strings=[\"\\n\\n\", \"Spanish:\", \"\\nSpanish\"],\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "    \n",
    "    # Calculate how many new tokens were generated per sequence\n",
    "    input_length = encodings.input_ids.shape[1]\n",
    "    new_token_counts = (generated_ids.shape[1] - input_length)\n",
    "    hit_max_tokens = [new_token_counts >= max_new_tokens] * len(spanish_texts)\n",
    "    \n",
    "    # Decode only the new tokens (exclude input prompt)\n",
    "    generated_texts = tokenizer.batch_decode(\n",
    "        generated_ids[:, input_length:],\n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "    \n",
    "    # Clean up outputs - take only the first line/sentence\n",
    "    cleaned_texts = []\n",
    "    for text in generated_texts:\n",
    "        # Stop at newline or \"Spanish:\" echo\n",
    "        for stop in [\"\\n\", \"Spanish:\"]:\n",
    "            if stop in text:\n",
    "                text = text.split(stop)[0]\n",
    "        cleaned_texts.append(text.strip())\n",
    "    \n",
    "    return cleaned_texts, hit_max_tokens\n",
    "\n",
    "\n",
    "def get_peak_vram_mb(handle) -> float:\n",
    "    \"\"\"Get peak VRAM usage in MB using pynvml.\"\"\"\n",
    "    info = pynvml.nvmlDeviceGetMemoryInfo(handle)\n",
    "    return info.used / (1024 ** 2)\n",
    "\n",
    "\n",
    "def reset_peak_memory_stats():\n",
    "    \"\"\"Reset PyTorch's peak memory tracking.\"\"\"\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "\n",
    "def get_pytorch_peak_memory_mb() -> float:\n",
    "    \"\"\"Get PyTorch's tracked peak memory in MB.\"\"\"\n",
    "    return torch.cuda.max_memory_allocated() / (1024 ** 2)\n",
    "\n",
    "\n",
    "print(\"Benchmark functions defined successfully.\")\n",
    "print(f\"Generation will use max_new_tokens={MAX_NEW_TOKENS} with early stopping\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "STARTING BENCHMARK\n",
      "======================================================================\n",
      "\n",
      "Configuration:\n",
      "  Total samples: 5000\n",
      "  Batch size: 32\n",
      "  Total batches: 157\n",
      "  Max new tokens: 128\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Warming up GPU...\n",
      "Warm-up complete.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  13%|█▎        | 20/157 [00:53<06:07,  2.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 20/157 | Throughput: 11.9 samples/s | Peak VRAM: 5518 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  25%|██▌       | 40/157 [01:47<05:13,  2.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 40/157 | Throughput: 11.9 samples/s | Peak VRAM: 5518 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  38%|███▊      | 60/157 [02:41<04:20,  2.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 60/157 | Throughput: 11.9 samples/s | Peak VRAM: 5518 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  51%|█████     | 80/157 [03:35<03:26,  2.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 80/157 | Throughput: 11.9 samples/s | Peak VRAM: 5518 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  64%|██████▎   | 100/157 [04:29<02:34,  2.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 100/157 | Throughput: 11.9 samples/s | Peak VRAM: 5518 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  76%|███████▋  | 120/157 [05:23<01:38,  2.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 120/157 | Throughput: 11.9 samples/s | Peak VRAM: 5518 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  89%|████████▉ | 140/157 [06:17<00:46,  2.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 140/157 | Throughput: 11.9 samples/s | Peak VRAM: 5518 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|██████████| 157/157 [07:02<00:00,  2.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------\n",
      "Benchmark complete!\n",
      "Total samples processed: 5000\n",
      "Total time: 422.78 seconds\n",
      "Overall throughput: 11.83 samples/second\n",
      "Max peak VRAM observed: 5518 MB\n",
      "Average peak VRAM: 5490 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Main Benchmark Loop (Improved)\n",
    "\n",
    "# Results storage\n",
    "all_spanish = []\n",
    "all_english = []\n",
    "all_outputs = []\n",
    "all_ppls = []\n",
    "all_hit_max = []\n",
    "peak_vram_readings = []\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"STARTING BENCHMARK\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nConfiguration:\")\n",
    "print(f\"  Total samples: {len(torch_dataset)}\")\n",
    "print(f\"  Batch size: {BATCH_SIZE}\")\n",
    "print(f\"  Total batches: {len(dataloader)}\")\n",
    "print(f\"  Max new tokens: {MAX_NEW_TOKENS}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "# Warm-up run to stabilize GPU\n",
    "print(\"\\nWarming up GPU...\")\n",
    "warmup_texts = [\"Hola, ¿cómo estás? Esta es una prueba de traducción.\"] * 4\n",
    "_ = calculate_batch_perplexity(warmup_texts, model, tokenizer)\n",
    "_, _ = generate_translations(warmup_texts, model, tokenizer, max_new_tokens=32)\n",
    "torch.cuda.synchronize()\n",
    "print(\"Warm-up complete.\\n\")\n",
    "\n",
    "# Main benchmark\n",
    "start_time = time.perf_counter()\n",
    "\n",
    "for batch_idx, (spanish_batch, english_batch) in enumerate(tqdm(dataloader, desc=\"Processing batches\")):\n",
    "    # Convert tuple of strings to list\n",
    "    spanish_list = list(spanish_batch)\n",
    "    english_list = list(english_batch)\n",
    "    \n",
    "    # Reset memory tracking for this batch\n",
    "    reset_peak_memory_stats()\n",
    "    \n",
    "    # Metric 1: Calculate perplexity on Spanish input (batched)\n",
    "    batch_ppls = calculate_batch_perplexity(spanish_list, model, tokenizer)\n",
    "    \n",
    "    # Metric 2: Generate translations (batched) with max token tracking\n",
    "    batch_outputs, batch_hit_max = generate_translations(\n",
    "        spanish_list, model, tokenizer, max_new_tokens=MAX_NEW_TOKENS\n",
    "    )\n",
    "    \n",
    "    # Synchronize to ensure all GPU work is complete before measuring VRAM\n",
    "    torch.cuda.synchronize()\n",
    "    \n",
    "    # Log peak VRAM usage for this batch\n",
    "    batch_peak_vram = get_peak_vram_mb(gpu_handle)\n",
    "    peak_vram_readings.append(batch_peak_vram)\n",
    "    \n",
    "    # Accumulate results\n",
    "    all_spanish.extend(spanish_list)\n",
    "    all_english.extend(english_list)\n",
    "    all_outputs.extend(batch_outputs)\n",
    "    all_ppls.extend(batch_ppls)\n",
    "    all_hit_max.extend(batch_hit_max)\n",
    "    \n",
    "    # Progress logging every 20 batches\n",
    "    if (batch_idx + 1) % 20 == 0:\n",
    "        elapsed = time.perf_counter() - start_time\n",
    "        samples_done = (batch_idx + 1) * BATCH_SIZE\n",
    "        throughput = samples_done / elapsed\n",
    "        print(f\"  Batch {batch_idx + 1}/{len(dataloader)} | \"\n",
    "              f\"Throughput: {throughput:.1f} samples/s | \"\n",
    "              f\"Peak VRAM: {batch_peak_vram:.0f} MB\")\n",
    "\n",
    "end_time = time.perf_counter()\n",
    "total_time = end_time - start_time\n",
    "\n",
    "print(\"-\" * 70)\n",
    "print(f\"Benchmark complete!\")\n",
    "print(f\"Total samples processed: {len(all_spanish)}\")\n",
    "print(f\"Total time: {total_time:.2f} seconds\")\n",
    "print(f\"Overall throughput: {len(all_spanish) / total_time:.2f} samples/second\")\n",
    "print(f\"Max peak VRAM observed: {max(peak_vram_readings):.0f} MB\")\n",
    "print(f\"Average peak VRAM: {sum(peak_vram_readings) / len(peak_vram_readings):.0f} MB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to: pythia_1b_h100_baseline.csv\n",
      "\n",
      "======================================================================\n",
      "BENCHMARK SUMMARY\n",
      "======================================================================\n",
      "\n",
      "Model: EleutherAI/pythia-1b\n",
      "Dataset: alvations/globalvoices-en-es (filtered)\n",
      "Total Samples: 5000\n",
      "Batch Size: 32\n",
      "Max New Tokens: 128\n",
      "\n",
      "--- Performance Metrics ---\n",
      "Total Runtime: 422.78 seconds\n",
      "Throughput: 11.83 samples/second\n",
      "Average Time per Sample: 84.56 ms\n",
      "\n",
      "--- Token Count Statistics ---\n",
      "Metric                     Spanish Src  English Ref Model Output\n",
      "-----------------------------------------------------------------\n",
      "Mean tokens                       40.4         27.5         18.6\n",
      "Median tokens                     36.0         24.0         15.0\n",
      "Std tokens                        22.0         14.9         19.4\n",
      "Min tokens                           8            6            0\n",
      "Max tokens                         148          239          129\n",
      "\n",
      "--- Fertility (Tokens/Char) Statistics ---\n",
      "Metric                     Spanish Src  English Ref Model Output\n",
      "-----------------------------------------------------------------\n",
      "Mean fertility                  0.3766       0.2810       0.2058\n",
      "Median fertility                0.3699       0.2720       0.2472\n",
      "\n",
      "--- Perplexity Statistics ---\n",
      "Mean PPL: 63.31\n",
      "Median PPL: 33.00\n",
      "Std PPL: 147.00\n",
      "Min PPL: 4.72\n",
      "Max PPL: 5568.00\n",
      "\n",
      "--- Percentiles ---\n",
      "Percentile            PPL  Spanish Tok   Output Tok\n",
      "--------------------------------------------------\n",
      "P5                  13.56           14            0\n",
      "P10                 15.88           17            0\n",
      "P25                 21.38           23            0\n",
      "P50                 33.00           36           15\n",
      "P75                 60.00           52           27\n",
      "P90                115.50           70           39\n",
      "P95                191.00           82           48\n",
      "P99                520.00          112          128\n",
      "\n",
      "--- VRAM Usage ---\n",
      "Max Peak VRAM: 5518 MB\n",
      "Avg Peak VRAM: 5490 MB\n",
      "\n",
      "--- Failure Analysis ---\n",
      "Empty outputs: 1283 (25.7%)\n",
      "Repetitive outputs: 51 (1.0%)\n",
      "Spanish word leak: 273 (5.5%)\n",
      "Too short (<30% ref): 1316 (26.3%)\n",
      "Too long (>300% ref): 38 (0.8%)\n",
      "Hit max tokens (128): 5000 (100.0%)\n",
      "\n",
      "--- Length Ratio Statistics (output/reference) ---\n",
      "Mean: 0.73\n",
      "Median: 0.84\n",
      "Std: 0.72\n",
      "\n",
      "======================================================================\n",
      "SAMPLE OUTPUTS\n",
      "======================================================================\n",
      "\n",
      "--- BEST CASES (Lowest PPL, good length ratio 0.5-2.0) ---\n",
      "\n",
      "[Best #1] (PPL: 6.19, Out Tokens: 51, Ref Tokens: 44, Ratio: 1.16)\n",
      "  Spanish:     Los países del sudeste asiático pueden ser muy prósperos económicamente, pero la región debe trabajar en conjunto para a...\n",
      "  English ref: The countries in South Asia may be thriving economically, but the region must work together to tackle the problems of po...\n",
      "  Model out:   The countries of the South Asian region can be very prosperous economically, but the region must work together to tackle the challenges of poverty, gender inequality and climate change, said the exper...\n",
      "\n",
      "[Best #2] (PPL: 7.16, Out Tokens: 34, Ref Tokens: 40, Ratio: 0.85)\n",
      "  Spanish:     La reunión de los presidentes latinoamericanos de la Cumbre de Río, que tuvo lugar en la ciudad capital de la República ...\n",
      "  English ref: The gathering of Latin American presidents from the Rio Summit, which took place in the capital city of the Dominican Re...\n",
      "  Model out:   The Latin American Leaders' Summit, which took place in the capital of the Dominican Republic, has become one of the most important events in the history of the continent.\n",
      "\n",
      "[Best #3] (PPL: 7.38, Out Tokens: 61, Ref Tokens: 63, Ratio: 0.97)\n",
      "  Spanish:     Dentro de las recomendaciones de la Organización Internacional del Trabajo acerca del VIH/SIDA en el entorno de trabajo,...\n",
      "  English ref: Within the recommendations of the International Labor Organization regarding HIV/AIDS in the work environment, it is sta...\n",
      "  Model out:   \"The World Health Organization (WHO) has issued a statement on the HIV/AIDS epidemic in the workplace, which states that there is no profession in which people with HIV should be excluded, but in each...\n",
      "\n",
      "[Best #4] (PPL: 7.50, Out Tokens: 37, Ref Tokens: 32, Ratio: 1.16)\n",
      "  Spanish:     Los destinos más atractivos son los Países Bajos (15,3%), Suiza (10,6%), Islas Vírgenes Británicas (8,3%), EEUU (7,3%). \n",
      "  English ref: The most attractive destinations are Netherlands (15,3%), Switzerland (10,6%), British Virgin Islands (8,3%), USA (7,3%)...\n",
      "  Model out:   The most attractive destinations are the Netherlands (15,3%), the United Kingdom (10,6%), the United States (7,3%), and the United States (7,3%).\n",
      "\n",
      "[Best #5] (PPL: 7.62, Out Tokens: 62, Ref Tokens: 81, Ratio: 0.77)\n",
      "  Spanish:     Hace unas semanas, la Gazeta Wyborcza, uno de los diarios más importantes de Polonia, lanzó una nueva campaña social (Le...\n",
      "  English ref: A few weeks ago, a new social campaign - Reading in Poland - was launched by one of Poland's largest daily newspapers, G...\n",
      "  Model out:   The Gazette of Poland, one of the most important newspapers in Poland, published a new social campaign (Read in Poland) due to the low reading levels in the country: according to a report published by...\n",
      "\n",
      "--- WORST CASES (Highest PPL) ---\n",
      "\n",
      "[Worst #1] (PPL: 5568.00, Out Tokens: 9, Ref Tokens: 9, Ratio: 1.00)\n",
      "  Spanish:     Imagen tomada del blog Lokmet Eish \n",
      "  English ref: Image taken from Lokmet Eish blog \n",
      "  Model out:   Image taken from the blog Lokmet Eish\n",
      "\n",
      "[Worst #2] (PPL: 4928.00, Out Tokens: 6, Ref Tokens: 10, Ratio: 0.60)\n",
      "  Spanish:     kid5rivers lo resume diciendo : \n",
      "  English ref: kid5rivers sums it up by saying: \n",
      "  Model out:   kid5rivers resume saying :\n",
      "\n",
      "[Worst #3] (PPL: 2048.00, Out Tokens: 9, Ref Tokens: 12, Ratio: 0.75)\n",
      "  Spanish:     Campaña online de Obama divide a bloggers etíopes \n",
      "  English ref: Online Obama campaign splits Ethiopian bloggers · Global Voices \n",
      "  Model out:   Campaign for Obama divide bloggers etíopes\n",
      "\n",
      "[Worst #4] (PPL: 2048.00, Out Tokens: 11, Ref Tokens: 12, Ratio: 0.92)\n",
      "  Spanish:     Para Xica del blog La Alharaca: \n",
      "  English ref: For Xica of the blog La Alharaca : \n",
      "  Model out:   For Xica of the blog La Alharaca:\n",
      "\n",
      "[Worst #5] (PPL: 1544.00, Out Tokens: 11, Ref Tokens: 12, Ratio: 0.92)\n",
      "  Spanish:     Peter en Holties House, ‘All About Australia. \n",
      "  English ref: Peter at Holties House, ‘All About Australia. \n",
      "  Model out:   Peter in Holties House, ‘All About Australia.\n",
      "\n",
      "--- EMPTY OUTPUT FAILURES ---\n",
      "\n",
      "[Empty] (PPL: 32.50, Out Tokens: 0, Ref Tokens: 27, Ratio: 0.00)\n",
      "  Spanish:     700 trabajadores fueron instados por Huawei Technologies a renunciar para que la empresa evitara firmar contratos indefi...\n",
      "  English ref: 7000 employees were urged by the Huawei Technologies to resign so that the company could avoid signing open-ended contra...\n",
      "  Model out:   \n",
      "\n",
      "[Empty] (PPL: 38.75, Out Tokens: 0, Ref Tokens: 17, Ratio: 0.00)\n",
      "  Spanish:     Las calles estaban vacías, y parecía como su hubiera caído un terrible destino. \n",
      "  English ref: The streets became empty, and it was as if some terrible fate had fallen. \n",
      "  Model out:   \n",
      "\n",
      "[Empty] (PPL: 28.38, Out Tokens: 0, Ref Tokens: 66, Ratio: 0.00)\n",
      "  Spanish:     Antes de eso, le dijo a algunas personas que en Cerro Chalpón, Cerro Rajado y Cerro Penachí, había dejado enormes cruces...\n",
      "  English ref: Previously, he had let some people know that in Cerro Chalpón, Cerro Rajado and Cerro Penachí, he had left behind huge c...\n",
      "  Model out:   \n",
      "\n",
      "--- REPETITIVE OUTPUT FAILURES ---\n",
      "\n",
      "[Repetitive] (PPL: 378.00, Out Tokens: 128, Ref Tokens: 14, Ratio: 9.14)\n",
      "  Spanish:     Y Tom Gara saltó en defensa de Ghonim diciendo: \n",
      "  English ref: And Tom Gara comes to Ghonim's defense saying: \n",
      "  Model out:   \"I am Ghonim, the son of Ghayim, the son of Ghayim, the son of Ghayim, the son of Ghayim, the son of Ghayim, the son of Ghayim, the son of Ghayim, the son of Ghayim, the son of Ghayim, the son of Ghay...\n",
      "\n",
      "[Repetitive] (PPL: 21.00, Out Tokens: 128, Ref Tokens: 20, Ratio: 6.40)\n",
      "  Spanish:     Se trata de innovación, entendimiento y conocimiento y cómo la creatividad es el fundamento de todas las nuevas ideas. \n",
      "  English ref: It's about innovation, understanding and knowledge and how creativity is the foundation of all new thoughts. \n",
      "  Model out:   The aim of this book is to give you a clear and concise overview of the subject of creativity. It is a book about the creative process, the creative mind, and the creative spirit. It is a book about t...\n",
      "\n",
      "[Repetitive] (PPL: 46.00, Out Tokens: 128, Ref Tokens: 11, Ratio: 11.64)\n",
      "  Spanish:     Otro año casi se acaba, así que vamos, divirtámonos un poco. \n",
      "  English ref: The year is going, let him go； \n",
      "  Model out:   Another year almost gone, so let's, let's, let's, let's, let's, let's, let's, let's, let's, let's, let's, let's, let's, let's, let's, let's, let's, let's, let's, let's, let's, let's, let's, let's, let...\n",
      "\n",
      "--- SPANISH WORD LEAK (untranslated Spanish in output) ---\n",
      "\n",
      "[SpanishLeak] (PPL: 54.50, Out Tokens: 69, Ref Tokens: 55, Ratio: 1.25)\n",
      "  Spanish:     ISRO (ing), que constituye la espina dorsal, ofrece conectividad por satélite tal como GRAMSAT (ing) El Ministerio de sa...\n",
      "  English ref: ISRO, offering connectivity backbone through satcom applications such as GRAMSAT The Ministry of Health & Family welfare...\n",
      "  Model out:   ISRO (ing), which constitutes the dorsal spine, provides connectivity by satellite as GRAMSAT (ing) The Ministry of health and family and its rural telemedicine service work under the supervision of t...\n",
      "\n",
      "[SpanishLeak] (PPL: 32.50, Out Tokens: 57, Ref Tokens: 45, Ratio: 1.27)\n",
      "  Spanish:     PsicotravelMx (@PsicotravelMx) recordó que no sólo los capitalinos, sino aquellos que dejan de recibir la educación que ...\n",
      "  English ref: PsicotravelMx (@PsicotravelMx) remembered that not only those who live in the capital, but those who fail to receive the...\n",
      "  Model out:   PsicotravelMx (@PsicotravelMx) recordó que no sólo los capitalinos, sino aquellos que dejan de recibir la educación que los agremiados de la CNTE deberían impartir, are also affected:\n",
      "\n",
      "[SpanishLeak] (PPL: 112.00, Out Tokens: 17, Ref Tokens: 15, Ratio: 1.13)\n",
      "  Spanish:     “Amenaza de devaluación” - crisis financiera en EEUU \n",
      "  English ref: \"Threat of devaluation\" - financial crisis in the USA \n",
      "  Model out:   “Amenaza de devaluación” - crisis financiera en EEUU\n",
      "\n",
      "--- TOO SHORT OUTPUTS (<30% of reference length) ---\n",
      "\n",
      "[TooShort] (PPL: 32.50, Out Tokens: 0, Ref Tokens: 27, Ratio: 0.00)\n",
      "  Spanish:     700 trabajadores fueron instados por Huawei Technologies a renunciar para que la empresa evitara firmar contratos indefi...\n",
      "  English ref: 7000 employees were urged by the Huawei Technologies to resign so that the company could avoid signing open-ended contra...\n",
      "  Model out:   \n",
      "\n",
      "[TooShort] (PPL: 38.75, Out Tokens: 0, Ref Tokens: 17, Ratio: 0.00)\n",
      "  Spanish:     Las calles estaban vacías, y parecía como su hubiera caído un terrible destino. \n",
      "  English ref: The streets became empty, and it was as if some terrible fate had fallen. \n",
      "  Model out:   \n",
      "\n",
      "[TooShort] (PPL: 28.38, Out Tokens: 0, Ref Tokens: 66, Ratio: 0.00)\n",
      "  Spanish:     Antes de eso, le dijo a algunas personas que en Cerro Chalpón, Cerro Rajado y Cerro Penachí, había dejado enormes cruces...\n",
      "  English ref: Previously, he had let some people know that in Cerro Chalpón, Cerro Rajado and Cerro Penachí, he had left behind huge c...\n",
      "  Model out:   \n",
      "\n",
      "--- TOO LONG OUTPUTS (>300% of reference length) ---\n",
      "\n",
      "[TooLong] (PPL: 378.00, Out Tokens: 128, Ref Tokens: 14, Ratio: 9.14)\n",
      "  Spanish:     Y Tom Gara saltó en defensa de Ghonim diciendo: \n",
      "  English ref: And Tom Gara comes to Ghonim's defense saying: \n",
      "  Model out:   \"I am Ghonim, the son of Ghayim, the son of Ghayim, the son of Ghayim, the son of Ghayim, the son of Ghayim, the son of Ghayim, the son of Ghayim, the son of Ghayim, the son of Ghayim, the son of Ghay...\n",
      "\n",
      "[TooLong] (PPL: 21.00, Out Tokens: 128, Ref Tokens: 20, Ratio: 6.40)\n",
      "  Spanish:     Se trata de innovación, entendimiento y conocimiento y cómo la creatividad es el fundamento de todas las nuevas ideas. \n",
      "  English ref: It's about innovation, understanding and knowledge and how creativity is the foundation of all new thoughts. \n",
      "  Model out:   The aim of this book is to give you a clear and concise overview of the subject of creativity. It is a book about the creative process, the creative mind, and the creative spirit. It is a book about t...\n",
      "\n",
      "[TooLong] (PPL: 46.00, Out Tokens: 128, Ref Tokens: 11, Ratio: 11.64)\n",
      "  Spanish:     Otro año casi se acaba, así que vamos, divirtámonos un poco. \n",
      "  English ref: The year is going, let him go； \n",
      "  Model out:   Another year almost gone, so let's, let's, let's, let's, let's, let's, let's, let's, let's, let's, let's, let's, let's, let's, let's, let's, let's, let's, let's, let's, let's, let's, let's, let's, let...\n",
      "\n",
      "--- RANDOM SAMPLES ---\n",
      "\n",
      "[Random] (PPL: 25.38, Out Tokens: 22, Ref Tokens: 21, Ratio: 1.05)\n",
      "  Spanish:     Algunos crearon una cuenta falsa de Twitter para el embajador estadounidense en #Russia, y difundieron rumores de fraude...\n",
      "  English ref: Some created fake twitter account for U.S. Ambassador to #Russia, spreading rumors of fraud. \n",
      "  Model out:   Some created a fake Twitter account for the U.S. Ambassador to Russia, and spread rumors of fraud.\n",
      "\n",
      "[Random] (PPL: 29.62, Out Tokens: 37, Ref Tokens: 41, Ratio: 0.90)\n",
      "  Spanish:     Al momento de escribir y de acuerdo a la herramienta de alcance de publicidad de Facebook, Armenia tiene 106,500 usuario...\n",
      "  English ref: At time of writing, and according to Facebook's own advertising reach tool, Armenia has 106,500 users on the social netw...\n",
      "  Model out:   At the moment of writing and according to the Facebook advertising platform, Armenia has 106,500 users on the social network, Azerbaijan 276,680, and Georgia 425,540.\n",
      "\n",
      "[Random] (PPL: 25.00, Out Tokens: 27, Ref Tokens: 28, Ratio: 0.96)\n",
      "  Spanish:     Las fuentes confirmarán tan pronto como terminen la investigación, y si tales pruebas son viables, serán presentadas al ...\n",
      "  English ref: Sources will confirm as soon as the investigation process has ended, and if such proof is viable, it will be presented t...\n",
      "  Model out:   The sources will confirm as soon as they finish the investigation, and if such tests are valid, they will be presented to the public.\n",
      "\n",
      "[Random] (PPL: 131.00, Out Tokens: 12, Ref Tokens: 10, Ratio: 1.20)\n",
      "  Spanish:     El periodista Tom Finn estaba en el lugar de los hechos: \n",
      "  English ref: Journalist Tom Finn was at the scene: \n",
      "  Model out:   The reporter Tom Finn was in the place of the events:\n",
      "\n",
      "[Random] (PPL: 29.25, Out Tokens: 0, Ref Tokens: 57, Ratio: 0.00)\n",
      "  Spanish:     Presenta tres prominentes mujeres israelíes en posiciones de poder para ilustrar a nuestra libre y equitativa sociedad, ...\n",
      "  English ref: It features three prominent Israeli Women in positions of power to illustrate our free, equal society where Women are ge...\n",
      "  Model out:   \n",
      "\n",
      "======================================================================\n",
      "Benchmark complete. Results saved to pythia_1b_h100_baseline.csv\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Output and Reporting with Token Analysis\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Create results DataFrame\n",
    "results_df = pd.DataFrame({\n",
    "    \"spanish_src\": all_spanish,\n",
    "    \"english_ref\": all_english,\n",
    "    \"model_output\": all_outputs,\n",
    "    \"input_ppl\": all_ppls,\n",
    "    \"hit_max_tokens\": all_hit_max\n",
    "})\n",
    "\n",
    "# --- Token Count Analysis ---\n",
    "def count_tokens(text):\n",
    "    \"\"\"Count tokens using the model's tokenizer.\"\"\"\n",
    "    if not text:\n",
    "        return 0\n",
    "    return len(tokenizer.encode(text, add_special_tokens=False))\n",
    "\n",
    "def count_chars(text):\n",
    "    \"\"\"Count characters (excluding whitespace for fertility calc).\"\"\"\n",
    "    return len(text.replace(\" \", \"\"))\n",
    "\n",
    "# Calculate token counts and character counts\n",
    "results_df[\"spanish_tokens\"] = results_df[\"spanish_src\"].apply(count_tokens)\n",
    "results_df[\"english_ref_tokens\"] = results_df[\"english_ref\"].apply(count_tokens)\n",
    "results_df[\"output_tokens\"] = results_df[\"model_output\"].apply(count_tokens)\n",
    "\n",
    "results_df[\"spanish_chars\"] = results_df[\"spanish_src\"].apply(count_chars)\n",
    "results_df[\"english_ref_chars\"] = results_df[\"english_ref\"].apply(count_chars)\n",
    "results_df[\"output_chars\"] = results_df[\"model_output\"].apply(count_chars)\n",
    "\n",
    "# Calculate Fertility (Tokens per Character)\n",
    "results_df[\"spanish_fertility\"] = results_df[\"spanish_tokens\"] / results_df[\"spanish_chars\"].replace(0, 1)\n",
    "results_df[\"english_ref_fertility\"] = results_df[\"english_ref_tokens\"] / results_df[\"english_ref_chars\"].replace(0, 1)\n",
    "results_df[\"output_fertility\"] = results_df[\"output_tokens\"] / results_df[\"output_chars\"].replace(0, 1)\n",
    "\n",
    "# Detect failure cases\n",
    "results_df[\"is_empty\"] = results_df[\"model_output\"].str.strip() == \"\"\n",
    "results_df[\"is_repetitive\"] = results_df[\"model_output\"].apply(\n",
    "    lambda x: len(set(x.split())) < len(x.split()) * 0.4 if len(x.split()) > 5 else False\n",
    ")\n",
    "# Check if output contains Spanish words from input (translation failure)\n",
    "results_df[\"has_spanish_leak\"] = results_df.apply(\n",
    "    lambda row: any(\n",
    "        word.lower() in row[\"model_output\"].lower() \n",
    "        for word in row[\"spanish_src\"].split()[:3] \n",
    "        if len(word) > 4 and word.lower() not in row[\"english_ref\"].lower()\n",
    "    ) if len(row[\"model_output\"]) > 0 else False,\n",
    "    axis=1\n",
    ")\n",
    "# Check output length ratio (too short or too long compared to reference)\n",
    "results_df[\"length_ratio\"] = results_df[\"output_tokens\"] / results_df[\"english_ref_tokens\"].replace(0, 1)\n",
    "results_df[\"is_too_short\"] = results_df[\"length_ratio\"] < 0.3\n",
    "results_df[\"is_too_long\"] = results_df[\"length_ratio\"] > 3.0\n",
    "\n",
    "# Save to CSV\n",
    "output_file = \"pythia_1b_h100_baseline.csv\"\n",
    "results_df.to_csv(output_file, index=False)\n",
    "print(f\"Results saved to: {output_file}\")\n",
    "\n",
    "# Display summary statistics\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"BENCHMARK SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nModel: {MODEL_NAME}\")\n",
    "print(f\"Dataset: alvations/globalvoices-en-es (filtered)\")\n",
    "print(f\"Total Samples: {len(results_df)}\")\n",
    "print(f\"Batch Size: {BATCH_SIZE}\")\n",
    "print(f\"Max New Tokens: {MAX_NEW_TOKENS}\")\n",
    "\n",
    "print(f\"\\n--- Performance Metrics ---\")\n",
    "print(f\"Total Runtime: {total_time:.2f} seconds\")\n",
    "print(f\"Throughput: {len(results_df) / total_time:.2f} samples/second\")\n",
    "print(f\"Average Time per Sample: {total_time / len(results_df) * 1000:.2f} ms\")\n",
    "\n",
    "print(f\"\\n--- Token Count Statistics ---\")\n",
    "print(f\"{'Metric':<25} {'Spanish Src':>12} {'English Ref':>12} {'Model Output':>12}\")\n",
    "print(\"-\" * 65)\n",
    "print(f\"{'Mean tokens':<25} {results_df['spanish_tokens'].mean():>12.1f} {results_df['english_ref_tokens'].mean():>12.1f} {results_df['output_tokens'].mean():>12.1f}\")\n",
    "print(f\"{'Median tokens':<25} {results_df['spanish_tokens'].median():>12.1f} {results_df['english_ref_tokens'].median():>12.1f} {results_df['output_tokens'].median():>12.1f}\")\n",
    "print(f\"{'Std tokens':<25} {results_df['spanish_tokens'].std():>12.1f} {results_df['english_ref_tokens'].std():>12.1f} {results_df['output_tokens'].std():>12.1f}\")\n",
    "print(f\"{'Min tokens':<25} {results_df['spanish_tokens'].min():>12} {results_df['english_ref_tokens'].min():>12} {results_df['output_tokens'].min():>12}\")\n",
    "print(f\"{'Max tokens':<25} {results_df['spanish_tokens'].max():>12} {results_df['english_ref_tokens'].max():>12} {results_df['output_tokens'].max():>12}\")\n",
    "\n",
    "print(f\"\\n--- Fertility (Tokens/Char) Statistics ---\")\n",
    "print(f\"{'Metric':<25} {'Spanish Src':>12} {'English Ref':>12} {'Model Output':>12}\")\n",
    "print(\"-\" * 65)\n",
    "print(f\"{'Mean fertility':<25} {results_df['spanish_fertility'].mean():>12.4f} {results_df['english_ref_fertility'].mean():>12.4f} {results_df['output_fertility'].mean():>12.4f}\")\n",
    "print(f\"{'Median fertility':<25} {results_df['spanish_fertility'].median():>12.4f} {results_df['english_ref_fertility'].median():>12.4f} {results_df['output_fertility'].median():>12.4f}\")\n",
    "\n",
    "print(f\"\\n--- Perplexity Statistics ---\")\n",
    "print(f\"Mean PPL: {results_df['input_ppl'].mean():.2f}\")\n",
    "print(f\"Median PPL: {results_df['input_ppl'].median():.2f}\")\n",
    "print(f\"Std PPL: {results_df['input_ppl'].std():.2f}\")\n",
    "print(f\"Min PPL: {results_df['input_ppl'].min():.2f}\")\n",
    "print(f\"Max PPL: {results_df['input_ppl'].max():.2f}\")\n",
    "\n",
    "print(f\"\\n--- Percentiles ---\")\n",
    "percentiles = [5, 10, 25, 50, 75, 90, 95, 99]\n",
    "print(f\"{'Percentile':<12} {'PPL':>12} {'Spanish Tok':>12} {'Output Tok':>12}\")\n",
    "print(\"-\" * 50)\n",
    "for p in percentiles:\n",
    "    ppl_p = np.percentile(results_df['input_ppl'], p)\n",
    "    src_tok_p = np.percentile(results_df['spanish_tokens'], p)\n",
    "    out_tok_p = np.percentile(results_df['output_tokens'], p)\n",
    "    print(f\"{'P' + str(p):<12} {ppl_p:>12.2f} {src_tok_p:>12.0f} {out_tok_p:>12.0f}\")\n",
    "\n",
    "print(f\"\\n--- VRAM Usage ---\")\n",
    "print(f\"Max Peak VRAM: {max(peak_vram_readings):.0f} MB\")\n",
    "print(f\"Avg Peak VRAM: {sum(peak_vram_readings) / len(peak_vram_readings):.0f} MB\")\n",
    "\n",
    "print(f\"\\n--- Failure Analysis ---\")\n",
    "empty_count = results_df[\"is_empty\"].sum()\n",
    "repetitive_count = results_df[\"is_repetitive\"].sum()\n",
    "spanish_leak_count = results_df[\"has_spanish_leak\"].sum()\n",
    "too_short_count = results_df[\"is_too_short\"].sum()\n",
    "too_long_count = results_df[\"is_too_long\"].sum()\n",
    "hit_max_count = results_df[\"hit_max_tokens\"].sum()\n",
    "\n",
    "print(f\"Empty outputs: {empty_count} ({100*empty_count/len(results_df):.1f}%)\")\n",
    "print(f\"Repetitive outputs: {repetitive_count} ({100*repetitive_count/len(results_df):.1f}%)\")\n",
    "print(f\"Spanish word leak: {spanish_leak_count} ({100*spanish_leak_count/len(results_df):.1f}%)\")\n",
    "print(f\"Too short (<30% ref): {too_short_count} ({100*too_short_count/len(results_df):.1f}%)\")\n",
    "print(f\"Too long (>300% ref): {too_long_count} ({100*too_long_count/len(results_df):.1f}%)\")\n",
    "print(f\"Hit max tokens ({MAX_NEW_TOKENS}): {hit_max_count} ({100*hit_max_count/len(results_df):.1f}%)\")\n",
    "\n",
    "print(f\"\\n--- Length Ratio Statistics (output/reference) ---\")\n",
    "print(f\"Mean: {results_df['length_ratio'].mean():.2f}\")\n",
    "print(f\"Median: {results_df['length_ratio'].median():.2f}\")\n",
    "print(f\"Std: {results_df['length_ratio'].std():.2f}\")\n",
    "\n",
    "# --- Sampled Outputs: Good, Bad, and Ugly ---\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"SAMPLE OUTPUTS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "def print_sample(row, label):\n",
    "    ratio_str = f\", Ratio: {row['length_ratio']:.2f}\" if 'length_ratio' in row else \"\"\n",
    "    print(f\"\\n[{label}] (PPL: {row['input_ppl']:.2f}, Out Tokens: {row['output_tokens']}, Ref Tokens: {row['english_ref_tokens']}{ratio_str})\")\n",
    "    print(f\"  Spanish:     {row['spanish_src'][:120]}{'...' if len(row['spanish_src']) > 120 else ''}\")\n",
    "    print(f\"  English ref: {row['english_ref'][:120]}{'...' if len(row['english_ref']) > 120 else ''}\")\n",
    "    print(f\"  Model out:   {row['model_output'][:200]}{'...' if len(row['model_output']) > 200 else ''}\")\n",
    "\n",
    "# Best cases (lowest PPL, good length ratio)\n",
    "print(\"\\n--- BEST CASES (Lowest PPL, good length ratio 0.5-2.0) ---\")\n",
    "good_ratio = (results_df[\"length_ratio\"] >= 0.5) & (results_df[\"length_ratio\"] <= 2.0)\n",
    "best_df = results_df[(~results_df[\"is_empty\"]) & good_ratio].nsmallest(5, \"input_ppl\")\n",
    "for idx, row in best_df.iterrows():\n",
    "    print_sample(row, f\"Best #{list(best_df.index).index(idx)+1}\")\n",
    "\n",
    "# Worst cases (highest PPL)\n",
    "print(\"\\n--- WORST CASES (Highest PPL) ---\")\n",
    "worst_df = results_df.nlargest(5, \"input_ppl\")\n",
    "for idx, row in worst_df.iterrows():\n",
    "    print_sample(row, f\"Worst #{list(worst_df.index).index(idx)+1}\")\n",
    "\n",
    "# Empty outputs\n",
    "if empty_count > 0:\n",
    "    print(\"\\n--- EMPTY OUTPUT FAILURES ---\")\n",
    "    empty_df = results_df[results_df[\"is_empty\"]].head(3)\n",
    "    for idx, row in empty_df.iterrows():\n",
    "        print_sample(row, \"Empty\")\n",
    "\n",
    "# Repetitive outputs\n",
    "if repetitive_count > 0:\n",
    "    print(\"\\n--- REPETITIVE OUTPUT FAILURES ---\")\n",
    "    rep_df = results_df[results_df[\"is_repetitive\"]].head(3)\n",
    "    for idx, row in rep_df.iterrows():\n",
    "        print_sample(row, \"Repetitive\")\n",
    "\n",
    "# Spanish word leak\n",
    "if spanish_leak_count > 0:\n",
    "    print(\"\\n--- SPANISH WORD LEAK (untranslated Spanish in output) ---\")\n",
    "    leak_df = results_df[results_df[\"has_spanish_leak\"]].head(3)\n",
    "    for idx, row in leak_df.iterrows():\n",
    "        print_sample(row, \"SpanishLeak\")\n",
    "\n",
    "# Too short outputs\n",
    "if too_short_count > 0:\n",
    "    print(\"\\n--- TOO SHORT OUTPUTS (<30% of reference length) ---\")\n",
    "    short_df = results_df[results_df[\"is_too_short\"]].head(3)\n",
    "    for idx, row in short_df.iterrows():\n",
    "        print_sample(row, \"TooShort\")\n",
    "\n",
    "# Too long outputs  \n",
    "if too_long_count > 0:\n",
    "    print(\"\\n--- TOO LONG OUTPUTS (>300% of reference length) ---\")\n",
    "    long_df = results_df[results_df[\"is_too_long\"]].head(3)\n",
    "    for idx, row in long_df.iterrows():\n",
    "        print_sample(row, \"TooLong\")\n",
    "\n",
    "# Random sample for diversity\n",
    "print(\"\\n--- RANDOM SAMPLES ---\")\n",
    "random_df = results_df.sample(n=min(5, len(results_df)), random_state=42)\n",
    "for idx, row in random_df.iterrows():\n",
    "    print_sample(row, f\"Random\")\n",
    "\n",
    "# Cleanup pynvml\n",
    "pynvml.nvmlShutdown()\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Benchmark complete. Results saved to\", output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
