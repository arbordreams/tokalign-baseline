{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer Efficiency Analysis\n",
    "\n",
    "**TokAlign ACL Publication-Grade Evaluation Framework - Notebook 1/3**\n",
    "\n",
    "This notebook performs tokenizer-level analysis comparing baseline and adapted tokenizers.\n",
    "No model inference is required - pure tokenizer metrics only.\n",
    "\n",
    "## Metrics (per CVA study + STRR paper)\n",
    "\n",
    "| Metric | Formula | Target |\n",
    "|--------|---------|--------|\n",
    "| **Fertility** | tokens / words | Spanish approaching English ~1.4 |\n",
    "| **Compression Ratio** | bytes / tokens | Higher = better |\n",
    "| **PCW** | % words split into 2+ subwords | Lower = better |\n",
    "| **UNK Rate** | % unknown tokens | Should approach 0% |\n",
    "| **STRR** | % words preserved as single tokens | Higher = better |\n",
    "\n",
    "## References\n",
    "- Trans-Tokenization (Remy et al., 2024)\n",
    "- CVA Study (Yamaguchi et al., EMNLP 2024) - 271.5% inference speedup\n",
    "- STRR Metric (Nayeem et al., 2025) - Single Token Retention Rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T00:31:48.687329Z",
     "iopub.status.busy": "2025-11-29T00:31:48.687198Z",
     "iopub.status.idle": "2025-11-29T00:31:48.692536Z",
     "shell.execute_reply": "2025-11-29T00:31:48.692089Z"
    }
   },
   "outputs": [],
   "source": [
    "# Cell 1: Configuration\n",
    "# =====================\n",
    "\n",
    "# === MODEL PATHS ===\n",
    "BASELINE_MODEL = \"EleutherAI/pythia-1b\"\n",
    "ADAPTED_MODEL = \"/home/ubuntu/aryan-true-tokalign/true-tokalign/log/1b/0_qwen2-7b_S2/checkpoint-2500\"\n",
    "\n",
    "# === DATASET CONFIGURATION ===\n",
    "WIKIPEDIA_SAMPLES = 10_000\n",
    "OSCAR_SAMPLES = 10_000\n",
    "\n",
    "# === OUTPUT FILES ===\n",
    "OUTPUT_DIR = \"results\"\n",
    "BASELINE_OUTPUT = f\"{OUTPUT_DIR}/tokenizer_analysis_baseline.csv\"\n",
    "ADAPTED_OUTPUT = f\"{OUTPUT_DIR}/tokenizer_analysis_adapted.csv\"\n",
    "COMPARISON_OUTPUT = f\"{OUTPUT_DIR}/tokenizer_comparison.csv\"\n",
    "\n",
    "# === STATISTICAL ANALYSIS ===\n",
    "BOOTSTRAP_ITERATIONS = 1000\n",
    "CONFIDENCE_LEVEL = 0.95\n",
    "RANDOM_SEED = 42\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T00:31:48.693839Z",
     "iopub.status.busy": "2025-11-29T00:31:48.693683Z",
     "iopub.status.idle": "2025-11-29T00:31:50.924398Z",
     "shell.execute_reply": "2025-11-29T00:31:50.923696Z"
    }
   },
   "outputs": [],
   "source": [
    "# Cell 2: Install Dependencies\n",
    "!pip install transformers datasets pandas numpy scipy tqdm -q\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T00:31:50.926348Z",
     "iopub.status.busy": "2025-11-29T00:31:50.926175Z",
     "iopub.status.idle": "2025-11-29T00:31:54.450334Z",
     "shell.execute_reply": "2025-11-29T00:31:54.449751Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
      "  import pynvml  # type: ignore[import]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports completed successfully.\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Imports\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "from tqdm.auto import tqdm\n",
    "from typing import Dict, List, Tuple\n",
    "from dataclasses import dataclass\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "print(\"Imports completed successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T00:31:54.452226Z",
     "iopub.status.busy": "2025-11-29T00:31:54.451897Z",
     "iopub.status.idle": "2025-11-29T00:31:54.461052Z",
     "shell.execute_reply": "2025-11-29T00:31:54.460526Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer metrics functions defined (TRUE PARALLEL mode, 26 workers).\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Tokenizer Metrics Functions (TRUE PARALLEL with datasets.map)\n",
    "# =====================================================================\n",
    "# Using HuggingFace datasets.map() with num_proc for true multiprocessing\n",
    "# Each worker loads its own tokenizer to avoid pickling issues\n",
    "\n",
    "from datasets import Dataset\n",
    "import multiprocessing as mp\n",
    "\n",
    "NUM_PROC = mp.cpu_count() // 2  # Use 26 physical cores (avoid hyperthreading overhead)\n",
    "\n",
    "# Global tokenizer cache per worker (loaded once per process)\n",
    "_WORKER_TOKENIZER = None\n",
    "_WORKER_TOKENIZER_PATH = None\n",
    "\n",
    "def _get_tokenizer(tokenizer_path: str):\n",
    "    \"\"\"Get or load tokenizer for current worker process.\"\"\"\n",
    "    global _WORKER_TOKENIZER, _WORKER_TOKENIZER_PATH\n",
    "    if _WORKER_TOKENIZER is None or _WORKER_TOKENIZER_PATH != tokenizer_path:\n",
    "        from transformers import AutoTokenizer\n",
    "        _WORKER_TOKENIZER = AutoTokenizer.from_pretrained(tokenizer_path)\n",
    "        _WORKER_TOKENIZER_PATH = tokenizer_path\n",
    "    return _WORKER_TOKENIZER\n",
    "\n",
    "\n",
    "def compute_metrics_for_map(examples, tokenizer_path: str):\n",
    "    \"\"\"\n",
    "    Compute tokenizer metrics for a batch - designed for datasets.map().\n",
    "    Each worker loads its own tokenizer to avoid pickling issues.\n",
    "    \"\"\"\n",
    "    tokenizer = _get_tokenizer(tokenizer_path)\n",
    "    \n",
    "    texts = examples['text']\n",
    "    \n",
    "    # Batch tokenize all texts at once (uses Rust backend - fast!)\n",
    "    all_encodings = tokenizer(texts, add_special_tokens=False)['input_ids']\n",
    "    \n",
    "    unk_id = tokenizer.unk_token_id\n",
    "    \n",
    "    # Pre-allocate result lists\n",
    "    num_words_list = []\n",
    "    num_tokens_list = []\n",
    "    num_bytes_list = []\n",
    "    num_unk_list = []\n",
    "    num_continued_list = []\n",
    "    num_single_list = []\n",
    "    fertility_list = []\n",
    "    compression_list = []\n",
    "    pcw_list = []\n",
    "    unk_rate_list = []\n",
    "    strr_list = []\n",
    "    \n",
    "    for text, token_ids in zip(texts, all_encodings):\n",
    "        words = text.split() if text else []\n",
    "        num_words = len(words)\n",
    "        num_bytes = len(text.encode('utf-8')) if text else 0\n",
    "        num_tokens = len(token_ids)\n",
    "        num_unk = sum(1 for t in token_ids if t == unk_id) if unk_id else 0\n",
    "        \n",
    "        # Word-level analysis: batch encode all words at once for speed\n",
    "        if words:\n",
    "            word_encodings = tokenizer(words, add_special_tokens=False)['input_ids']\n",
    "            num_single = sum(1 for enc in word_encodings if len(enc) == 1)\n",
    "            num_continued = sum(1 for enc in word_encodings if len(enc) > 1)\n",
    "        else:\n",
    "            num_single = 0\n",
    "            num_continued = 0\n",
    "        \n",
    "        num_words_list.append(num_words)\n",
    "        num_tokens_list.append(num_tokens)\n",
    "        num_bytes_list.append(num_bytes)\n",
    "        num_unk_list.append(num_unk)\n",
    "        num_continued_list.append(num_continued)\n",
    "        num_single_list.append(num_single)\n",
    "        fertility_list.append(num_tokens / max(num_words, 1))\n",
    "        compression_list.append(num_bytes / max(num_tokens, 1))\n",
    "        pcw_list.append(num_continued / max(num_words, 1))\n",
    "        unk_rate_list.append(num_unk / max(num_tokens, 1))\n",
    "        strr_list.append(num_single / max(num_words, 1))\n",
    "    \n",
    "    return {\n",
    "        'num_words': num_words_list,\n",
    "        'num_tokens': num_tokens_list,\n",
    "        'num_bytes': num_bytes_list,\n",
    "        'num_unk_tokens': num_unk_list,\n",
    "        'num_continued_words': num_continued_list,\n",
    "        'num_single_token_words': num_single_list,\n",
    "        'fertility': fertility_list,\n",
    "        'compression_ratio': compression_list,\n",
    "        'pcw': pcw_list,\n",
    "        'unk_rate': unk_rate_list,\n",
    "        'strr': strr_list\n",
    "    }\n",
    "\n",
    "\n",
    "def analyze_corpus_parallel(texts: List[str], tokenizer_path: str, desc: str = \"Analyzing\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Analyze corpus using TRUE parallel processing with datasets.map().\n",
    "    Uses all 26 physical cores for maximum throughput.\n",
    "    \"\"\"\n",
    "    print(f\"  {desc}: Processing {len(texts)} texts with {NUM_PROC} parallel workers...\")\n",
    "    \n",
    "    # Filter empty texts\n",
    "    texts = [t for t in texts if t and t.strip()]\n",
    "    \n",
    "    # Create HuggingFace Dataset\n",
    "    ds = Dataset.from_dict({'text': texts})\n",
    "    \n",
    "    # Use datasets.map() with fn_kwargs to pass tokenizer_path (avoids lambda pickling issues)\n",
    "    ds = ds.map(\n",
    "        compute_metrics_for_map,\n",
    "        batched=True,\n",
    "        batch_size=500,\n",
    "        num_proc=NUM_PROC,\n",
    "        fn_kwargs={'tokenizer_path': tokenizer_path},\n",
    "        desc=desc\n",
    "    )\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    return ds.to_pandas()\n",
    "\n",
    "\n",
    "print(f\"Tokenizer metrics functions defined (TRUE PARALLEL mode, {NUM_PROC} workers).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T00:31:54.462516Z",
     "iopub.status.busy": "2025-11-29T00:31:54.462359Z",
     "iopub.status.idle": "2025-11-29T00:31:54.469571Z",
     "shell.execute_reply": "2025-11-29T00:31:54.469080Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistical analysis functions defined.\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Statistical Analysis Functions\n",
    "# ======================================\n",
    "\n",
    "def bootstrap_ci(data: np.ndarray, n_bootstrap: int = 1000, confidence: float = 0.95) -> Tuple[float, float, float]:\n",
    "    \"\"\"\n",
    "    Compute bootstrap confidence interval.\n",
    "    \n",
    "    Returns: (mean, lower_ci, upper_ci)\n",
    "    \"\"\"\n",
    "    means = []\n",
    "    n = len(data)\n",
    "    \n",
    "    for _ in range(n_bootstrap):\n",
    "        sample = np.random.choice(data, size=n, replace=True)\n",
    "        means.append(np.mean(sample))\n",
    "    \n",
    "    alpha = 1 - confidence\n",
    "    lower = np.percentile(means, 100 * alpha / 2)\n",
    "    upper = np.percentile(means, 100 * (1 - alpha / 2))\n",
    "    \n",
    "    return np.mean(data), lower, upper\n",
    "\n",
    "\n",
    "def cohens_d(group1: np.ndarray, group2: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Compute Cohen's d effect size.\n",
    "    \n",
    "    Interpretation:\n",
    "    - |d| < 0.2: negligible\n",
    "    - 0.2 <= |d| < 0.5: small\n",
    "    - 0.5 <= |d| < 0.8: medium\n",
    "    - |d| >= 0.8: large\n",
    "    \"\"\"\n",
    "    n1, n2 = len(group1), len(group2)\n",
    "    var1, var2 = np.var(group1, ddof=1), np.var(group2, ddof=1)\n",
    "    \n",
    "    # Pooled standard deviation\n",
    "    pooled_std = np.sqrt(((n1 - 1) * var1 + (n2 - 1) * var2) / (n1 + n2 - 2))\n",
    "    \n",
    "    return (np.mean(group1) - np.mean(group2)) / pooled_std if pooled_std > 0 else 0\n",
    "\n",
    "\n",
    "def compare_tokenizers(baseline_df: pd.DataFrame, adapted_df: pd.DataFrame, \n",
    "                       metric: str) -> Dict:\n",
    "    \"\"\"\n",
    "    Statistical comparison of a metric between two tokenizers.\n",
    "    \n",
    "    Returns dict with:\n",
    "    - baseline_mean, baseline_ci\n",
    "    - adapted_mean, adapted_ci\n",
    "    - t_statistic, p_value (paired t-test)\n",
    "    - cohens_d\n",
    "    - percent_change\n",
    "    \"\"\"\n",
    "    baseline_vals = baseline_df[metric].values\n",
    "    adapted_vals = adapted_df[metric].values\n",
    "    \n",
    "    # Ensure same length for paired test\n",
    "    min_len = min(len(baseline_vals), len(adapted_vals))\n",
    "    baseline_vals = baseline_vals[:min_len]\n",
    "    adapted_vals = adapted_vals[:min_len]\n",
    "    \n",
    "    # Bootstrap CIs\n",
    "    b_mean, b_lower, b_upper = bootstrap_ci(baseline_vals, BOOTSTRAP_ITERATIONS, CONFIDENCE_LEVEL)\n",
    "    a_mean, a_lower, a_upper = bootstrap_ci(adapted_vals, BOOTSTRAP_ITERATIONS, CONFIDENCE_LEVEL)\n",
    "    \n",
    "    # Paired t-test\n",
    "    t_stat, p_value = stats.ttest_rel(baseline_vals, adapted_vals)\n",
    "    \n",
    "    # Effect size\n",
    "    d = cohens_d(baseline_vals, adapted_vals)\n",
    "    \n",
    "    # Percent change\n",
    "    pct_change = ((a_mean - b_mean) / b_mean * 100) if b_mean != 0 else 0\n",
    "    \n",
    "    return {\n",
    "        'metric': metric,\n",
    "        'baseline_mean': b_mean,\n",
    "        'baseline_ci_lower': b_lower,\n",
    "        'baseline_ci_upper': b_upper,\n",
    "        'adapted_mean': a_mean,\n",
    "        'adapted_ci_lower': a_lower,\n",
    "        'adapted_ci_upper': a_upper,\n",
    "        't_statistic': t_stat,\n",
    "        'p_value': p_value,\n",
    "        'cohens_d': d,\n",
    "        'percent_change': pct_change,\n",
    "        'significant': p_value < 0.05\n",
    "    }\n",
    "\n",
    "\n",
    "def interpret_effect_size(d: float) -> str:\n",
    "    \"\"\"Interpret Cohen's d effect size.\"\"\"\n",
    "    d_abs = abs(d)\n",
    "    if d_abs < 0.2:\n",
    "        return \"negligible\"\n",
    "    elif d_abs < 0.5:\n",
    "        return \"small\"\n",
    "    elif d_abs < 0.8:\n",
    "        return \"medium\"\n",
    "    else:\n",
    "        return \"large\"\n",
    "\n",
    "\n",
    "print(\"Statistical analysis functions defined.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T00:31:54.470935Z",
     "iopub.status.busy": "2025-11-29T00:31:54.470763Z",
     "iopub.status.idle": "2025-11-29T00:31:55.080977Z",
     "shell.execute_reply": "2025-11-29T00:31:55.080400Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizers...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline tokenizer: EleutherAI/pythia-1b\n",
      "  Vocab size: 50254\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Adapted tokenizer: /home/ubuntu/aryan-true-tokalign/true-tokalign/log/1b/0_qwen2-7b_S2/checkpoint-2500\n",
      "  Vocab size: 151643\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Load Tokenizers\n",
    "# =======================\n",
    "\n",
    "print(\"Loading tokenizers...\")\n",
    "\n",
    "# Load baseline tokenizer\n",
    "baseline_tokenizer = AutoTokenizer.from_pretrained(BASELINE_MODEL)\n",
    "print(f\"Baseline tokenizer: {BASELINE_MODEL}\")\n",
    "print(f\"  Vocab size: {baseline_tokenizer.vocab_size}\")\n",
    "\n",
    "# Load adapted tokenizer (will fail gracefully if not available)\n",
    "try:\n",
    "    adapted_tokenizer = AutoTokenizer.from_pretrained(ADAPTED_MODEL)\n",
    "    print(f\"\\nAdapted tokenizer: {ADAPTED_MODEL}\")\n",
    "    print(f\"  Vocab size: {adapted_tokenizer.vocab_size}\")\n",
    "    HAS_ADAPTED = True\n",
    "except Exception as e:\n",
    "    print(f\"\\nAdapted model not found: {ADAPTED_MODEL}\")\n",
    "    print(\"  Running baseline-only analysis.\")\n",
    "    adapted_tokenizer = None\n",
    "    HAS_ADAPTED = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T00:31:55.082636Z",
     "iopub.status.busy": "2025-11-29T00:31:55.082464Z",
     "iopub.status.idle": "2025-11-29T00:31:58.348615Z",
     "shell.execute_reply": "2025-11-29T00:31:58.348019Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets with 26 parallel workers...\n",
      "\n",
      "1. Loading Spanish Wikipedia...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Loaded 20000 Spanish Wikipedia samples\n",
      "\n",
      "2. Loading English Wikipedia for comparison...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Loaded 5000 English Wikipedia samples\n",
      "\n",
      "Total Spanish corpus: 20000 samples\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Load Datasets (PARALLELIZED)\n",
    "# =====================================\n",
    "\n",
    "import multiprocessing\n",
    "NUM_PROC = multiprocessing.cpu_count() // 2  # Use 26 physical cores\n",
    "print(f\"Loading datasets with {NUM_PROC} parallel workers...\")\n",
    "\n",
    "# Load Spanish Wikipedia in parallel (non-streaming for speed)\n",
    "print(\"\\n1. Loading Spanish Wikipedia...\")\n",
    "target_samples = WIKIPEDIA_SAMPLES + OSCAR_SAMPLES  # Combine both quotas\n",
    "\n",
    "wiki_es_dataset = load_dataset(\n",
    "    \"wikimedia/wikipedia\", \n",
    "    \"20231101.es\",\n",
    "    split=f\"train[:{target_samples * 2}]\",  # Load extra to account for filtering\n",
    "    num_proc=NUM_PROC\n",
    ")\n",
    "\n",
    "# Filter in parallel\n",
    "def filter_short_texts(example):\n",
    "    text = example.get('text', '')\n",
    "    return text and len(text) > 50\n",
    "\n",
    "wiki_es_filtered = wiki_es_dataset.filter(filter_short_texts, num_proc=NUM_PROC)\n",
    "all_spanish_texts = wiki_es_filtered['text'][:target_samples]\n",
    "print(f\"   Loaded {len(all_spanish_texts)} Spanish Wikipedia samples\")\n",
    "\n",
    "# Load English Wikipedia in parallel\n",
    "print(\"\\n2. Loading English Wikipedia for comparison...\")\n",
    "wiki_en_dataset = load_dataset(\n",
    "    \"wikimedia/wikipedia\",\n",
    "    \"20231101.en\",\n",
    "    split=\"train[:15000]\",  # Load extra to account for filtering\n",
    "    num_proc=NUM_PROC\n",
    ")\n",
    "\n",
    "wiki_en_filtered = wiki_en_dataset.filter(filter_short_texts, num_proc=NUM_PROC)\n",
    "english_texts = wiki_en_filtered['text'][:5000]\n",
    "print(f\"   Loaded {len(english_texts)} English Wikipedia samples\")\n",
    "\n",
    "print(f\"\\nTotal Spanish corpus: {len(all_spanish_texts)} samples\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T00:31:58.350222Z",
     "iopub.status.busy": "2025-11-29T00:31:58.350059Z",
     "iopub.status.idle": "2025-11-29T00:33:50.873366Z",
     "shell.execute_reply": "2025-11-29T00:33:50.872711Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ANALYZING BASELINE TOKENIZER\n",
      "======================================================================\n",
      "\n",
      "--- Spanish Corpus ---\n",
      "  Baseline (Spanish): Processing 20000 texts with 26 parallel workers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Baseline (Spanish) (num_proc=26):   0%|                                                                                                      | 0/20000 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Baseline (Spanish) (num_proc=26):   2%|██▎                                                                                         | 500/20000 [00:21<14:13, 22.85 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Baseline (Spanish) (num_proc=26):   4%|███▌                                                                                        | 769/20000 [00:22<08:01, 39.93 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Baseline (Spanish) (num_proc=26):   6%|█████▊                                                                                     | 1269/20000 [00:25<04:49, 64.61 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Baseline (Spanish) (num_proc=26):   9%|████████                                                                                   | 1769/20000 [00:31<04:15, 71.43 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Baseline (Spanish) (num_proc=26):  11%|██████████▏                                                                               | 2269/20000 [00:32<02:38, 111.81 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Baseline (Spanish) (num_proc=26):  13%|███████████▍                                                                              | 2538/20000 [00:33<02:22, 122.27 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Baseline (Spanish) (num_proc=26):  15%|█████████████▋                                                                            | 3038/20000 [00:33<01:29, 190.38 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Baseline (Spanish) (num_proc=26):  18%|███████████████▉                                                                          | 3538/20000 [00:39<02:04, 132.33 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Baseline (Spanish) (num_proc=26):  20%|██████████████████▏                                                                       | 4038/20000 [00:40<01:25, 187.67 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Baseline (Spanish) (num_proc=26):  25%|██████████████████████▋                                                                   | 5038/20000 [00:41<00:52, 287.17 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Baseline (Spanish) (num_proc=26):  28%|████████████████████████▉                                                                 | 5538/20000 [00:44<00:55, 262.60 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Baseline (Spanish) (num_proc=26):  33%|█████████████████████████████▍                                                            | 6538/20000 [00:44<00:31, 429.45 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Baseline (Spanish) (num_proc=26):  34%|██████████████████████████████▋                                                           | 6807/20000 [00:44<00:29, 445.77 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Baseline (Spanish) (num_proc=26):  35%|███████████████████████████████▊                                                          | 7076/20000 [00:45<00:26, 493.20 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Baseline (Spanish) (num_proc=26):  37%|█████████████████████████████████                                                         | 7345/20000 [00:45<00:28, 442.47 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Baseline (Spanish) (num_proc=26):  39%|███████████████████████████████████▎                                                      | 7845/20000 [00:46<00:21, 552.71 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Baseline (Spanish) (num_proc=26):  43%|██████████████████████████████████████▊                                                   | 8615/20000 [00:47<00:16, 679.67 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Baseline (Spanish) (num_proc=26):  46%|█████████████████████████████████████████                                                 | 9115/20000 [00:47<00:12, 873.35 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Baseline (Spanish) (num_proc=26):  48%|███████████████████████████████████████████▎                                              | 9615/20000 [00:48<00:13, 778.57 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Baseline (Spanish) (num_proc=26):  51%|█████████████████████████████████████████████                                            | 10115/20000 [00:49<00:17, 567.38 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Baseline (Spanish) (num_proc=26):  54%|████████████████████████████████████████████████▍                                        | 10885/20000 [00:49<00:11, 826.66 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Baseline (Spanish) (num_proc=26):  56%|█████████████████████████████████████████████████▋                                       | 11154/20000 [00:50<00:10, 808.67 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Baseline (Spanish) (num_proc=26):  57%|██████████████████████████████████████████████████▊                                      | 11423/20000 [00:53<00:25, 338.50 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Baseline (Spanish) (num_proc=26):  58%|████████████████████████████████████████████████████                                     | 11692/20000 [00:53<00:22, 369.00 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Baseline (Spanish) (num_proc=26):  63%|████████████████████████████████████████████████████████▍                                | 12692/20000 [00:54<00:11, 611.11 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Baseline (Spanish) (num_proc=26):  66%|██████████████████████████████████████████████████████████▋                              | 13192/20000 [00:57<00:18, 365.16 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Baseline (Spanish) (num_proc=26):  67%|███████████████████████████████████████████████████████████▉                             | 13461/20000 [00:57<00:18, 361.87 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Baseline (Spanish) (num_proc=26):  70%|██████████████████████████████████████████████████████████████▏                          | 13961/20000 [00:58<00:12, 487.32 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Baseline (Spanish) (num_proc=26):  71%|███████████████████████████████████████████████████████████████▎                         | 14230/20000 [00:58<00:10, 574.52 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Baseline (Spanish) (num_proc=26):  72%|████████████████████████████████████████████████████████████████▌                        | 14499/20000 [00:58<00:08, 680.33 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Baseline (Spanish) (num_proc=26):  74%|█████████████████████████████████████████████████████████████████▋                       | 14768/20000 [00:58<00:06, 798.65 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Baseline (Spanish) (num_proc=26):  75%|██████████████████████████████████████████████████████████████████▉                      | 15037/20000 [00:59<00:09, 512.11 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Baseline (Spanish) (num_proc=26):  78%|█████████████████████████████████████████████████████████████████████▏                   | 15537/20000 [01:00<00:09, 480.17 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Baseline (Spanish) (num_proc=26):  79%|██████████████████████████████████████████████████████████████████████▎                  | 15806/20000 [01:02<00:11, 357.58 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Baseline (Spanish) (num_proc=26):  80%|███████████████████████████████████████████████████████████████████████▌                 | 16076/20000 [01:02<00:08, 454.85 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Baseline (Spanish) (num_proc=26):  83%|█████████████████████████████████████████████████████████████████████████▊               | 16576/20000 [01:02<00:06, 547.37 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Baseline (Spanish) (num_proc=26):  84%|██████████████████████████████████████████████████████████████████████████▉              | 16846/20000 [01:04<00:09, 337.52 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Baseline (Spanish) (num_proc=26):  87%|█████████████████████████████████████████████████████████████████████████████▏           | 17346/20000 [01:05<00:07, 363.70 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Baseline (Spanish) (num_proc=26):  88%|██████████████████████████████████████████████████████████████████████████████▍          | 17615/20000 [01:06<00:06, 350.46 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Baseline (Spanish) (num_proc=26):  89%|███████████████████████████████████████████████████████████████████████████████▌         | 17884/20000 [01:07<00:06, 334.48 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Baseline (Spanish) (num_proc=26):  91%|████████████████████████████████████████████████████████████████████████████████▊        | 18154/20000 [01:08<00:05, 331.07 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Baseline (Spanish) (num_proc=26):  92%|█████████████████████████████████████████████████████████████████████████████████▉       | 18423/20000 [01:08<00:03, 413.98 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Baseline (Spanish) (num_proc=26):  95%|████████████████████████████████████████████████████████████████████████████████████▏    | 18923/20000 [01:12<00:05, 208.92 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Baseline (Spanish) (num_proc=26):  96%|█████████████████████████████████████████████████████████████████████████████████████▍   | 19193/20000 [01:13<00:03, 214.47 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Baseline (Spanish) (num_proc=26):  97%|██████████████████████████████████████████████████████████████████████████████████████▌  | 19462/20000 [01:14<00:01, 275.16 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Baseline (Spanish) (num_proc=26):  99%|███████████████████████████████████████████████████████████████████████████████████████▊ | 19731/20000 [01:20<00:02, 113.48 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Baseline (Spanish) (num_proc=26): 100%|█████████████████████████████████████████████████████████████████████████████████████████| 20000/20000 [01:22<00:00, 112.52 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Baseline (Spanish) (num_proc=26): 100%|█████████████████████████████████████████████████████████████████████████████████████████| 20000/20000 [01:23<00:00, 240.57 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- English Corpus (Reference) ---\n",
      "  Baseline (English): Processing 5000 texts with 26 parallel workers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Baseline (English) (num_proc=26):   0%|                                                                                                       | 0/5000 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Baseline (English) (num_proc=26):   4%|███▌                                                                                         | 192/5000 [00:10<04:16, 18.76 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Baseline (English) (num_proc=26):  12%|██████████▋                                                                                  | 576/5000 [00:10<01:03, 69.15 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Baseline (English) (num_proc=26):  15%|██████████████▎                                                                              | 768/5000 [00:11<00:47, 89.77 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Baseline (English) (num_proc=26):  19%|█████████████████▋                                                                          | 960/5000 [00:12<00:34, 118.32 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Baseline (English) (num_proc=26):  23%|████████████████████▉                                                                      | 1152/5000 [00:12<00:26, 147.09 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Baseline (English) (num_proc=26):  27%|████████████████████████▍                                                                  | 1345/5000 [00:13<00:22, 165.74 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Baseline (English) (num_proc=26):  31%|███████████████████████████▉                                                               | 1537/5000 [00:13<00:15, 230.66 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Baseline (English) (num_proc=26):  35%|███████████████████████████████▍                                                           | 1729/5000 [00:13<00:11, 286.11 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Baseline (English) (num_proc=26):  38%|██████████████████████████████████▉                                                        | 1922/5000 [00:14<00:08, 347.66 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Baseline (English) (num_proc=26):  42%|██████████████████████████████████████▍                                                    | 2114/5000 [00:14<00:08, 324.65 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Baseline (English) (num_proc=26):  46%|█████████████████████████████████████████▉                                                 | 2306/5000 [00:15<00:07, 354.87 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Baseline (English) (num_proc=26):  50%|█████████████████████████████████████████████▍                                             | 2499/5000 [00:15<00:06, 401.56 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Baseline (English) (num_proc=26):  54%|████████████████████████████████████████████████▉                                          | 2691/5000 [00:15<00:04, 475.85 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Baseline (English) (num_proc=26):  62%|███████████████████████████████████████████████████████▉                                   | 3076/5000 [00:16<00:03, 505.80 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Baseline (English) (num_proc=26):  65%|███████████████████████████████████████████████████████████▍                               | 3269/5000 [00:17<00:05, 319.17 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Baseline (English) (num_proc=26):  69%|███████████████████████████████████████████████████████████████                            | 3462/5000 [00:18<00:05, 270.71 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Baseline (English) (num_proc=26):  73%|██████████████████████████████████████████████████████████████████▌                        | 3655/5000 [00:19<00:04, 309.92 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Baseline (English) (num_proc=26):  77%|██████████████████████████████████████████████████████████████████████                     | 3847/5000 [00:20<00:04, 285.99 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Baseline (English) (num_proc=26):  81%|█████████████████████████████████████████████████████████████████████████▌                 | 4039/5000 [00:21<00:04, 204.97 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Baseline (English) (num_proc=26):  85%|█████████████████████████████████████████████████████████████████████████████              | 4232/5000 [00:21<00:02, 274.33 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Baseline (English) (num_proc=26):  92%|████████████████████████████████████████████████████████████████████████████████████       | 4616/5000 [00:21<00:00, 469.92 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Baseline (English) (num_proc=26):  96%|███████████████████████████████████████████████████████████████████████████████████████▌   | 4808/5000 [00:22<00:00, 516.34 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Baseline (English) (num_proc=26): 100%|███████████████████████████████████████████████████████████████████████████████████████████| 5000/5000 [00:22<00:00, 623.41 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Baseline (English) (num_proc=26): 100%|███████████████████████████████████████████████████████████████████████████████████████████| 5000/5000 [00:22<00:00, 219.48 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Baseline Spanish results saved to: results/tokenizer_analysis_baseline.csv\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: Analyze Baseline Tokenizer (PARALLEL)\n",
    "# ==============================================\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"ANALYZING BASELINE TOKENIZER\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Spanish analysis - use parallel processing\n",
    "print(\"\\n--- Spanish Corpus ---\")\n",
    "baseline_spanish_df = analyze_corpus_parallel(all_spanish_texts, BASELINE_MODEL, \"Baseline (Spanish)\")\n",
    "\n",
    "# English analysis (for fertility comparison)\n",
    "print(\"\\n--- English Corpus (Reference) ---\")\n",
    "baseline_english_df = analyze_corpus_parallel(english_texts, BASELINE_MODEL, \"Baseline (English)\")\n",
    "\n",
    "# Save baseline results\n",
    "baseline_spanish_df.to_csv(BASELINE_OUTPUT, index=False)\n",
    "print(f\"\\nBaseline Spanish results saved to: {BASELINE_OUTPUT}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T00:33:50.875135Z",
     "iopub.status.busy": "2025-11-29T00:33:50.874958Z",
     "iopub.status.idle": "2025-11-29T00:35:49.590268Z",
     "shell.execute_reply": "2025-11-29T00:35:49.589620Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ANALYZING ADAPTED TOKENIZER\n",
      "======================================================================\n",
      "\n",
      "--- Spanish Corpus ---\n",
      "  Adapted (Spanish): Processing 20000 texts with 26 parallel workers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Adapted (Spanish) (num_proc=26):   0%|                                                                                                       | 0/20000 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (42059 > 32768). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (35791 > 32768). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (40102 > 32768). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (50885 > 32768). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (37656 > 32768). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (33291 > 32768). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (51298 > 32768). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (46362 > 32768). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (38010 > 32768). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (41823 > 32768). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Adapted (Spanish) (num_proc=26):   2%|██▎                                                                                          | 500/20000 [00:22<14:46, 22.00 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Adapted (Spanish) (num_proc=26):   4%|███▌                                                                                         | 769/20000 [00:23<08:29, 37.74 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (61242 > 32768). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Adapted (Spanish) (num_proc=26):   6%|█████▊                                                                                      | 1269/20000 [00:26<05:05, 61.40 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (35987 > 32768). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (69423 > 32768). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (32774 > 32768). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (39471 > 32768). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Adapted (Spanish) (num_proc=26):   9%|████████▏                                                                                   | 1769/20000 [00:33<04:26, 68.40 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Adapted (Spanish) (num_proc=26):  11%|██████████▎                                                                                | 2269/20000 [00:33<02:45, 107.04 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (37670 > 32768). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Adapted (Spanish) (num_proc=26):  14%|████████████▌                                                                              | 2769/20000 [00:35<02:06, 136.33 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Adapted (Spanish) (num_proc=26):  15%|█████████████▊                                                                             | 3038/20000 [00:35<01:39, 170.82 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (40715 > 32768). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Adapted (Spanish) (num_proc=26):  18%|████████████████                                                                           | 3538/20000 [00:41<02:09, 127.53 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Adapted (Spanish) (num_proc=26):  20%|██████████████████▎                                                                        | 4038/20000 [00:41<01:30, 176.33 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Adapted (Spanish) (num_proc=26):  23%|████████████████████▋                                                                      | 4538/20000 [00:42<01:01, 250.64 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Adapted (Spanish) (num_proc=26):  25%|██████████████████████▉                                                                    | 5038/20000 [00:43<00:53, 277.44 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (38600 > 32768). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (35989 > 32768). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Adapted (Spanish) (num_proc=26):  28%|█████████████████████████▏                                                                 | 5538/20000 [00:46<00:58, 246.78 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Adapted (Spanish) (num_proc=26):  30%|███████████████████████████▍                                                               | 6038/20000 [00:46<00:41, 337.89 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Adapted (Spanish) (num_proc=26):  33%|█████████████████████████████▋                                                             | 6538/20000 [00:46<00:28, 472.61 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Adapted (Spanish) (num_proc=26):  34%|██████████████████████████████▉                                                            | 6807/20000 [00:46<00:23, 554.70 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (36398 > 32768). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Adapted (Spanish) (num_proc=26):  35%|████████████████████████████████▏                                                          | 7076/20000 [00:47<00:29, 438.94 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Adapted (Spanish) (num_proc=26):  37%|█████████████████████████████████▍                                                         | 7346/20000 [00:48<00:32, 385.06 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Adapted (Spanish) (num_proc=26):  38%|██████████████████████████████████▋                                                        | 7615/20000 [00:48<00:25, 478.59 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Adapted (Spanish) (num_proc=26):  41%|████████████████████████████████████▉                                                      | 8115/20000 [00:48<00:16, 715.85 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Adapted (Spanish) (num_proc=26):  43%|███████████████████████████████████████▏                                                   | 8615/20000 [00:49<00:13, 843.40 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Adapted (Spanish) (num_proc=26):  46%|█████████████████████████████████████████                                                 | 9115/20000 [00:49<00:09, 1165.70 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (35850 > 32768). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Adapted (Spanish) (num_proc=26):  48%|███████████████████████████████████████████▋                                               | 9615/20000 [00:50<00:11, 916.77 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Adapted (Spanish) (num_proc=26):  49%|████████████████████████████████████████████▉                                              | 9885/20000 [00:52<00:21, 460.77 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Adapted (Spanish) (num_proc=26):  52%|██████████████████████████████████████████████▋                                           | 10385/20000 [00:52<00:15, 604.17 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Adapted (Spanish) (num_proc=26):  54%|████████████████████████████████████████████████▉                                         | 10885/20000 [00:52<00:10, 835.95 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Adapted (Spanish) (num_proc=26):  57%|███████████████████████████████████████████████████▍                                      | 11423/20000 [00:55<00:23, 367.91 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Adapted (Spanish) (num_proc=26):  58%|████████████████████████████████████████████████████▌                                     | 11692/20000 [00:55<00:19, 422.68 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Adapted (Spanish) (num_proc=26):  61%|██████████████████████████████████████████████████████▊                                   | 12192/20000 [00:56<00:15, 516.02 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Adapted (Spanish) (num_proc=26):  63%|█████████████████████████████████████████████████████████                                 | 12692/20000 [00:57<00:14, 496.50 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (52379 > 32768). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Adapted (Spanish) (num_proc=26):  66%|███████████████████████████████████████████████████████████▎                              | 13192/20000 [00:59<00:19, 346.83 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Adapted (Spanish) (num_proc=26):  67%|████████████████████████████████████████████████████████████▌                             | 13461/20000 [01:00<00:18, 345.59 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Adapted (Spanish) (num_proc=26):  69%|█████████████████████████████████████████████████████████████▊                            | 13730/20000 [01:00<00:14, 427.08 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Adapted (Spanish) (num_proc=26):  72%|█████████████████████████████████████████████████████████████████▏                        | 14499/20000 [01:01<00:08, 613.20 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Adapted (Spanish) (num_proc=26):  75%|███████████████████████████████████████████████████████████████████▋                      | 15037/20000 [01:03<00:10, 462.32 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Adapted (Spanish) (num_proc=26):  78%|█████████████████████████████████████████████████████████████████████▉                    | 15537/20000 [01:03<00:07, 595.86 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Adapted (Spanish) (num_proc=26):  79%|███████████████████████████████████████████████████████████████████████▏                  | 15806/20000 [01:05<00:11, 374.54 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Adapted (Spanish) (num_proc=26):  83%|██████████████████████████████████████████████████████████████████████████▌               | 16576/20000 [01:05<00:05, 590.80 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Adapted (Spanish) (num_proc=26):  84%|███████████████████████████████████████████████████████████████████████████▊              | 16846/20000 [01:07<00:08, 357.95 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Adapted (Spanish) (num_proc=26):  87%|██████████████████████████████████████████████████████████████████████████████            | 17346/20000 [01:08<00:07, 367.83 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Adapted (Spanish) (num_proc=26):  88%|███████████████████████████████████████████████████████████████████████████████▎          | 17615/20000 [01:10<00:07, 313.30 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Adapted (Spanish) (num_proc=26):  89%|████████████████████████████████████████████████████████████████████████████████▍         | 17884/20000 [01:10<00:05, 387.68 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Adapted (Spanish) (num_proc=26):  91%|█████████████████████████████████████████████████████████████████████████████████▋        | 18153/20000 [01:11<00:05, 333.11 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Adapted (Spanish) (num_proc=26):  92%|██████████████████████████████████████████████████████████████████████████████████▉       | 18423/20000 [01:12<00:04, 343.98 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Adapted (Spanish) (num_proc=26):  95%|█████████████████████████████████████████████████████████████████████████████████████▏    | 18923/20000 [01:16<00:05, 200.67 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Adapted (Spanish) (num_proc=26):  96%|██████████████████████████████████████████████████████████████████████████████████████▎   | 19192/20000 [01:17<00:03, 223.11 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Adapted (Spanish) (num_proc=26):  99%|████████████████████████████████████████████████████████████████████████████████████████▊ | 19731/20000 [01:23<00:02, 128.92 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Adapted (Spanish) (num_proc=26): 100%|██████████████████████████████████████████████████████████████████████████████████████████| 20000/20000 [01:26<00:00, 122.50 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Adapted (Spanish) (num_proc=26): 100%|██████████████████████████████████████████████████████████████████████████████████████████| 20000/20000 [01:27<00:00, 229.74 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- English Corpus ---\n",
      "  Adapted (English): Processing 5000 texts with 26 parallel workers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Adapted (English) (num_proc=26):   0%|                                                                                                        | 0/5000 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Adapted (English) (num_proc=26):   4%|███▌                                                                                          | 192/5000 [00:12<05:23, 14.86 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Adapted (English) (num_proc=26):   8%|███████▏                                                                                      | 384/5000 [00:13<02:09, 35.58 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Adapted (English) (num_proc=26):  12%|██████████▊                                                                                   | 576/5000 [00:13<01:09, 63.82 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Adapted (English) (num_proc=26):  15%|██████████████▍                                                                               | 768/5000 [00:14<00:49, 85.14 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Adapted (English) (num_proc=26):  19%|█████████████████▊                                                                           | 961/5000 [00:14<00:33, 119.92 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Adapted (English) (num_proc=26):  23%|█████████████████████▏                                                                      | 1153/5000 [00:15<00:23, 166.80 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Adapted (English) (num_proc=26):  27%|████████████████████████▊                                                                   | 1346/5000 [00:15<00:18, 202.50 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Adapted (English) (num_proc=26):  31%|████████████████████████████▎                                                               | 1538/5000 [00:15<00:13, 259.42 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Adapted (English) (num_proc=26):  35%|███████████████████████████████▊                                                            | 1730/5000 [00:17<00:15, 205.60 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Adapted (English) (num_proc=26):  46%|██████████████████████████████████████████▍                                                 | 2308/5000 [00:17<00:07, 352.27 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Adapted (English) (num_proc=26):  50%|██████████████████████████████████████████████                                              | 2500/5000 [00:18<00:07, 354.20 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Adapted (English) (num_proc=26):  54%|█████████████████████████████████████████████████▌                                          | 2693/5000 [00:18<00:06, 369.37 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Adapted (English) (num_proc=26):  62%|████████████████████████████████████████████████████████▌                                   | 3077/5000 [00:19<00:04, 405.51 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Adapted (English) (num_proc=26):  69%|███████████████████████████████████████████████████████████████▋                            | 3462/5000 [00:20<00:02, 515.66 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Adapted (English) (num_proc=26):  73%|███████████████████████████████████████████████████████████████████▎                        | 3655/5000 [00:21<00:03, 349.90 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Adapted (English) (num_proc=26):  77%|██████████████████████████████████████████████████████████████████████▊                     | 3847/5000 [00:22<00:03, 311.32 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Adapted (English) (num_proc=26):  81%|██████████████████████████████████████████████████████████████████████████▎                 | 4040/5000 [00:22<00:02, 329.05 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Adapted (English) (num_proc=26):  85%|█████████████████████████████████████████████████████████████████████████████▊              | 4232/5000 [00:24<00:03, 211.97 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Adapted (English) (num_proc=26):  92%|████████████████████████████████████████████████████████████████████████████████████▉       | 4616/5000 [00:24<00:01, 337.85 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Adapted (English) (num_proc=26): 100%|████████████████████████████████████████████████████████████████████████████████████████████| 5000/5000 [00:25<00:00, 196.83 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Adapted Spanish results saved to: results/tokenizer_analysis_adapted.csv\n"
     ]
    }
   ],
   "source": [
    "# Cell 9: Analyze Adapted Tokenizer (PARALLEL)\n",
    "# =============================================\n",
    "\n",
    "if HAS_ADAPTED:\n",
    "    print(\"=\" * 70)\n",
    "    print(\"ANALYZING ADAPTED TOKENIZER\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Spanish analysis - use parallel processing\n",
    "    print(\"\\n--- Spanish Corpus ---\")\n",
    "    adapted_spanish_df = analyze_corpus_parallel(all_spanish_texts, ADAPTED_MODEL, \"Adapted (Spanish)\")\n",
    "    \n",
    "    # English analysis\n",
    "    print(\"\\n--- English Corpus ---\")\n",
    "    adapted_english_df = analyze_corpus_parallel(english_texts, ADAPTED_MODEL, \"Adapted (English)\")\n",
    "    \n",
    "    # Save adapted results\n",
    "    adapted_spanish_df.to_csv(ADAPTED_OUTPUT, index=False)\n",
    "    print(f\"\\nAdapted Spanish results saved to: {ADAPTED_OUTPUT}\")\n",
    "else:\n",
    "    adapted_spanish_df = None\n",
    "    adapted_english_df = None\n",
    "    print(\"Skipping adapted tokenizer analysis (model not available).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T00:35:49.592071Z",
     "iopub.status.busy": "2025-11-29T00:35:49.591896Z",
     "iopub.status.idle": "2025-11-29T00:35:53.638685Z",
     "shell.execute_reply": "2025-11-29T00:35:53.638133Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "BASELINE TOKENIZER - SPANISH\n",
      "============================================================\n",
      "\n",
      "Metric                     Mean        Std     Median         P5        P95\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fertility                2.0591     0.2316     1.9925     1.8155     2.4877\n",
      "  95% CI             [    2.0558,     2.0622]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compression_ratio        3.1687     0.2203     3.1986     2.7828     3.4907\n",
      "  95% CI             [    3.1659,     3.1718]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pcw                      0.5883     0.0548     0.5741     0.5273     0.7031\n",
      "  95% CI             [    0.5875,     0.5890]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unk_rate                 0.0000     0.0000     0.0000     0.0000     0.0000\n",
      "  95% CI             [    0.0000,     0.0000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "strr                     0.4117     0.0548     0.4259     0.2969     0.4727\n",
      "  95% CI             [    0.4110,     0.4125]\n",
      "\n",
      "============================================================\n",
      "BASELINE TOKENIZER - ENGLISH (Reference)\n",
      "============================================================\n",
      "\n",
      "Metric                     Mean        Std     Median         P5        P95\n",
      "----------------------------------------------------------------------\n",
      "fertility                1.5276     0.2401     1.4625     1.2994     1.9520\n",
      "  95% CI             [    1.5210,     1.5340]\n",
      "compression_ratio        4.2671     0.4574     4.3118     3.4530     4.9427\n",
      "  95% CI             [    4.2541,     4.2792]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pcw                      0.3983     0.0722     0.3864     0.3072     0.5327\n",
      "  95% CI             [    0.3963,     0.4002]\n",
      "unk_rate                 0.0000     0.0000     0.0000     0.0000     0.0000\n",
      "  95% CI             [    0.0000,     0.0000]\n",
      "strr                     0.6017     0.0722     0.6136     0.4673     0.6928\n",
      "  95% CI             [    0.5998,     0.6036]\n",
      "\n",
      "============================================================\n",
      "ADAPTED TOKENIZER - SPANISH\n",
      "============================================================\n",
      "\n",
      "Metric                     Mean        Std     Median         P5        P95\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fertility                1.8739     0.2627     1.8022     1.5949     2.4336\n",
      "  95% CI             [    1.8705,     1.8774]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compression_ratio        3.5014     0.3490     3.5323     2.8710     4.0531\n",
      "  95% CI             [    3.4966,     3.5060]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pcw                      0.5722     0.0598     0.5554     0.5036     0.6990\n",
      "  95% CI             [    0.5714,     0.5731]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unk_rate                 0.0000     0.0000     0.0000     0.0000     0.0000\n",
      "  95% CI             [    0.0000,     0.0000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "strr                     0.4278     0.0598     0.4446     0.3010     0.4964\n",
      "  95% CI             [    0.4270,     0.4285]\n",
      "\n",
      "============================================================\n",
      "ADAPTED TOKENIZER - ENGLISH\n",
      "============================================================\n",
      "\n",
      "Metric                     Mean        Std     Median         P5        P95\n",
      "----------------------------------------------------------------------\n",
      "fertility                1.5612     0.2633     1.4894     1.2945     2.1154\n",
      "  95% CI             [    1.5539,     1.5688]\n",
      "compression_ratio        4.1903     0.5108     4.2307     3.2357     4.9741\n",
      "  95% CI             [    4.1759,     4.2055]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pcw                      0.3885     0.0722     0.3781     0.2928     0.5355\n",
      "  95% CI             [    0.3865,     0.3904]\n",
      "unk_rate                 0.0000     0.0000     0.0000     0.0000     0.0000\n",
      "  95% CI             [    0.0000,     0.0000]\n",
      "strr                     0.6115     0.0722     0.6219     0.4645     0.7072\n",
      "  95% CI             [    0.6094,     0.6135]\n"
     ]
    }
   ],
   "source": [
    "# Cell 10: Summary Statistics\n",
    "# ============================\n",
    "\n",
    "def print_summary_stats(df: pd.DataFrame, name: str):\n",
    "    \"\"\"Print summary statistics for a metrics DataFrame.\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"{name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    metrics = ['fertility', 'compression_ratio', 'pcw', 'unk_rate', 'strr']\n",
    "    \n",
    "    print(f\"\\n{'Metric':<20} {'Mean':>10} {'Std':>10} {'Median':>10} {'P5':>10} {'P95':>10}\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    for metric in metrics:\n",
    "        vals = df[metric].values\n",
    "        mean_val, ci_lower, ci_upper = bootstrap_ci(vals, BOOTSTRAP_ITERATIONS, CONFIDENCE_LEVEL)\n",
    "        print(f\"{metric:<20} {np.mean(vals):>10.4f} {np.std(vals):>10.4f} \"\n",
    "              f\"{np.median(vals):>10.4f} {np.percentile(vals, 5):>10.4f} {np.percentile(vals, 95):>10.4f}\")\n",
    "        print(f\"{'  95% CI':<20} [{ci_lower:>10.4f}, {ci_upper:>10.4f}]\")\n",
    "\n",
    "\n",
    "# Print baseline statistics\n",
    "print_summary_stats(baseline_spanish_df, \"BASELINE TOKENIZER - SPANISH\")\n",
    "print_summary_stats(baseline_english_df, \"BASELINE TOKENIZER - ENGLISH (Reference)\")\n",
    "\n",
    "# Print adapted statistics if available\n",
    "if HAS_ADAPTED:\n",
    "    print_summary_stats(adapted_spanish_df, \"ADAPTED TOKENIZER - SPANISH\")\n",
    "    print_summary_stats(adapted_english_df, \"ADAPTED TOKENIZER - ENGLISH\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T00:35:53.640382Z",
     "iopub.status.busy": "2025-11-29T00:35:53.640214Z",
     "iopub.status.idle": "2025-11-29T00:35:56.793888Z",
     "shell.execute_reply": "2025-11-29T00:35:56.793326Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "STATISTICAL COMPARISON: BASELINE vs ADAPTED (SPANISH)\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- FERTILITY ---\n",
      "  Baseline: 2.0591 [2.0559, 2.0620]\n",
      "  Adapted:  1.8739 [1.8705, 1.8776]\n",
      "  Change:   -8.99%\n",
      "  t-stat:   219.567, p-value: 0.00e+00 ***\n",
      "  Cohen's d: 0.748 (medium)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- COMPRESSION_RATIO ---\n",
      "  Baseline: 3.1687 [3.1656, 3.1719]\n",
      "  Adapted:  3.5014 [3.4970, 3.5065]\n",
      "  Change:   +10.50%\n",
      "  t-stat:   -231.707, p-value: 0.00e+00 ***\n",
      "  Cohen's d: -1.140 (large)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- PCW ---\n",
      "  Baseline: 0.5883 [0.5875, 0.5890]\n",
      "  Adapted:  0.5722 [0.5714, 0.5730]\n",
      "  Change:   -2.73%\n",
      "  t-stat:   112.742, p-value: 0.00e+00 ***\n",
      "  Cohen's d: 0.279 (small)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- UNK_RATE ---\n",
      "  Baseline: 0.0000 [0.0000, 0.0000]\n",
      "  Adapted:  0.0000 [0.0000, 0.0000]\n",
      "  Change:   +0.00%\n",
      "  t-stat:   nan, p-value: nan \n",
      "  Cohen's d: 0.000 (negligible)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- STRR ---\n",
      "  Baseline: 0.4117 [0.4109, 0.4125]\n",
      "  Adapted:  0.4278 [0.4269, 0.4286]\n",
      "  Change:   +3.89%\n",
      "  t-stat:   -112.742, p-value: 0.00e+00 ***\n",
      "  Cohen's d: -0.279 (small)\n",
      "\n",
      "Comparison results saved to: results/tokenizer_comparison.csv\n"
     ]
    }
   ],
   "source": [
    "# Cell 11: Statistical Comparison (Baseline vs Adapted)\n",
    "# =====================================================\n",
    "\n",
    "if HAS_ADAPTED:\n",
    "    print(\"=\" * 70)\n",
    "    print(\"STATISTICAL COMPARISON: BASELINE vs ADAPTED (SPANISH)\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    metrics_to_compare = ['fertility', 'compression_ratio', 'pcw', 'unk_rate', 'strr']\n",
    "    comparison_results = []\n",
    "    \n",
    "    for metric in metrics_to_compare:\n",
    "        result = compare_tokenizers(baseline_spanish_df, adapted_spanish_df, metric)\n",
    "        comparison_results.append(result)\n",
    "        \n",
    "        effect_interpretation = interpret_effect_size(result['cohens_d'])\n",
    "        sig_marker = \"***\" if result['p_value'] < 0.001 else \"**\" if result['p_value'] < 0.01 else \"*\" if result['p_value'] < 0.05 else \"\"\n",
    "        \n",
    "        print(f\"\\n--- {metric.upper()} ---\")\n",
    "        print(f\"  Baseline: {result['baseline_mean']:.4f} [{result['baseline_ci_lower']:.4f}, {result['baseline_ci_upper']:.4f}]\")\n",
    "        print(f\"  Adapted:  {result['adapted_mean']:.4f} [{result['adapted_ci_lower']:.4f}, {result['adapted_ci_upper']:.4f}]\")\n",
    "        print(f\"  Change:   {result['percent_change']:+.2f}%\")\n",
    "        print(f\"  t-stat:   {result['t_statistic']:.3f}, p-value: {result['p_value']:.2e} {sig_marker}\")\n",
    "        print(f\"  Cohen's d: {result['cohens_d']:.3f} ({effect_interpretation})\")\n",
    "    \n",
    "    # Save comparison results\n",
    "    comparison_df = pd.DataFrame(comparison_results)\n",
    "    comparison_df.to_csv(COMPARISON_OUTPUT, index=False)\n",
    "    print(f\"\\nComparison results saved to: {COMPARISON_OUTPUT}\")\n",
    "else:\n",
    "    print(\"Comparison skipped (adapted model not available).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T00:35:56.795555Z",
     "iopub.status.busy": "2025-11-29T00:35:56.795385Z",
     "iopub.status.idle": "2025-11-29T00:35:56.802358Z",
     "shell.execute_reply": "2025-11-29T00:35:56.801891Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "FERTILITY GAP ANALYSIS\n",
      "======================================================================\n",
      "\n",
      "Goal: Spanish fertility approaching English (~1.4 tokens/word)\n",
      "\n",
      "--- BASELINE ---\n",
      "  English fertility:  1.5276 tokens/word\n",
      "  Spanish fertility:  2.0591 tokens/word\n",
      "  Fertility gap:      +0.5315 (+34.8% overhead)\n",
      "\n",
      "--- ADAPTED ---\n",
      "  English fertility:  1.5612 tokens/word\n",
      "  Spanish fertility:  1.8739 tokens/word\n",
      "  Fertility gap:      +0.3127 (+20.0% overhead)\n",
      "\n",
      "--- IMPROVEMENT ---\n",
      "  Fertility gap reduction: 0.2188 (41.2% reduction)\n",
      "  Estimated inference speedup: 9.9%\n"
     ]
    }
   ],
   "source": [
    "# Cell 12: Fertility Gap Analysis\n",
    "# ================================\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"FERTILITY GAP ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\\nGoal: Spanish fertility approaching English (~1.4 tokens/word)\")\n",
    "\n",
    "# Calculate fertility gap for baseline\n",
    "baseline_es_fertility = baseline_spanish_df['fertility'].mean()\n",
    "baseline_en_fertility = baseline_english_df['fertility'].mean()\n",
    "baseline_gap = baseline_es_fertility - baseline_en_fertility\n",
    "baseline_gap_pct = (baseline_gap / baseline_en_fertility) * 100\n",
    "\n",
    "print(f\"\\n--- BASELINE ---\")\n",
    "print(f\"  English fertility:  {baseline_en_fertility:.4f} tokens/word\")\n",
    "print(f\"  Spanish fertility:  {baseline_es_fertility:.4f} tokens/word\")\n",
    "print(f\"  Fertility gap:      {baseline_gap:+.4f} ({baseline_gap_pct:+.1f}% overhead)\")\n",
    "\n",
    "if HAS_ADAPTED:\n",
    "    adapted_es_fertility = adapted_spanish_df['fertility'].mean()\n",
    "    adapted_en_fertility = adapted_english_df['fertility'].mean()\n",
    "    adapted_gap = adapted_es_fertility - adapted_en_fertility\n",
    "    adapted_gap_pct = (adapted_gap / adapted_en_fertility) * 100\n",
    "    \n",
    "    print(f\"\\n--- ADAPTED ---\")\n",
    "    print(f\"  English fertility:  {adapted_en_fertility:.4f} tokens/word\")\n",
    "    print(f\"  Spanish fertility:  {adapted_es_fertility:.4f} tokens/word\")\n",
    "    print(f\"  Fertility gap:      {adapted_gap:+.4f} ({adapted_gap_pct:+.1f}% overhead)\")\n",
    "    \n",
    "    # Improvement calculation\n",
    "    gap_reduction = baseline_gap - adapted_gap\n",
    "    gap_reduction_pct = (gap_reduction / baseline_gap) * 100 if baseline_gap != 0 else 0\n",
    "    \n",
    "    print(f\"\\n--- IMPROVEMENT ---\")\n",
    "    print(f\"  Fertility gap reduction: {gap_reduction:.4f} ({gap_reduction_pct:.1f}% reduction)\")\n",
    "    print(f\"  Estimated inference speedup: {(baseline_es_fertility / adapted_es_fertility - 1) * 100:.1f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T00:35:56.803772Z",
     "iopub.status.busy": "2025-11-29T00:35:56.803612Z",
     "iopub.status.idle": "2025-11-29T00:35:56.812092Z",
     "shell.execute_reply": "2025-11-29T00:35:56.811611Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "LATEX TABLE OUTPUT\n",
      "======================================================================\n",
      "\n",
      "% Table 1: Tokenizer Efficiency Metrics\n",
      "\\begin{table}[h]\n",
      "\\centering\n",
      "\\caption{Tokenizer Efficiency Analysis on Spanish Corpus}\n",
      "\\label{tab:tokenizer-efficiency}\n",
      "\\begin{tabular}{lccccc}\n",
      "\\toprule\n",
      "Model & Fertility$\\downarrow$ & Compression$\\uparrow$ & PCW$\\downarrow$ & UNK$\\downarrow$ & STRR$\\uparrow$ \\\\\n",
      "\\midrule\n",
      "Baseline & 2.059 & 3.169 & 0.588 & 0.0000 & 0.412 \\\\\n",
      "Adapted & 1.874 & 3.501 & 0.572 & 0.0000 & 0.428 \\\\\n",
      "\\midrule\n",
      "$\\Delta$ & -9.0\\% & +10.5\\% & -2.7\\% & -- & +3.9\\% \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\\end{table}\n",
      "\n",
      "======================================================================\n",
      "TOKENIZER EFFICIENCY ANALYSIS COMPLETE\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Cell 13: Generate LaTeX Tables\n",
    "# ===============================\n",
    "\n",
    "def generate_latex_table():\n",
    "    \"\"\"Generate LaTeX-formatted results table.\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"LATEX TABLE OUTPUT\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Table 1: Summary metrics\n",
    "    print(\"\\n% Table 1: Tokenizer Efficiency Metrics\")\n",
    "    print(\"\\\\begin{table}[h]\")\n",
    "    print(\"\\\\centering\")\n",
    "    print(\"\\\\caption{Tokenizer Efficiency Analysis on Spanish Corpus}\")\n",
    "    print(\"\\\\label{tab:tokenizer-efficiency}\")\n",
    "    print(\"\\\\begin{tabular}{lccccc}\")\n",
    "    print(\"\\\\toprule\")\n",
    "    print(\"Model & Fertility$\\\\downarrow$ & Compression$\\\\uparrow$ & PCW$\\\\downarrow$ & UNK$\\\\downarrow$ & STRR$\\\\uparrow$ \\\\\\\\\")\n",
    "    print(\"\\\\midrule\")\n",
    "    \n",
    "    # Baseline row\n",
    "    b_fert = baseline_spanish_df['fertility'].mean()\n",
    "    b_comp = baseline_spanish_df['compression_ratio'].mean()\n",
    "    b_pcw = baseline_spanish_df['pcw'].mean()\n",
    "    b_unk = baseline_spanish_df['unk_rate'].mean()\n",
    "    b_strr = baseline_spanish_df['strr'].mean()\n",
    "    print(f\"Baseline & {b_fert:.3f} & {b_comp:.3f} & {b_pcw:.3f} & {b_unk:.4f} & {b_strr:.3f} \\\\\\\\\")\n",
    "    \n",
    "    if HAS_ADAPTED:\n",
    "        a_fert = adapted_spanish_df['fertility'].mean()\n",
    "        a_comp = adapted_spanish_df['compression_ratio'].mean()\n",
    "        a_pcw = adapted_spanish_df['pcw'].mean()\n",
    "        a_unk = adapted_spanish_df['unk_rate'].mean()\n",
    "        a_strr = adapted_spanish_df['strr'].mean()\n",
    "        print(f\"Adapted & {a_fert:.3f} & {a_comp:.3f} & {a_pcw:.3f} & {a_unk:.4f} & {a_strr:.3f} \\\\\\\\\")\n",
    "        \n",
    "        # Delta row\n",
    "        print(\"\\\\midrule\")\n",
    "        delta_fert = ((a_fert - b_fert) / b_fert) * 100 if b_fert != 0 else 0\n",
    "        delta_comp = ((a_comp - b_comp) / b_comp) * 100 if b_comp != 0 else 0\n",
    "        delta_pcw = ((a_pcw - b_pcw) / b_pcw) * 100 if b_pcw != 0 else 0\n",
    "        delta_strr = ((a_strr - b_strr) / b_strr) * 100 if b_strr != 0 else 0\n",
    "        print(f\"$\\\\Delta$ & {delta_fert:+.1f}\\\\% & {delta_comp:+.1f}\\\\% & {delta_pcw:+.1f}\\\\% & -- & {delta_strr:+.1f}\\\\% \\\\\\\\\")\n",
    "    \n",
    "    print(\"\\\\bottomrule\")\n",
    "    print(\"\\\\end{tabular}\")\n",
    "    print(\"\\\\end{table}\")\n",
    "\n",
    "\n",
    "generate_latex_table()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"TOKENIZER EFFICIENCY ANALYSIS COMPLETE\")\n",
    "print(\"=\" * 70)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
