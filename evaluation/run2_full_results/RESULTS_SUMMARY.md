# TokAlign Evaluation Results Summary

**Date:** November 29, 2025  
**Hardware:** 2x NVIDIA H100 80GB HBM3, Intel Xeon Platinum 8480+ (52 cores)  
**Runtime:** ~20 minutes

---

## Models Compared

| Model | Description | Parameters |
|-------|-------------|------------|
| **Baseline** | EleutherAI/pythia-1b | 1.01B |
| **Adapted** | checkpoint-2500 (Qwen2 tokenizer) | 1.43B |

---

## Section A: Tokenizer Efficiency

Analysis on ~2.1M Spanish Wikipedia samples.

| Metric | Baseline | Adapted | Change | Significant |
|--------|----------|---------|--------|-------------|
| **Fertility** (tokens/word) | 2.059 | 1.874 | **-9.0%** ‚úÖ | Yes |
| **Compression Ratio** | 3.169 | 3.501 | **+10.5%** ‚úÖ | Yes |
| **PCW** (continued words) | 0.588 | 0.572 | -2.7% | Yes |
| **STRR** (single-token rate) | 0.412 | 0.428 | +3.9% | Yes |
| **UNK Rate** | 0.0% | 0.0% | ‚Äî | No |

**Key Insight:** The adapted Qwen2 tokenizer achieves **9% lower fertility** on Spanish text, meaning fewer tokens are needed to represent the same content. This directly translates to faster inference.

---

## Section B: Perplexity

| Language | Baseline | Adapted | Delta |
|----------|----------|---------|-------|
| **Spanish** | 13.88 | 39.06 | +181% ‚ö†Ô∏è |
| **English** | 36.31 | 43.48 | +20% |

**Note:** Higher perplexity on the adapted model is expected during early training (checkpoint-2500). The model is still learning to utilize the new tokenizer effectively.

---

## Section C: Natural Language Understanding (0-shot)

### ARC-Easy
| Metric | Baseline | Adapted | Delta |
|--------|----------|---------|-------|
| Accuracy | 56.94% | 53.20% | -3.7% |
| Acc (normalized) | 49.12% | 46.55% | -2.6% |

### HellaSwag
| Metric | Baseline | Adapted | Delta |
|--------|----------|---------|-------|
| Accuracy | 37.63% | 35.54% | -2.1% |
| Acc (normalized) | 47.19% | 43.50% | -3.7% |

### LAMBADA (OpenAI)
| Metric | Baseline | Adapted | Delta |
|--------|----------|---------|-------|
| Perplexity | 7.92 | 15.15 | +91% |
| Accuracy | 55.99% | 46.67% | -9.3% |

**Summary:** Moderate degradation on English NLU tasks (~3-9%), which is within acceptable range for early-stage tokenizer adaptation.

---

## Section D: Machine Translation (Spanish ‚Üí English)

Dataset: OPUS-100 (en-es), 1000 samples

| Metric | Baseline | Adapted | Delta |
|--------|----------|---------|-------|
| **BLEU** | 16.25 | 14.30 | -12.0% |
| **chrF++** | 33.31 | 37.70 | **+13.2%** ‚úÖ |
| **TER** ‚Üì | 83.07 | 101.18 | +21.8% |

**BLEU Signatures:**
- Baseline: `BLEU = 16.25 48.2/24.8/15.5/10.3 (BP = 0.778 ratio = 0.799)`
- Adapted: `BLEU = 14.30 38.8/17.7/10.0/6.1 (BP = 1.000 ratio = 1.215)`

**Insight:** The adapted model produces longer outputs (ratio 1.215 vs 0.799), improving chrF++ but increasing TER.

---

## Section E: Generation Quality

Spanish Wikipedia prompts, max 128 tokens.

### Greedy Decoding
| Metric | Baseline | Adapted | Delta |
|--------|----------|---------|-------|
| Distinct-1 | 0.3463 | 0.3891 | +12.4% ‚úÖ |
| Distinct-2 | 0.4550 | 0.5261 | +15.6% ‚úÖ |
| Distinct-3 | 0.4930 | 0.5973 | +21.2% ‚úÖ |
| Repetition Rate | 0.4434 | 0.2847 | **-35.8%** ‚úÖ |
| Self-BLEU | 2.40 | 4.29 | +78.8% |

### Nucleus Sampling (p=0.9)
| Metric | Baseline | Adapted | Delta |
|--------|----------|---------|-------|
| Distinct-1 | 0.6278 | 0.5358 | -14.7% |
| Distinct-2 | 0.9027 | 0.7817 | -13.4% |
| Distinct-3 | 0.9637 | 0.8582 | -10.9% |
| Repetition Rate | 0.0128 | 0.0549 | +329% |
| Self-BLEU | 2.37 | 1.62 | **-31.6%** ‚úÖ |

**Key Finding:** The adapted model shows **35.8% less repetition** in greedy decoding, indicating reduced degeneration issues.

---

## Section F: Computational Efficiency

Benchmark: 1000 samples, batch size 8, 3 runs averaged.

| Metric | Baseline | Adapted | Delta |
|--------|----------|---------|-------|
| **Tokens/second** | 619.77 | 496.22 | -19.9% |
| **Samples/second** | 9.68 | 7.75 | -19.9% |
| **TTFT** (ms) | 12.91 | 16.12 | +24.9% |
| **Peak VRAM** (MB) | 2,314 | 3,108 | +34.3% |

**Note:** The adapted model has 41% more parameters (1.43B vs 1.01B), which explains the increased VRAM and slightly lower throughput. When normalized by fertility improvement (9%), effective throughput per semantic unit may be comparable.

---

## Summary & Conclusions

### ‚úÖ Improvements
1. **Tokenizer Efficiency:** 9% fertility reduction on Spanish ‚Üí fewer tokens needed
2. **Compression:** 10.5% better byte-to-token ratio
3. **Generation Diversity:** 35% less repetition in greedy decoding
4. **chrF++:** 13% improvement in character-level MT quality

### ‚ö†Ô∏è Trade-offs
1. **Perplexity:** Higher on both languages (expected for checkpoint-2500)
2. **NLU Tasks:** 3-9% degradation on English benchmarks
3. **BLEU:** 12% lower (but chrF++ improved)
4. **Throughput:** 20% slower (but model is 41% larger)

### üìä Key Takeaway
The adapted model with Qwen2 tokenizer shows **promising tokenizer efficiency gains** for Spanish while maintaining acceptable English performance. Further training beyond checkpoint-2500 is expected to recover NLU performance while retaining tokenizer benefits.

---

## Output Files

```
results/
‚îú‚îÄ‚îÄ tokenizer_analysis_baseline.csv    (257 MB, 2.1M samples)
‚îú‚îÄ‚îÄ tokenizer_analysis_adapted.csv     (257 MB, 2.1M samples)
‚îú‚îÄ‚îÄ tokenizer_comparison.csv           (statistical comparison)
‚îú‚îÄ‚îÄ perplexity_baseline.csv            (5,258 samples)
‚îú‚îÄ‚îÄ perplexity_adapted.csv             (5,258 samples)
‚îú‚îÄ‚îÄ nlu_results_baseline.json          (ARC-Easy, HellaSwag, LAMBADA)
‚îú‚îÄ‚îÄ nlu_results_adapted.json           (ARC-Easy, HellaSwag, LAMBADA)
‚îú‚îÄ‚îÄ mt_results_baseline.csv            (1,000 translations)
‚îú‚îÄ‚îÄ mt_results_adapted.csv             (1,000 translations)
‚îú‚îÄ‚îÄ generation_baseline.csv            (9 prompts √ó 2 methods)
‚îú‚îÄ‚îÄ generation_adapted.csv             (9 prompts √ó 2 methods)
‚îú‚îÄ‚îÄ efficiency_baseline.csv            (throughput metrics)
‚îî‚îÄ‚îÄ efficiency_adapted.csv             (throughput metrics)
```

---

*Generated by TokAlign Evaluation Framework*

