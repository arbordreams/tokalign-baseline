{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Full Model Evaluation - Baseline (Pythia-1B)\n",
        "\n",
        "**TokAlign ACL Publication-Grade Evaluation Framework - Notebook 2/3**\n",
        "\n",
        "Comprehensive evaluation of `EleutherAI/pythia-1b` covering:\n",
        "- **Section A**: Perplexity (Spanish + English catastrophic forgetting check)\n",
        "- **Section B**: Downstream NLU Tasks (Belebele, SIB-200, XCOPA, XNLI)\n",
        "- **Section C**: Machine Translation (FLORES-200, BLEU/chrF++/COMET)\n",
        "- **Section D**: Generation Quality (Distinct-N, repetition, Self-BLEU)\n",
        "- **Section E**: Computational Efficiency (throughput, latency, VRAM)\n",
        "\n",
        "## References\n",
        "- Belebele (Bandarkar et al., ACL 2024) - 221 citations\n",
        "- SIB-200 (Adelani et al., EACL 2023) - 118 citations\n",
        "- CVA Study (Yamaguchi et al., EMNLP 2024) - 271.5% speedup\n",
        "- Branch-and-Merge (Alexandrov et al., EMNLP 2024) - Catastrophic forgetting mitigation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 1: Configuration\n",
        "# =====================\n",
        "\n",
        "# === MODEL CONFIGURATION ===\n",
        "MODEL_PATH = \"EleutherAI/pythia-1b\"\n",
        "MODEL_NAME = \"baseline\"  # Used for output file naming\n",
        "\n",
        "# === EVALUATION SETTINGS ===\n",
        "PERPLEXITY_SAMPLES_ES = 2000\n",
        "PERPLEXITY_SAMPLES_EN = 1000\n",
        "GENERATION_PROMPTS = 500\n",
        "MT_SAMPLES = 1012  # FLORES-200 devtest size\n",
        "EFFICIENCY_SAMPLES = 1000\n",
        "EFFICIENCY_RUNS = 3\n",
        "WARMUP_SAMPLES = 100\n",
        "\n",
        "# === BATCH SIZES ===\n",
        "BATCH_SIZE_PPL = 16\n",
        "BATCH_SIZE_GEN = 8\n",
        "MAX_NEW_TOKENS = 128\n",
        "\n",
        "# === OUTPUT ===\n",
        "OUTPUT_DIR = \"results\"\n",
        "PPL_OUTPUT = f\"{OUTPUT_DIR}/perplexity_{MODEL_NAME}.csv\"\n",
        "NLU_OUTPUT = f\"{OUTPUT_DIR}/nlu_results_{MODEL_NAME}.json\"\n",
        "MT_OUTPUT = f\"{OUTPUT_DIR}/mt_results_{MODEL_NAME}.csv\"\n",
        "GEN_OUTPUT = f\"{OUTPUT_DIR}/generation_{MODEL_NAME}.csv\"\n",
        "EFFICIENCY_OUTPUT = f\"{OUTPUT_DIR}/efficiency_{MODEL_NAME}.csv\"\n",
        "\n",
        "# === THRESHOLDS (per plan) ===\n",
        "ENGLISH_PPL_DEGRADATION_THRESHOLD = 0.05  # <5% increase acceptable\n",
        "ENGLISH_ACCURACY_DEGRADATION_THRESHOLD = 0.02  # <2% drop acceptable\n",
        "\n",
        "RANDOM_SEED = 42\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 2: Install Dependencies\n",
        "!pip install transformers datasets accelerate pandas numpy scipy tqdm pynvml -q\n",
        "!pip install sacrebleu unbabel-comet lm-eval -q\n",
        "!pip install flash-attn --no-build-isolation -q\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 3: Imports and GPU Validation\n",
        "import os\n",
        "import json\n",
        "import time\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pynvml\n",
        "from scipy import stats\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from tqdm.auto import tqdm\n",
        "from typing import Dict, List, Tuple, Optional\n",
        "from collections import Counter\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# GPU Validation\n",
        "assert torch.cuda.is_available(), \"CUDA is not available - GPU required\"\n",
        "compute_capability = torch.cuda.get_device_capability()\n",
        "print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "print(f\"Compute Capability: {compute_capability[0]}.{compute_capability[1]}\")\n",
        "print(f\"CUDA Version: {torch.version.cuda}\")\n",
        "print(f\"PyTorch Version: {torch.__version__}\")\n",
        "\n",
        "# Initialize VRAM monitoring\n",
        "pynvml.nvmlInit()\n",
        "gpu_handle = pynvml.nvmlDeviceGetHandleByIndex(0)\n",
        "\n",
        "# Set random seed\n",
        "np.random.seed(RANDOM_SEED)\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "\n",
        "# Create output directory\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "print(f\"\\nOutput directory: {OUTPUT_DIR}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 4: Load Model\n",
        "# ==================\n",
        "\n",
        "print(f\"Loading model: {MODEL_PATH}\")\n",
        "\n",
        "# Determine if Flash Attention 2 is available\n",
        "use_flash_attn = compute_capability[0] >= 8\n",
        "attn_impl = \"flash_attention_2\" if use_flash_attn else \"eager\"\n",
        "print(f\"Attention implementation: {attn_impl}\")\n",
        "\n",
        "# Load model with optimized settings\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_PATH,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map=\"cuda\",\n",
        "    attn_implementation=attn_impl,\n",
        ")\n",
        "model.eval()\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"left\"  # Required for batched generation\n",
        "\n",
        "print(f\"\\nModel loaded successfully on {next(model.parameters()).device}\")\n",
        "print(f\"Model dtype: {next(model.parameters()).dtype}\")\n",
        "print(f\"Vocab size: {tokenizer.vocab_size}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section A: Perplexity Evaluation\n",
        "\n",
        "Evaluates model perplexity on:\n",
        "1. **Spanish (Target Language)**: OSCAR-2301 Spanish validation\n",
        "2. **English (Catastrophic Forgetting)**: WikiText-2 validation\n",
        "\n",
        "Threshold: <5% PPL increase on English acceptable per Branch-and-Merge methodology.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 5: Perplexity Functions\n",
        "# =============================\n",
        "\n",
        "@torch.no_grad()\n",
        "def calculate_batch_perplexity(texts: List[str], model, tokenizer, max_length: int = 512) -> List[Dict]:\n",
        "    \"\"\"\n",
        "    Vectorized perplexity calculation using cross-entropy loss.\n",
        "    Returns list of dicts with PPL and token counts per sample.\n",
        "    \"\"\"\n",
        "    # Tokenize batch with padding\n",
        "    encodings = tokenizer(\n",
        "        texts,\n",
        "        return_tensors=\"pt\",\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        max_length=max_length,\n",
        "    ).to(\"cuda\")\n",
        "    \n",
        "    input_ids = encodings.input_ids\n",
        "    attention_mask = encodings.attention_mask\n",
        "    \n",
        "    # Forward pass\n",
        "    outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "    logits = outputs.logits\n",
        "    \n",
        "    # Shift logits and labels for causal LM loss calculation\n",
        "    shift_logits = logits[:, :-1, :].contiguous()\n",
        "    shift_labels = input_ids[:, 1:].contiguous()\n",
        "    shift_mask = attention_mask[:, 1:].contiguous()\n",
        "    \n",
        "    # Calculate per-token cross entropy loss\n",
        "    loss_fct = torch.nn.CrossEntropyLoss(reduction='none')\n",
        "    losses = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
        "    losses = losses.view(shift_labels.size())\n",
        "    \n",
        "    # Mask out padding tokens and calculate mean loss per sequence\n",
        "    masked_losses = losses * shift_mask\n",
        "    seq_lengths = shift_mask.sum(dim=1).clamp(min=1)\n",
        "    mean_losses = masked_losses.sum(dim=1) / seq_lengths\n",
        "    \n",
        "    # Convert to perplexity\n",
        "    perplexities = torch.exp(mean_losses)\n",
        "    \n",
        "    # Return detailed results\n",
        "    results = []\n",
        "    for i, (text, ppl, n_tokens) in enumerate(zip(texts, perplexities.cpu().tolist(), seq_lengths.cpu().tolist())):\n",
        "        results.append({\n",
        "            'text': text[:200],\n",
        "            'perplexity': ppl,\n",
        "            'num_tokens': int(n_tokens),\n",
        "            'cross_entropy': mean_losses[i].cpu().item()\n",
        "        })\n",
        "    \n",
        "    return results\n",
        "\n",
        "\n",
        "def evaluate_perplexity(texts: List[str], model, tokenizer, batch_size: int, desc: str) -> pd.DataFrame:\n",
        "    \"\"\"Evaluate perplexity on a corpus.\"\"\"\n",
        "    all_results = []\n",
        "    \n",
        "    for i in tqdm(range(0, len(texts), batch_size), desc=desc):\n",
        "        batch = texts[i:i+batch_size]\n",
        "        batch = [t for t in batch if t and t.strip()]  # Filter empty\n",
        "        if batch:\n",
        "            results = calculate_batch_perplexity(batch, model, tokenizer)\n",
        "            all_results.extend(results)\n",
        "    \n",
        "    return pd.DataFrame(all_results)\n",
        "\n",
        "\n",
        "print(\"Perplexity functions defined.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 6: Load Perplexity Datasets\n",
        "# =================================\n",
        "\n",
        "print(\"Loading perplexity evaluation datasets...\")\n",
        "\n",
        "# Spanish: OSCAR-2301\n",
        "print(\"\\n1. Loading Spanish OSCAR...\")\n",
        "oscar_es = load_dataset(\n",
        "    \"oscar-corpus/OSCAR-2301\",\n",
        "    \"es\",\n",
        "    split=\"train\",\n",
        "    streaming=True,\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "spanish_texts = []\n",
        "for i, sample in enumerate(oscar_es):\n",
        "    if i >= PERPLEXITY_SAMPLES_ES:\n",
        "        break\n",
        "    text = sample.get('text', '')\n",
        "    if text and len(text) > 100:  # Filter short texts\n",
        "        spanish_texts.append(text[:1000])  # Truncate very long texts\n",
        "\n",
        "print(f\"   Loaded {len(spanish_texts)} Spanish samples\")\n",
        "\n",
        "# English: WikiText-2\n",
        "print(\"\\n2. Loading English WikiText-2...\")\n",
        "wikitext = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"validation\")\n",
        "english_texts = [t for t in wikitext['text'] if t and len(t) > 100][:PERPLEXITY_SAMPLES_EN]\n",
        "print(f\"   Loaded {len(english_texts)} English samples\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 7: Run Perplexity Evaluation\n",
        "# ==================================\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"PERPLEXITY EVALUATION\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Spanish perplexity\n",
        "print(\"\\n--- Spanish (Target Language) ---\")\n",
        "spanish_ppl_df = evaluate_perplexity(spanish_texts, model, tokenizer, BATCH_SIZE_PPL, \"Spanish PPL\")\n",
        "\n",
        "# English perplexity\n",
        "print(\"\\n--- English (Catastrophic Forgetting Check) ---\")\n",
        "english_ppl_df = evaluate_perplexity(english_texts, model, tokenizer, BATCH_SIZE_PPL, \"English PPL\")\n",
        "\n",
        "# Combine and save\n",
        "spanish_ppl_df['language'] = 'es'\n",
        "english_ppl_df['language'] = 'en'\n",
        "ppl_df = pd.concat([spanish_ppl_df, english_ppl_df], ignore_index=True)\n",
        "ppl_df.to_csv(PPL_OUTPUT, index=False)\n",
        "print(f\"\\nPerplexity results saved to: {PPL_OUTPUT}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 8: Perplexity Summary Statistics\n",
        "# ======================================\n",
        "\n",
        "def ppl_summary(df: pd.DataFrame, lang: str) -> Dict:\n",
        "    \"\"\"Compute perplexity summary statistics.\"\"\"\n",
        "    ppl_vals = df[df['language'] == lang]['perplexity'].values\n",
        "    return {\n",
        "        'language': lang,\n",
        "        'mean': np.mean(ppl_vals),\n",
        "        'median': np.median(ppl_vals),\n",
        "        'std': np.std(ppl_vals),\n",
        "        'p5': np.percentile(ppl_vals, 5),\n",
        "        'p25': np.percentile(ppl_vals, 25),\n",
        "        'p50': np.percentile(ppl_vals, 50),\n",
        "        'p75': np.percentile(ppl_vals, 75),\n",
        "        'p95': np.percentile(ppl_vals, 95),\n",
        "        'min': np.min(ppl_vals),\n",
        "        'max': np.max(ppl_vals),\n",
        "        'n_samples': len(ppl_vals)\n",
        "    }\n",
        "\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"PERPLEXITY SUMMARY\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Spanish summary\n",
        "es_summary = ppl_summary(ppl_df, 'es')\n",
        "print(f\"\\n--- SPANISH ---\")\n",
        "print(f\"  Mean PPL:   {es_summary['mean']:.2f}\")\n",
        "print(f\"  Median PPL: {es_summary['median']:.2f}\")\n",
        "print(f\"  Std PPL:    {es_summary['std']:.2f}\")\n",
        "print(f\"  Percentiles: P5={es_summary['p5']:.2f}, P25={es_summary['p25']:.2f}, \"\n",
        "      f\"P50={es_summary['p50']:.2f}, P75={es_summary['p75']:.2f}, P95={es_summary['p95']:.2f}\")\n",
        "\n",
        "# English summary\n",
        "en_summary = ppl_summary(ppl_df, 'en')\n",
        "print(f\"\\n--- ENGLISH ---\")\n",
        "print(f\"  Mean PPL:   {en_summary['mean']:.2f}\")\n",
        "print(f\"  Median PPL: {en_summary['median']:.2f}\")\n",
        "print(f\"  Std PPL:    {en_summary['std']:.2f}\")\n",
        "print(f\"  Percentiles: P5={en_summary['p5']:.2f}, P25={en_summary['p25']:.2f}, \"\n",
        "      f\"P50={en_summary['p50']:.2f}, P75={en_summary['p75']:.2f}, P95={en_summary['p95']:.2f}\")\n",
        "\n",
        "# Store for later comparison\n",
        "BASELINE_ENGLISH_PPL = en_summary['mean']\n",
        "print(f\"\\n[BASELINE] English Mean PPL: {BASELINE_ENGLISH_PPL:.2f} (reference for catastrophic forgetting)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section B: Downstream NLU Tasks\n",
        "\n",
        "Using lm-evaluation-harness for standardized benchmarks:\n",
        "- **Belebele** (spa_Latn + eng_Latn): Reading comprehension, 221 citations\n",
        "- **SIB-200** (spa_Latn): Topic classification, 118 citations  \n",
        "- **XCOPA** (et/translation_es): Commonsense reasoning\n",
        "- **XNLI** (es): Natural language inference\n",
        "\n",
        "English regression tests: arc_easy, hellaswag, lambada_openai\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 9: NLU Evaluation via lm-evaluation-harness\n",
        "# ================================================\n",
        "\n",
        "# Note: lm-eval tasks are run via command line for best compatibility\n",
        "# This cell generates the command and parses results\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"NLU EVALUATION (lm-evaluation-harness)\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Define tasks to evaluate\n",
        "SPANISH_TASKS = [\n",
        "    \"belebele_spa_latn\",  # Reading comprehension\n",
        "    # \"sib200_spa_latn\",    # Topic classification (if available)\n",
        "    # \"xcopa_translation-es\",  # Commonsense (if available)\n",
        "]\n",
        "\n",
        "ENGLISH_TASKS = [\n",
        "    \"belebele_eng_latn\",  # Reading comprehension (English baseline)\n",
        "    \"arc_easy\",           # Regression test\n",
        "    \"hellaswag\",          # Regression test\n",
        "    \"lambada_openai\",     # Regression test\n",
        "]\n",
        "\n",
        "ALL_TASKS = SPANISH_TASKS + ENGLISH_TASKS\n",
        "\n",
        "print(\"\\nTasks to evaluate:\")\n",
        "for task in ALL_TASKS:\n",
        "    print(f\"  - {task}\")\n",
        "\n",
        "# Generate lm-eval command\n",
        "lm_eval_cmd = f\"\"\"\n",
        "# Run this command in terminal:\n",
        "lm_eval --model hf \\\\\n",
        "    --model_args pretrained={MODEL_PATH},dtype=bfloat16 \\\\\n",
        "    --tasks {','.join(ALL_TASKS)} \\\\\n",
        "    --batch_size auto \\\\\n",
        "    --output_path {NLU_OUTPUT.replace('.json', '')}\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"LM-EVAL COMMAND\")\n",
        "print(\"=\" * 70)\n",
        "print(lm_eval_cmd)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 10: Run lm-eval (alternative: programmatic API)\n",
        "# ====================================================\n",
        "\n",
        "try:\n",
        "    from lm_eval import evaluator\n",
        "    from lm_eval.models.huggingface import HFLM\n",
        "    \n",
        "    print(\"Running lm-evaluation-harness programmatically...\")\n",
        "    \n",
        "    # Create model wrapper using our already-loaded model\n",
        "    lm = HFLM(\n",
        "        pretrained=model,\n",
        "        tokenizer=tokenizer,\n",
        "        batch_size=\"auto\"\n",
        "    )\n",
        "    \n",
        "    # Run evaluation on available tasks\n",
        "    # Note: Some tasks may not be available in all lm-eval versions\n",
        "    available_tasks = []\n",
        "    for task in ALL_TASKS:\n",
        "        try:\n",
        "            results = evaluator.simple_evaluate(\n",
        "                model=lm,\n",
        "                tasks=[task],\n",
        "                batch_size=\"auto\",\n",
        "                log_samples=False\n",
        "            )\n",
        "            available_tasks.append(task)\n",
        "            print(f\"  [OK] {task}\")\n",
        "        except Exception as e:\n",
        "            print(f\"  [SKIP] {task}: {str(e)[:50]}...\")\n",
        "    \n",
        "    if available_tasks:\n",
        "        # Full evaluation on available tasks\n",
        "        nlu_results = evaluator.simple_evaluate(\n",
        "            model=lm,\n",
        "            tasks=available_tasks,\n",
        "            batch_size=\"auto\",\n",
        "            log_samples=False\n",
        "        )\n",
        "        \n",
        "        # Save results\n",
        "        with open(NLU_OUTPUT, 'w') as f:\n",
        "            json.dump(nlu_results, f, indent=2, default=str)\n",
        "        print(f\"\\nNLU results saved to: {NLU_OUTPUT}\")\n",
        "    else:\n",
        "        print(\"\\nNo tasks available. Run lm-eval command manually.\")\n",
        "        nlu_results = None\n",
        "        \n",
        "except ImportError:\n",
        "    print(\"lm-eval not installed. Run the command above manually.\")\n",
        "    nlu_results = None\n",
        "except Exception as e:\n",
        "    print(f\"Error running lm-eval: {e}\")\n",
        "    print(\"Run the command above manually.\")\n",
        "    nlu_results = None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 11: Parse and Display NLU Results\n",
        "# ======================================\n",
        "\n",
        "def parse_nlu_results(results_path: str) -> pd.DataFrame:\n",
        "    \"\"\"Parse lm-eval JSON results into DataFrame.\"\"\"\n",
        "    try:\n",
        "        with open(results_path, 'r') as f:\n",
        "            results = json.load(f)\n",
        "        \n",
        "        rows = []\n",
        "        for task, metrics in results.get('results', {}).items():\n",
        "            row = {'task': task}\n",
        "            for metric, value in metrics.items():\n",
        "                if isinstance(value, (int, float)):\n",
        "                    row[metric] = value\n",
        "            rows.append(row)\n",
        "        \n",
        "        return pd.DataFrame(rows)\n",
        "    except Exception as e:\n",
        "        print(f\"Could not parse results: {e}\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "\n",
        "if nlu_results:\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"NLU RESULTS SUMMARY\")\n",
        "    print(\"=\" * 70)\n",
        "    \n",
        "    results_df = []\n",
        "    for task, metrics in nlu_results.get('results', {}).items():\n",
        "        acc = metrics.get('acc,none', metrics.get('acc_norm,none', metrics.get('acc', 'N/A')))\n",
        "        acc_stderr = metrics.get('acc_stderr,none', metrics.get('acc_norm_stderr,none', 'N/A'))\n",
        "        print(f\"\\n{task}:\")\n",
        "        print(f\"  Accuracy: {acc:.4f} (± {acc_stderr:.4f})\" if isinstance(acc, float) else f\"  Accuracy: {acc}\")\n",
        "        results_df.append({\n",
        "            'task': task,\n",
        "            'accuracy': acc,\n",
        "            'stderr': acc_stderr\n",
        "        })\n",
        "    \n",
        "    nlu_df = pd.DataFrame(results_df)\n",
        "    print(\"\\n\" + nlu_df.to_string(index=False))\n",
        "else:\n",
        "    print(\"NLU results not available. Run lm-eval manually and reload results.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section C: Machine Translation Evaluation\n",
        "\n",
        "FLORES-200 style evaluation:\n",
        "- Direction: Spanish → English\n",
        "- Metrics: BLEU, chrF++, COMET, TER (per WMT standards)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 12: MT Evaluation Functions\n",
        "# =================================\n",
        "\n",
        "@torch.no_grad()\n",
        "def generate_translations(spanish_texts: List[str], model, tokenizer, \n",
        "                          max_new_tokens: int = 128, batch_size: int = 8) -> List[str]:\n",
        "    \"\"\"Generate English translations from Spanish inputs.\"\"\"\n",
        "    all_translations = []\n",
        "    \n",
        "    for i in tqdm(range(0, len(spanish_texts), batch_size), desc=\"Translating\"):\n",
        "        batch = spanish_texts[i:i+batch_size]\n",
        "        \n",
        "        # Format prompts\n",
        "        prompts = [f\"Spanish: {text}\\nEnglish:\" for text in batch]\n",
        "        \n",
        "        # Tokenize\n",
        "        encodings = tokenizer(\n",
        "            prompts,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            max_length=512,\n",
        "        ).to(\"cuda\")\n",
        "        \n",
        "        # Generate\n",
        "        generated_ids = model.generate(\n",
        "            input_ids=encodings.input_ids,\n",
        "            attention_mask=encodings.attention_mask,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            do_sample=False,\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "            eos_token_id=tokenizer.eos_token_id,\n",
        "        )\n",
        "        \n",
        "        # Decode only new tokens\n",
        "        input_length = encodings.input_ids.shape[1]\n",
        "        translations = tokenizer.batch_decode(\n",
        "            generated_ids[:, input_length:],\n",
        "            skip_special_tokens=True\n",
        "        )\n",
        "        \n",
        "        # Clean up\n",
        "        cleaned = []\n",
        "        for t in translations:\n",
        "            # Stop at newline\n",
        "            if \"\\n\" in t:\n",
        "                t = t.split(\"\\n\")[0]\n",
        "            cleaned.append(t.strip())\n",
        "        \n",
        "        all_translations.extend(cleaned)\n",
        "    \n",
        "    return all_translations\n",
        "\n",
        "\n",
        "print(\"MT evaluation functions defined.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 13: Load MT Dataset and Generate Translations\n",
        "# ===================================================\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"MACHINE TRANSLATION EVALUATION\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Try to load FLORES-200\n",
        "print(\"\\nLoading translation dataset...\")\n",
        "try:\n",
        "    flores = load_dataset(\"facebook/flores\", \"spa_Latn-eng_Latn\", split=\"devtest\")\n",
        "    spanish_sources = [x['sentence_spa_Latn'] for x in flores][:MT_SAMPLES]\n",
        "    english_references = [x['sentence_eng_Latn'] for x in flores][:MT_SAMPLES]\n",
        "    print(f\"Loaded FLORES-200: {len(spanish_sources)} samples\")\n",
        "except Exception as e:\n",
        "    print(f\"FLORES-200 not available: {e}\")\n",
        "    print(\"Falling back to globalvoices dataset...\")\n",
        "    gv = load_dataset(\"alvations/globalvoices-en-es\", split=\"train\")\n",
        "    gv = gv.shuffle(seed=RANDOM_SEED).select(range(min(MT_SAMPLES, len(gv))))\n",
        "    spanish_sources = [x['es'] for x in gv]\n",
        "    english_references = [x['en'] for x in gv]\n",
        "    print(f\"Loaded GlobalVoices: {len(spanish_sources)} samples\")\n",
        "\n",
        "# Generate translations\n",
        "print(\"\\nGenerating translations...\")\n",
        "hypotheses = generate_translations(spanish_sources, model, tokenizer, MAX_NEW_TOKENS, BATCH_SIZE_GEN)\n",
        "print(f\"Generated {len(hypotheses)} translations\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 14: Compute MT Metrics\n",
        "# ============================\n",
        "\n",
        "from sacrebleu import corpus_bleu, corpus_chrf, corpus_ter\n",
        "\n",
        "print(\"\\nComputing MT metrics...\")\n",
        "\n",
        "# BLEU (with signature for reproducibility)\n",
        "bleu_result = corpus_bleu(hypotheses, [english_references])\n",
        "bleu_score = bleu_result.score\n",
        "bleu_signature = bleu_result.get_signature()\n",
        "\n",
        "# chrF++ (word_order=2 for chrF++)\n",
        "chrf_result = corpus_chrf(hypotheses, [english_references], word_order=2)\n",
        "chrf_score = chrf_result.score\n",
        "\n",
        "# TER\n",
        "ter_result = corpus_ter(hypotheses, [english_references])\n",
        "ter_score = ter_result.score\n",
        "\n",
        "print(f\"\\n--- MT Metrics ---\")\n",
        "print(f\"BLEU:   {bleu_score:.2f}\")\n",
        "print(f\"chrF++: {chrf_score:.2f}\")\n",
        "print(f\"TER:    {ter_score:.2f}\")\n",
        "print(f\"\\nBLEU signature: {bleu_signature}\")\n",
        "\n",
        "# Try COMET if available\n",
        "try:\n",
        "    from comet import download_model, load_from_checkpoint\n",
        "    \n",
        "    print(\"\\nComputing COMET score...\")\n",
        "    comet_model_path = download_model(\"Unbabel/wmt22-comet-da\")\n",
        "    comet_model = load_from_checkpoint(comet_model_path)\n",
        "    \n",
        "    comet_data = [\n",
        "        {\"src\": src, \"mt\": hyp, \"ref\": ref}\n",
        "        for src, hyp, ref in zip(spanish_sources, hypotheses, english_references)\n",
        "    ]\n",
        "    comet_output = comet_model.predict(comet_data, batch_size=8, gpus=1)\n",
        "    comet_score = comet_output.system_score\n",
        "    print(f\"COMET:  {comet_score:.4f}\")\n",
        "except Exception as e:\n",
        "    print(f\"COMET not available: {e}\")\n",
        "    comet_score = None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 15: Save MT Results\n",
        "# =========================\n",
        "\n",
        "mt_results = {\n",
        "    'source': spanish_sources,\n",
        "    'reference': english_references,\n",
        "    'hypothesis': hypotheses,\n",
        "}\n",
        "mt_df = pd.DataFrame(mt_results)\n",
        "mt_df.to_csv(MT_OUTPUT, index=False)\n",
        "\n",
        "# Summary metrics\n",
        "mt_summary = {\n",
        "    'bleu': bleu_score,\n",
        "    'chrf++': chrf_score,\n",
        "    'ter': ter_score,\n",
        "    'comet': comet_score,\n",
        "    'bleu_signature': str(bleu_signature),\n",
        "    'n_samples': len(hypotheses)\n",
        "}\n",
        "\n",
        "print(f\"\\nMT results saved to: {MT_OUTPUT}\")\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"MT SUMMARY\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"BLEU:   {bleu_score:.2f}\")\n",
        "print(f\"chrF++: {chrf_score:.2f}\")\n",
        "print(f\"TER:    {ter_score:.2f}\")\n",
        "if comet_score:\n",
        "    print(f\"COMET:  {comet_score:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section D: Generation Quality Analysis\n",
        "\n",
        "Metrics per HelloBench and Contrastive Decoding papers:\n",
        "- **Distinct-1/2/3**: Unique n-gram ratios (diversity)\n",
        "- **Repetition Rate**: % repeated 4-grams (degeneration)\n",
        "- **Self-BLEU**: Corpus-level diversity (lower = more diverse)\n",
        "- **Length Ratio**: Output length vs expected\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 16: Generation Quality Functions\n",
        "# ======================================\n",
        "\n",
        "def get_ngrams(text: str, n: int) -> List[Tuple]:\n",
        "    \"\"\"Extract n-grams from text.\"\"\"\n",
        "    tokens = text.split()\n",
        "    if len(tokens) < n:\n",
        "        return []\n",
        "    return [tuple(tokens[i:i+n]) for i in range(len(tokens) - n + 1)]\n",
        "\n",
        "\n",
        "def distinct_n(texts: List[str], n: int) -> float:\n",
        "    \"\"\"\n",
        "    Distinct-N: Ratio of unique n-grams to total n-grams.\n",
        "    Higher = more diverse generation.\n",
        "    \"\"\"\n",
        "    all_ngrams = []\n",
        "    for text in texts:\n",
        "        all_ngrams.extend(get_ngrams(text, n))\n",
        "    \n",
        "    if not all_ngrams:\n",
        "        return 0.0\n",
        "    \n",
        "    return len(set(all_ngrams)) / len(all_ngrams)\n",
        "\n",
        "\n",
        "def repetition_rate(texts: List[str], n: int = 4) -> float:\n",
        "    \"\"\"\n",
        "    Repetition rate: Proportion of n-grams that appear more than once.\n",
        "    Lower = less repetitive (better).\n",
        "    \"\"\"\n",
        "    all_ngrams = []\n",
        "    for text in texts:\n",
        "        all_ngrams.extend(get_ngrams(text, n))\n",
        "    \n",
        "    if not all_ngrams:\n",
        "        return 0.0\n",
        "    \n",
        "    ngram_counts = Counter(all_ngrams)\n",
        "    repeated = sum(1 for count in ngram_counts.values() if count > 1)\n",
        "    \n",
        "    return repeated / len(ngram_counts) if ngram_counts else 0.0\n",
        "\n",
        "\n",
        "def self_bleu(texts: List[str], sample_size: int = 100) -> float:\n",
        "    \"\"\"\n",
        "    Self-BLEU: Average BLEU of each text against all others.\n",
        "    Lower = more diverse corpus.\n",
        "    \"\"\"\n",
        "    from sacrebleu import sentence_bleu\n",
        "    \n",
        "    if len(texts) < 2:\n",
        "        return 0.0\n",
        "    \n",
        "    # Sample for efficiency\n",
        "    if len(texts) > sample_size:\n",
        "        indices = np.random.choice(len(texts), sample_size, replace=False)\n",
        "        texts = [texts[i] for i in indices]\n",
        "    \n",
        "    scores = []\n",
        "    for i, hyp in enumerate(texts):\n",
        "        refs = [texts[j] for j in range(len(texts)) if j != i]\n",
        "        if refs and hyp:\n",
        "            score = sentence_bleu(hyp, refs[:5]).score  # Use up to 5 refs\n",
        "            scores.append(score)\n",
        "    \n",
        "    return np.mean(scores) if scores else 0.0\n",
        "\n",
        "\n",
        "def analyze_generation_quality(texts: List[str]) -> Dict:\n",
        "    \"\"\"Compute all generation quality metrics.\"\"\"\n",
        "    # Filter empty texts\n",
        "    texts = [t for t in texts if t and t.strip()]\n",
        "    \n",
        "    return {\n",
        "        'distinct_1': distinct_n(texts, 1),\n",
        "        'distinct_2': distinct_n(texts, 2),\n",
        "        'distinct_3': distinct_n(texts, 3),\n",
        "        'repetition_rate_4gram': repetition_rate(texts, 4),\n",
        "        'self_bleu': self_bleu(texts),\n",
        "        'avg_length': np.mean([len(t.split()) for t in texts]),\n",
        "        'n_samples': len(texts)\n",
        "    }\n",
        "\n",
        "\n",
        "print(\"Generation quality functions defined.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 17: Load Generation Prompts and Generate\n",
        "# ==============================================\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"GENERATION QUALITY ANALYSIS\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Load Spanish prompts from OSCAR\n",
        "print(\"\\nLoading Spanish prompts...\")\n",
        "oscar_prompts = load_dataset(\n",
        "    \"oscar-corpus/OSCAR-2301\",\n",
        "    \"es\",\n",
        "    split=\"train\",\n",
        "    streaming=True,\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "# Filter for good prompt length (50-200 chars, complete sentences)\n",
        "prompts = []\n",
        "for sample in oscar_prompts:\n",
        "    text = sample.get('text', '')\n",
        "    if 50 <= len(text) <= 200 and text.endswith(('.', '!', '?')):\n",
        "        prompts.append(text)\n",
        "        if len(prompts) >= GENERATION_PROMPTS:\n",
        "            break\n",
        "\n",
        "print(f\"Collected {len(prompts)} Spanish prompts\")\n",
        "\n",
        "# Generate with greedy decoding\n",
        "print(\"\\nGenerating with greedy decoding...\")\n",
        "greedy_outputs = []\n",
        "for i in tqdm(range(0, len(prompts), BATCH_SIZE_GEN), desc=\"Greedy generation\"):\n",
        "    batch = prompts[i:i+BATCH_SIZE_GEN]\n",
        "    \n",
        "    encodings = tokenizer(\n",
        "        batch,\n",
        "        return_tensors=\"pt\",\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        max_length=256,\n",
        "    ).to(\"cuda\")\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        generated = model.generate(\n",
        "            input_ids=encodings.input_ids,\n",
        "            attention_mask=encodings.attention_mask,\n",
        "            max_new_tokens=64,\n",
        "            do_sample=False,\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "        )\n",
        "    \n",
        "    input_len = encodings.input_ids.shape[1]\n",
        "    outputs = tokenizer.batch_decode(generated[:, input_len:], skip_special_tokens=True)\n",
        "    greedy_outputs.extend(outputs)\n",
        "\n",
        "# Generate with nucleus sampling\n",
        "print(\"\\nGenerating with nucleus sampling (p=0.9)...\")\n",
        "nucleus_outputs = []\n",
        "for i in tqdm(range(0, len(prompts), BATCH_SIZE_GEN), desc=\"Nucleus generation\"):\n",
        "    batch = prompts[i:i+BATCH_SIZE_GEN]\n",
        "    \n",
        "    encodings = tokenizer(\n",
        "        batch,\n",
        "        return_tensors=\"pt\",\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        max_length=256,\n",
        "    ).to(\"cuda\")\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        generated = model.generate(\n",
        "            input_ids=encodings.input_ids,\n",
        "            attention_mask=encodings.attention_mask,\n",
        "            max_new_tokens=64,\n",
        "            do_sample=True,\n",
        "            top_p=0.9,\n",
        "            temperature=1.0,\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "        )\n",
        "    \n",
        "    input_len = encodings.input_ids.shape[1]\n",
        "    outputs = tokenizer.batch_decode(generated[:, input_len:], skip_special_tokens=True)\n",
        "    nucleus_outputs.extend(outputs)\n",
        "\n",
        "print(f\"\\nGenerated {len(greedy_outputs)} greedy and {len(nucleus_outputs)} nucleus samples\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 18: Compute Generation Quality Metrics\n",
        "# ============================================\n",
        "\n",
        "print(\"\\nComputing generation quality metrics...\")\n",
        "\n",
        "greedy_metrics = analyze_generation_quality(greedy_outputs)\n",
        "nucleus_metrics = analyze_generation_quality(nucleus_outputs)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"GENERATION QUALITY SUMMARY\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "print(f\"\\n{'Metric':<25} {'Greedy':>12} {'Nucleus (p=0.9)':>15}\")\n",
        "print(\"-\" * 55)\n",
        "print(f\"{'Distinct-1':<25} {greedy_metrics['distinct_1']:>12.4f} {nucleus_metrics['distinct_1']:>15.4f}\")\n",
        "print(f\"{'Distinct-2':<25} {greedy_metrics['distinct_2']:>12.4f} {nucleus_metrics['distinct_2']:>15.4f}\")\n",
        "print(f\"{'Distinct-3':<25} {greedy_metrics['distinct_3']:>12.4f} {nucleus_metrics['distinct_3']:>15.4f}\")\n",
        "print(f\"{'Repetition Rate (4-gram)':<25} {greedy_metrics['repetition_rate_4gram']:>12.4f} {nucleus_metrics['repetition_rate_4gram']:>15.4f}\")\n",
        "print(f\"{'Self-BLEU':<25} {greedy_metrics['self_bleu']:>12.2f} {nucleus_metrics['self_bleu']:>15.2f}\")\n",
        "print(f\"{'Avg Length (words)':<25} {greedy_metrics['avg_length']:>12.1f} {nucleus_metrics['avg_length']:>15.1f}\")\n",
        "\n",
        "# Save results\n",
        "gen_results = pd.DataFrame({\n",
        "    'prompt': prompts,\n",
        "    'greedy_output': greedy_outputs,\n",
        "    'nucleus_output': nucleus_outputs\n",
        "})\n",
        "gen_results.to_csv(GEN_OUTPUT, index=False)\n",
        "print(f\"\\nGeneration results saved to: {GEN_OUTPUT}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section E: Computational Efficiency\n",
        "\n",
        "Protocol per CVA study:\n",
        "- Warm-up: 100 samples discarded\n",
        "- Measurement: 1000 samples, 3 runs\n",
        "- Metrics: Tokens/sec, TTFT (Time to First Token), Peak VRAM\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 19: Efficiency Benchmark Functions\n",
        "# =======================================\n",
        "\n",
        "def get_vram_mb() -> float:\n",
        "    \"\"\"Get current VRAM usage in MB.\"\"\"\n",
        "    info = pynvml.nvmlDeviceGetMemoryInfo(gpu_handle)\n",
        "    return info.used / (1024 ** 2)\n",
        "\n",
        "\n",
        "def benchmark_throughput(texts: List[str], model, tokenizer, \n",
        "                         batch_size: int, max_new_tokens: int,\n",
        "                         warmup: int = 100) -> Dict:\n",
        "    \"\"\"\n",
        "    Benchmark generation throughput.\n",
        "    \n",
        "    Returns:\n",
        "        - tokens_per_second: Average generation throughput\n",
        "        - time_to_first_token: Average TTFT\n",
        "        - peak_vram_mb: Peak VRAM during benchmark\n",
        "    \"\"\"\n",
        "    # Warm-up\n",
        "    print(f\"  Warming up ({warmup} samples)...\")\n",
        "    for i in range(0, min(warmup, len(texts)), batch_size):\n",
        "        batch = texts[i:i+batch_size]\n",
        "        encodings = tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True, max_length=256).to(\"cuda\")\n",
        "        with torch.no_grad():\n",
        "            _ = model.generate(\n",
        "                input_ids=encodings.input_ids,\n",
        "                attention_mask=encodings.attention_mask,\n",
        "                max_new_tokens=max_new_tokens,\n",
        "                do_sample=False,\n",
        "                pad_token_id=tokenizer.eos_token_id,\n",
        "            )\n",
        "    torch.cuda.synchronize()\n",
        "    \n",
        "    # Reset peak memory\n",
        "    torch.cuda.reset_peak_memory_stats()\n",
        "    \n",
        "    # Benchmark\n",
        "    total_tokens = 0\n",
        "    total_time = 0\n",
        "    ttft_times = []\n",
        "    \n",
        "    print(f\"  Benchmarking ({len(texts)} samples)...\")\n",
        "    for i in tqdm(range(0, len(texts), batch_size), desc=\"  Throughput\"):\n",
        "        batch = texts[i:i+batch_size]\n",
        "        \n",
        "        encodings = tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True, max_length=256).to(\"cuda\")\n",
        "        input_len = encodings.input_ids.shape[1]\n",
        "        \n",
        "        torch.cuda.synchronize()\n",
        "        start_time = time.perf_counter()\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            generated = model.generate(\n",
        "                input_ids=encodings.input_ids,\n",
        "                attention_mask=encodings.attention_mask,\n",
        "                max_new_tokens=max_new_tokens,\n",
        "                do_sample=False,\n",
        "                pad_token_id=tokenizer.eos_token_id,\n",
        "            )\n",
        "        \n",
        "        torch.cuda.synchronize()\n",
        "        end_time = time.perf_counter()\n",
        "        \n",
        "        batch_time = end_time - start_time\n",
        "        batch_tokens = (generated.shape[1] - input_len) * generated.shape[0]\n",
        "        \n",
        "        total_tokens += batch_tokens\n",
        "        total_time += batch_time\n",
        "        \n",
        "        # Estimate TTFT (first token is generated quickly)\n",
        "        ttft_times.append(batch_time / max(generated.shape[1] - input_len, 1))\n",
        "    \n",
        "    peak_vram = torch.cuda.max_memory_allocated() / (1024 ** 2)\n",
        "    \n",
        "    return {\n",
        "        'tokens_per_second': total_tokens / total_time,\n",
        "        'samples_per_second': len(texts) / total_time,\n",
        "        'avg_ttft_ms': np.mean(ttft_times) * 1000,\n",
        "        'total_time_s': total_time,\n",
        "        'total_tokens': total_tokens,\n",
        "        'peak_vram_mb': peak_vram\n",
        "    }\n",
        "\n",
        "\n",
        "print(\"Efficiency benchmark functions defined.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 20: Run Efficiency Benchmark\n",
        "# ==================================\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"COMPUTATIONAL EFFICIENCY BENCHMARK\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Prepare benchmark texts (use Spanish texts for target language efficiency)\n",
        "efficiency_texts = spanish_texts[:EFFICIENCY_SAMPLES]\n",
        "print(f\"\\nBenchmark corpus: {len(efficiency_texts)} Spanish samples\")\n",
        "\n",
        "# Run multiple times for stability\n",
        "all_runs = []\n",
        "for run in range(EFFICIENCY_RUNS):\n",
        "    print(f\"\\n--- Run {run + 1}/{EFFICIENCY_RUNS} ---\")\n",
        "    run_results = benchmark_throughput(\n",
        "        efficiency_texts, model, tokenizer,\n",
        "        batch_size=BATCH_SIZE_GEN,\n",
        "        max_new_tokens=64,\n",
        "        warmup=WARMUP_SAMPLES\n",
        "    )\n",
        "    all_runs.append(run_results)\n",
        "    print(f\"  Tokens/sec: {run_results['tokens_per_second']:.1f}\")\n",
        "    print(f\"  Peak VRAM: {run_results['peak_vram_mb']:.0f} MB\")\n",
        "\n",
        "# Aggregate results\n",
        "efficiency_summary = {\n",
        "    'tokens_per_second_mean': np.mean([r['tokens_per_second'] for r in all_runs]),\n",
        "    'tokens_per_second_std': np.std([r['tokens_per_second'] for r in all_runs]),\n",
        "    'samples_per_second_mean': np.mean([r['samples_per_second'] for r in all_runs]),\n",
        "    'avg_ttft_ms': np.mean([r['avg_ttft_ms'] for r in all_runs]),\n",
        "    'peak_vram_mb': max([r['peak_vram_mb'] for r in all_runs]),\n",
        "    'n_samples': len(efficiency_texts),\n",
        "    'n_runs': EFFICIENCY_RUNS,\n",
        "    'batch_size': BATCH_SIZE_GEN\n",
        "}\n",
        "\n",
        "# Save results\n",
        "efficiency_df = pd.DataFrame([efficiency_summary])\n",
        "efficiency_df.to_csv(EFFICIENCY_OUTPUT, index=False)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"EFFICIENCY SUMMARY\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"Tokens/second:     {efficiency_summary['tokens_per_second_mean']:.1f} ± {efficiency_summary['tokens_per_second_std']:.1f}\")\n",
        "print(f\"Samples/second:    {efficiency_summary['samples_per_second_mean']:.2f}\")\n",
        "print(f\"Avg TTFT:          {efficiency_summary['avg_ttft_ms']:.2f} ms\")\n",
        "print(f\"Peak VRAM:         {efficiency_summary['peak_vram_mb']:.0f} MB\")\n",
        "print(f\"\\nResults saved to: {EFFICIENCY_OUTPUT}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 21: Final Summary and Cleanup\n",
        "# ===================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"EVALUATION COMPLETE\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "print(f\"\\nModel: {MODEL_PATH}\")\n",
        "print(f\"Model Name: {MODEL_NAME}\")\n",
        "\n",
        "print(\"\\n--- Output Files ---\")\n",
        "print(f\"  Perplexity:   {PPL_OUTPUT}\")\n",
        "print(f\"  NLU Results:  {NLU_OUTPUT}\")\n",
        "print(f\"  MT Results:   {MT_OUTPUT}\")\n",
        "print(f\"  Generation:   {GEN_OUTPUT}\")\n",
        "print(f\"  Efficiency:   {EFFICIENCY_OUTPUT}\")\n",
        "\n",
        "print(\"\\n--- Key Metrics Summary ---\")\n",
        "print(f\"  Spanish PPL (mean):      {es_summary['mean']:.2f}\")\n",
        "print(f\"  English PPL (mean):      {en_summary['mean']:.2f}\")\n",
        "print(f\"  MT BLEU:                 {bleu_score:.2f}\")\n",
        "print(f\"  MT chrF++:               {chrf_score:.2f}\")\n",
        "print(f\"  Distinct-2 (greedy):     {greedy_metrics['distinct_2']:.4f}\")\n",
        "print(f\"  Throughput (tok/s):      {efficiency_summary['tokens_per_second_mean']:.1f}\")\n",
        "print(f\"  Peak VRAM:               {efficiency_summary['peak_vram_mb']:.0f} MB\")\n",
        "\n",
        "# Cleanup\n",
        "pynvml.nvmlShutdown()\n",
        "print(\"\\n[BASELINE EVALUATION COMPLETE]\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
