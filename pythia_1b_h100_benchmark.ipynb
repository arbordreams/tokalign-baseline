{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 1: Package Installation\n",
        "!pip install transformers datasets accelerate pandas nvidia-ml-py flash-attn --no-build-isolation -q\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 2: Imports and GPU Validation\n",
        "import torch\n",
        "import pandas as pd\n",
        "import pynvml\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import time\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# Hard check for H100/A100 GPU (compute capability >= 8.0 required for Flash Attention 2)\n",
        "assert torch.cuda.is_available(), \"CUDA is not available - H100 GPU required\"\n",
        "compute_capability = torch.cuda.get_device_capability()\n",
        "assert compute_capability[0] >= 8, f\"H100/A100 GPU required for Flash Attention 2 (got compute capability {compute_capability})\"\n",
        "\n",
        "# Initialize pynvml for VRAM monitoring\n",
        "pynvml.nvmlInit()\n",
        "gpu_handle = pynvml.nvmlDeviceGetHandleByIndex(0)\n",
        "\n",
        "print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "print(f\"Compute Capability: {compute_capability[0]}.{compute_capability[1]}\")\n",
        "print(f\"CUDA Version: {torch.version.cuda}\")\n",
        "print(f\"PyTorch Version: {torch.__version__}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 3: Model Loading (H100 Native)\n",
        "MODEL_NAME = \"EleutherAI/pythia-1b\"\n",
        "\n",
        "print(f\"Loading {MODEL_NAME} with Flash Attention 2...\")\n",
        "\n",
        "# Load model with H100-optimized settings\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map=\"cuda\",\n",
        "    attn_implementation=\"flash_attention_2\",\n",
        ")\n",
        "model.eval()\n",
        "\n",
        "# Load tokenizer with proper padding configuration for batched generation\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"left\"  # Required for batched generation\n",
        "\n",
        "print(f\"Model loaded successfully on {next(model.parameters()).device}\")\n",
        "print(f\"Model dtype: {next(model.parameters()).dtype}\")\n",
        "print(f\"Tokenizer pad_token: '{tokenizer.pad_token}' (id: {tokenizer.pad_token_id})\")\n",
        "print(f\"Tokenizer padding_side: {tokenizer.padding_side}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 4: Data Pipeline (Robust)\n",
        "BATCH_SIZE = 64\n",
        "NUM_SAMPLES = 5000\n",
        "\n",
        "print(\"Loading global_voices dataset (es-en split)...\")\n",
        "dataset = load_dataset(\"Divyanshu/global_voices\", \"es-en\", split=\"train\")\n",
        "\n",
        "# Inspection step: verify dataset structure\n",
        "print(f\"\\nDataset column names: {dataset.column_names}\")\n",
        "print(f\"First sample: {dataset[0]}\")\n",
        "\n",
        "# Extract Spanish (es) and English (en) from the translation column dictionary\n",
        "def extract_translations(example):\n",
        "    return {\n",
        "        \"spanish\": example[\"translation\"][\"es\"],\n",
        "        \"english\": example[\"translation\"][\"en\"]\n",
        "    }\n",
        "\n",
        "dataset = dataset.map(extract_translations, remove_columns=dataset.column_names)\n",
        "\n",
        "# Filter to first 5,000 rows\n",
        "dataset = dataset.select(range(min(NUM_SAMPLES, len(dataset))))\n",
        "\n",
        "print(f\"\\nProcessed dataset size: {len(dataset)}\")\n",
        "print(f\"Sample after extraction: {dataset[0]}\")\n",
        "\n",
        "# Create a custom Dataset class for DataLoader\n",
        "class TranslationDataset(Dataset):\n",
        "    def __init__(self, hf_dataset):\n",
        "        self.data = hf_dataset\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        item = self.data[idx]\n",
        "        return item[\"spanish\"], item[\"english\"]\n",
        "\n",
        "# Create DataLoader\n",
        "torch_dataset = TranslationDataset(dataset)\n",
        "dataloader = DataLoader(\n",
        "    torch_dataset, \n",
        "    batch_size=BATCH_SIZE, \n",
        "    shuffle=False,\n",
        "    num_workers=0,  # Avoid multiprocessing issues in notebook\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "print(f\"\\nDataLoader created: {len(dataloader)} batches of size {BATCH_SIZE}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 5: Benchmark Functions\n",
        "\n",
        "@torch.no_grad()\n",
        "def calculate_batch_perplexity(texts: list[str], model, tokenizer) -> list[float]:\n",
        "    \"\"\"\n",
        "    Vectorized perplexity calculation using cross-entropy loss.\n",
        "    Uses teacher-forcing: compute loss on input tokens and convert to PPL via exp(loss).\n",
        "    \"\"\"\n",
        "    # Tokenize batch with padding\n",
        "    encodings = tokenizer(\n",
        "        texts,\n",
        "        return_tensors=\"pt\",\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        max_length=512,\n",
        "    ).to(\"cuda\")\n",
        "    \n",
        "    input_ids = encodings.input_ids\n",
        "    attention_mask = encodings.attention_mask\n",
        "    \n",
        "    # Forward pass\n",
        "    outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "    logits = outputs.logits\n",
        "    \n",
        "    # Shift logits and labels for causal LM loss calculation\n",
        "    shift_logits = logits[:, :-1, :].contiguous()\n",
        "    shift_labels = input_ids[:, 1:].contiguous()\n",
        "    shift_mask = attention_mask[:, 1:].contiguous()\n",
        "    \n",
        "    # Calculate per-token cross entropy loss\n",
        "    loss_fct = torch.nn.CrossEntropyLoss(reduction='none')\n",
        "    losses = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
        "    losses = losses.view(shift_labels.size())\n",
        "    \n",
        "    # Mask out padding tokens and calculate mean loss per sequence\n",
        "    masked_losses = losses * shift_mask\n",
        "    seq_lengths = shift_mask.sum(dim=1).clamp(min=1)  # Avoid division by zero\n",
        "    mean_losses = masked_losses.sum(dim=1) / seq_lengths\n",
        "    \n",
        "    # Convert to perplexity\n",
        "    perplexities = torch.exp(mean_losses)\n",
        "    \n",
        "    return perplexities.cpu().tolist()\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def generate_translations(spanish_texts: list[str], model, tokenizer, max_new_tokens: int = 64) -> list[str]:\n",
        "    \"\"\"\n",
        "    Batched generation with greedy decoding.\n",
        "    Format: \"Spanish: {es}\\nEnglish:\"\n",
        "    \"\"\"\n",
        "    # Format prompts\n",
        "    prompts = [f\"Spanish: {text}\\nEnglish:\" for text in spanish_texts]\n",
        "    \n",
        "    # Tokenize with left padding (already configured in tokenizer)\n",
        "    encodings = tokenizer(\n",
        "        prompts,\n",
        "        return_tensors=\"pt\",\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        max_length=512,\n",
        "    ).to(\"cuda\")\n",
        "    \n",
        "    # Generate with greedy decoding\n",
        "    generated_ids = model.generate(\n",
        "        input_ids=encodings.input_ids,\n",
        "        attention_mask=encodings.attention_mask,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        do_sample=False,  # Greedy decoding\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "    )\n",
        "    \n",
        "    # Decode only the new tokens (exclude input prompt)\n",
        "    input_length = encodings.input_ids.shape[1]\n",
        "    generated_texts = tokenizer.batch_decode(\n",
        "        generated_ids[:, input_length:],\n",
        "        skip_special_tokens=True\n",
        "    )\n",
        "    \n",
        "    return generated_texts\n",
        "\n",
        "\n",
        "def get_peak_vram_mb(handle) -> float:\n",
        "    \"\"\"Get peak VRAM usage in MB using pynvml.\"\"\"\n",
        "    info = pynvml.nvmlDeviceGetMemoryInfo(handle)\n",
        "    return info.used / (1024 ** 2)\n",
        "\n",
        "\n",
        "def reset_peak_memory_stats():\n",
        "    \"\"\"Reset PyTorch's peak memory tracking.\"\"\"\n",
        "    torch.cuda.reset_peak_memory_stats()\n",
        "\n",
        "\n",
        "def get_pytorch_peak_memory_mb() -> float:\n",
        "    \"\"\"Get PyTorch's tracked peak memory in MB.\"\"\"\n",
        "    return torch.cuda.max_memory_allocated() / (1024 ** 2)\n",
        "\n",
        "\n",
        "print(\"Benchmark functions defined successfully.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 6: Main Benchmark Loop (\"Unbottlenecked\" - no Python for-loops for inference)\n",
        "\n",
        "# Results storage\n",
        "all_spanish = []\n",
        "all_english = []\n",
        "all_outputs = []\n",
        "all_ppls = []\n",
        "peak_vram_readings = []\n",
        "\n",
        "print(\"Starting benchmark...\")\n",
        "print(f\"Total samples: {len(torch_dataset)}\")\n",
        "print(f\"Batch size: {BATCH_SIZE}\")\n",
        "print(f\"Total batches: {len(dataloader)}\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "# Warm-up run to stabilize GPU\n",
        "print(\"Warming up GPU...\")\n",
        "warmup_texts = [\"Hola, ¿cómo estás?\"] * 4\n",
        "_ = calculate_batch_perplexity(warmup_texts, model, tokenizer)\n",
        "_ = generate_translations(warmup_texts, model, tokenizer, max_new_tokens=16)\n",
        "torch.cuda.synchronize()\n",
        "print(\"Warm-up complete.\\n\")\n",
        "\n",
        "# Main benchmark\n",
        "start_time = time.perf_counter()\n",
        "\n",
        "for batch_idx, (spanish_batch, english_batch) in enumerate(tqdm(dataloader, desc=\"Processing batches\")):\n",
        "    # Convert tuple of strings to list\n",
        "    spanish_list = list(spanish_batch)\n",
        "    english_list = list(english_batch)\n",
        "    \n",
        "    # Reset memory tracking for this batch\n",
        "    reset_peak_memory_stats()\n",
        "    \n",
        "    # Metric 1: Calculate perplexity on Spanish input (batched)\n",
        "    batch_ppls = calculate_batch_perplexity(spanish_list, model, tokenizer)\n",
        "    \n",
        "    # Metric 2: Generate translations (batched)\n",
        "    batch_outputs = generate_translations(spanish_list, model, tokenizer, max_new_tokens=64)\n",
        "    \n",
        "    # Synchronize to ensure all GPU work is complete before measuring VRAM\n",
        "    torch.cuda.synchronize()\n",
        "    \n",
        "    # Log peak VRAM usage for this batch\n",
        "    batch_peak_vram = get_peak_vram_mb(gpu_handle)\n",
        "    peak_vram_readings.append(batch_peak_vram)\n",
        "    \n",
        "    # Accumulate results\n",
        "    all_spanish.extend(spanish_list)\n",
        "    all_english.extend(english_list)\n",
        "    all_outputs.extend(batch_outputs)\n",
        "    all_ppls.extend(batch_ppls)\n",
        "    \n",
        "    # Progress logging every 10 batches\n",
        "    if (batch_idx + 1) % 10 == 0:\n",
        "        elapsed = time.perf_counter() - start_time\n",
        "        samples_done = (batch_idx + 1) * BATCH_SIZE\n",
        "        throughput = samples_done / elapsed\n",
        "        print(f\"  Batch {batch_idx + 1}/{len(dataloader)} | \"\n",
        "              f\"Throughput: {throughput:.1f} samples/s | \"\n",
        "              f\"Peak VRAM: {batch_peak_vram:.0f} MB\")\n",
        "\n",
        "end_time = time.perf_counter()\n",
        "total_time = end_time - start_time\n",
        "\n",
        "print(\"-\" * 60)\n",
        "print(f\"Benchmark complete!\")\n",
        "print(f\"Total samples processed: {len(all_spanish)}\")\n",
        "print(f\"Total time: {total_time:.2f} seconds\")\n",
        "print(f\"Overall throughput: {len(all_spanish) / total_time:.2f} samples/second\")\n",
        "print(f\"Max peak VRAM observed: {max(peak_vram_readings):.0f} MB\")\n",
        "print(f\"Average peak VRAM: {sum(peak_vram_readings) / len(peak_vram_readings):.0f} MB\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 7: Output and Reporting\n",
        "\n",
        "# Create results DataFrame\n",
        "results_df = pd.DataFrame({\n",
        "    \"spanish_src\": all_spanish,\n",
        "    \"english_ref\": all_english,\n",
        "    \"model_output\": all_outputs,\n",
        "    \"input_ppl\": all_ppls\n",
        "})\n",
        "\n",
        "# Save to CSV\n",
        "output_file = \"pythia_1b_h100_baseline.csv\"\n",
        "results_df.to_csv(output_file, index=False)\n",
        "print(f\"Results saved to: {output_file}\")\n",
        "\n",
        "# Display summary statistics\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"BENCHMARK SUMMARY\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"\\nModel: {MODEL_NAME}\")\n",
        "print(f\"Dataset: global_voices (es-en)\")\n",
        "print(f\"Total Samples: {len(results_df)}\")\n",
        "print(f\"Batch Size: {BATCH_SIZE}\")\n",
        "\n",
        "print(f\"\\n--- Performance Metrics ---\")\n",
        "print(f\"Total Runtime: {total_time:.2f} seconds\")\n",
        "print(f\"Throughput: {len(results_df) / total_time:.2f} samples/second\")\n",
        "print(f\"Average Time per Sample: {total_time / len(results_df) * 1000:.2f} ms\")\n",
        "\n",
        "print(f\"\\n--- Perplexity Statistics ---\")\n",
        "print(f\"Mean PPL: {results_df['input_ppl'].mean():.2f}\")\n",
        "print(f\"Median PPL: {results_df['input_ppl'].median():.2f}\")\n",
        "print(f\"Std PPL: {results_df['input_ppl'].std():.2f}\")\n",
        "print(f\"Min PPL: {results_df['input_ppl'].min():.2f}\")\n",
        "print(f\"Max PPL: {results_df['input_ppl'].max():.2f}\")\n",
        "\n",
        "print(f\"\\n--- VRAM Usage ---\")\n",
        "print(f\"Max Peak VRAM: {max(peak_vram_readings):.0f} MB\")\n",
        "print(f\"Avg Peak VRAM: {sum(peak_vram_readings) / len(peak_vram_readings):.0f} MB\")\n",
        "\n",
        "print(\"\\n--- Sample Outputs ---\")\n",
        "for i in range(min(3, len(results_df))):\n",
        "    print(f\"\\nSample {i+1}:\")\n",
        "    print(f\"  Spanish: {results_df.iloc[i]['spanish_src'][:100]}...\")\n",
        "    print(f\"  English (ref): {results_df.iloc[i]['english_ref'][:100]}...\")\n",
        "    print(f\"  Model output: {results_df.iloc[i]['model_output'][:100]}...\")\n",
        "    print(f\"  Input PPL: {results_df.iloc[i]['input_ppl']:.2f}\")\n",
        "\n",
        "# Cleanup pynvml\n",
        "pynvml.nvmlShutdown()\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"Benchmark complete. Results saved to\", output_file)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
