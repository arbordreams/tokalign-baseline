% Vocabulary Adaptation Methods
@inproceedings{minixhofer2022wechsel,
  title={{WECHSEL}: Effective Initialization of Subword Embeddings for Cross-lingual Transfer of Monolingual Language Models},
  author={Minixhofer, Benjamin and Paischer, Fabian and Rekabsaz, Navid},
  booktitle={Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  pages={3992--4006},
  year={2022}
}

@inproceedings{li2025tokalign,
  title={{TokAlign}: A Token Alignment Method for Continual Vocabulary Adaptation of Language Models},
  author={Li, Chao and Zhang, Jian and Zong, Chengqing},
  booktitle={Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics},
  year={2025}
}

@inproceedings{ostendorff2023efficient,
  title={Efficient Language Model Training through Cross-Lingual and Progressive Transfer Learning},
  author={Ostendorff, Malte and Rehm, Georg},
  booktitle={Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
  year={2023}
}

@inproceedings{rust2021good,
  title={How Good is Your Tokenizer? On the Monolingual Performance of Multilingual Language Models},
  author={Rust, Phillip and Pfeiffer, Jonas and Vuli{\'c}, Ivan and Ruder, Sebastian and Gurevych, Iryna},
  booktitle={Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics},
  pages={3118--3135},
  year={2021}
}

@article{petrov2024language,
  title={Language Model Tokenizers Introduce Unfairness Between Languages},
  author={Petrov, Aleksandar and La Malfa, Emanuele and Torr, Philip and Biber, Adel},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

% MT Evaluation Metrics
@inproceedings{papineni2002bleu,
  title={{BLEU}: A Method for Automatic Evaluation of Machine Translation},
  author={Papineni, Kishore and Roukos, Salim and Ward, Todd and Zhu, Wei-Jing},
  booktitle={Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics},
  pages={311--318},
  year={2002}
}

@inproceedings{popovic2015chrf,
  title={{chrF}: Character n-gram F-score for Automatic MT Evaluation},
  author={Popovi{\'c}, Maja},
  booktitle={Proceedings of the Tenth Workshop on Statistical Machine Translation},
  pages={392--395},
  year={2015}
}

@inproceedings{rei2020comet,
  title={{COMET}: A Neural Framework for {MT} Evaluation},
  author={Rei, Ricardo and Stewart, Craig and Farinha, Ana C and Lavie, Alon},
  booktitle={Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing},
  pages={2685--2702},
  year={2020}
}

@inproceedings{freitag2022results,
  title={Results of {WMT22} Metrics Shared Task: Stop Using {BLEU} -- Neural Metrics Are Better and More Robust},
  author={Freitag, Markus and Rei, Ricardo and Mathur, Nitika and others},
  booktitle={Proceedings of the Seventh Conference on Machine Translation},
  pages={46--68},
  year={2022}
}

@inproceedings{kocmi2024navigating,
  title={Navigating the Metrics Maze: Reconciling Score Magnitudes and Accuracies},
  author={Kocmi, Tom and Federmann, Christian and Grundkiewicz, Roman and others},
  booktitle={Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics},
  year={2024}
}

% Efficient Inference and Vocabulary
@article{shao2025vqlogits,
  title={{VQ-Logits}: Compressing the Output Bottleneck of Large Language Models},
  author={Shao, Jiayi and Huang, Hao and Wu, Jian and others},
  journal={arXiv preprint arXiv:2505.10202},
  year={2025}
}

@inproceedings{amer2022fast,
  title={Fast Vocabulary Projection Method via Clustering for Multilingual Machine Translation on {GPU}},
  author={Amer, Hany and Kim, Young Jin and Afify, Mohamed and others},
  booktitle={Proceedings of the 15th Biennial Conference of the Association for Machine Translation in the Americas},
  year={2022}
}

@inproceedings{kim2022learned,
  title={Learned Token Pruning for Transformers},
  author={Kim, Sehoon and Shen, Sheng and Thorsley, David and Gholami, Amir and Kwon, Joseph and Mahoney, Michael W and Keutzer, Kurt},
  booktitle={Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  pages={784--794},
  year={2022}
}

% Base Models
@article{biderman2023pythia,
  title={Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling},
  author={Biderman, Stella and Schoelkopf, Hailey and Anthony, Quentin and others},
  journal={Proceedings of the 40th International Conference on Machine Learning},
  year={2023}
}

@article{touvron2023llama,
  title={{LLaMA}: Open and Efficient Foundation Language Models},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and others},
  journal={arXiv preprint arXiv:2302.13971},
  year={2023}
}

% Tokenization
@inproceedings{sennrich2016neural,
  title={Neural Machine Translation of Rare Words with Subword Units},
  author={Sennrich, Rico and Haddow, Barry and Birch, Alexandra},
  booktitle={Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics},
  pages={1715--1725},
  year={2016}
}

@inproceedings{kudo2018sentencepiece,
  title={{SentencePiece}: A Simple and Language Independent Subword Tokenizer and Detokenizer for Neural Text Processing},
  author={Kudo, Taku and Richardson, John},
  booktitle={Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations},
  pages={66--71},
  year={2018}
}

@article{radford2019language,
  title={Language Models are Unsupervised Multitask Learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  journal={OpenAI Blog},
  year={2019}
}

% Multilingual NLP
@inproceedings{conneau2020unsupervised,
  title={Unsupervised Cross-lingual Representation Learning at Scale},
  author={Conneau, Alexis and Khandelwal, Kartikay and Goyal, Naman and others},
  booktitle={Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  pages={8440--8451},
  year={2020}
}

@inproceedings{xue2021mt5,
  title={{mT5}: A Massively Multilingual Pre-trained Text-to-Text Transformer},
  author={Xue, Linting and Constant, Noah and Roberts, Adam and others},
  booktitle={Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics},
  pages={483--498},
  year={2021}
}
